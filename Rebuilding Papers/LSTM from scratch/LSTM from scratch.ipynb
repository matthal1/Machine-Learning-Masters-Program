{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18da7ecc",
   "metadata": {},
   "source": [
    "Implementation of 1997 Neural Computation Paper on LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce92096",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08db995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79c2b8",
   "metadata": {},
   "source": [
    "Construct a forget gate: f_t = sigmoid(w_t * concatenation(h_t-1, x_t)), where w_t are the weights, h_t-1 is the hidden state and x_t is the current state\n",
    "We expect the output to be \n",
    "x_t: (batch_size, input_size)\n",
    "h_prev: (batch_size, hidden_size)\n",
    "f_t: (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf85cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgetGate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for the hidden size and input size\n",
    "        self.W_x = nn.Parameter(torch.randn(hidden_size, input_size) / math.sqrt(input_size))\n",
    "        # Weights for the hidden state\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        # Bias term\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x_t, h_prev):\n",
    "        # Compute the forget gate activation\n",
    "\n",
    "        # Linear part: x_t * W_x^T + h_prev * W_h^T + b\n",
    "        z = x_t @ self.W_x.T + h_prev @ self.W_h.T + self.b\n",
    "        # Gate activation: sigmoid(z)\n",
    "        f_t = torch.sigmoid(z)\n",
    "        return f_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fadca1",
   "metadata": {},
   "source": [
    "Next we want to create an input gate: i_t = sigmoid(w_i * concate(h_t -1, x_t))\n",
    "Expected output: \n",
    "x_t -> (batch_size, input_size)\n",
    "h_prev -> (batch_size, hidden_state)\n",
    "i_t -> (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93987d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for the hidden size and input size\n",
    "        self.W_x = nn.Parameter(torch.randn(hidden_size, input_size) / math.sqrt(input_size))\n",
    "        # Weights for the hidden state\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        # Bias term\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        # Compute the input gate activation\n",
    "\n",
    "        # Linear part: x_t * W_x^T + h_prev * W_h^T + b\n",
    "        z = x_t @ self.W_x.T + h_prev @ self.W_h.T + self.b\n",
    "        # Gate activation: sigmoid(z)\n",
    "        i_t = torch.sigmoid(z)\n",
    "        return i_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18143b",
   "metadata": {},
   "source": [
    "New value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9e80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewValueFunction(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for the hidden size and input size\n",
    "        self.W_x = nn.Parameter(torch.randn(hidden_size, input_size) / math.sqrt(input_size))\n",
    "        # Weights for the hidden state\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        # Bias term\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        # Compute the new value function\n",
    "\n",
    "        # Linear part: x_t * W_x^T + h_prev * W_h^T + b\n",
    "        z = x_t @ self.W_x.T + h_prev @ self.W_h.T + self.b\n",
    "        # Gate activation: tanh(z)\n",
    "        g_t = torch.tanh(z)\n",
    "        return g_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8bc2b",
   "metadata": {},
   "source": [
    "Update conveyor belt -> To update the conveyor belt we must use the 3 previous functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846654f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConveyorBeltUpdate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.forget_gate = ForgetGate(input_size, hidden_size)\n",
    "        self.input_gate = InputGate(input_size, hidden_size)\n",
    "        self.new_value = NewValueFunction(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        # Update the cell state (conveyor belt)\n",
    "       \n",
    "        f_t = self.forget_gate(x_t, h_prev)\n",
    "        i_t = self.input_gate(x_t, h_prev)\n",
    "        g_t = self.new_value(x_t, h_prev)\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        return c_t, f_t, i_t, g_t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e467e4f",
   "metadata": {},
   "source": [
    "Create output gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e48e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for the hidden size and input size\n",
    "        self.W_x = nn.Parameter(torch.randn(hidden_size, input_size) / math.sqrt(input_size))\n",
    "        # Weights for the hidden state\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        # Bias term\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x_t, h_prev):\n",
    "        # Compute the output gate activation\n",
    "\n",
    "        # Linear part: x_t * W_x^T + h_prev * W_h^T + b\n",
    "        z = x_t @ self.W_x.T + h_prev @ self.W_h.T + self.b\n",
    "        # Gate activation: sigmoid(z)\n",
    "        o_t = torch.sigmoid(z)\n",
    "        return o_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55aa25f",
   "metadata": {},
   "source": [
    "Create update state class -> This will return the final current c_t and h_t states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba92b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateState(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.updateConveyorBelt = ConveyorBeltUpdate(input_size, hidden_size)\n",
    "        self.output_gate = OutputGate(input_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev, return_gates: bool=False):\n",
    "        # Compute the updated hidden state\n",
    "\n",
    "        # 1 Update cell using conveyor belt update\n",
    "        c_t, f_t, i_t, g_t = self.updateConveyorBelt(x_t, h_prev, c_prev)\n",
    "\n",
    "        # 2 Compute output gate\n",
    "        o_t = self.output_gate(x_t, h_prev)\n",
    "\n",
    "        # 3 Compute update hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        if return_gates: \n",
    "            gates = {\"f_t\": f_t, \"i_t\": i_t, \"g_t\": g_t, \"o_t\": o_t}\n",
    "            return h_t, c_t, gates\n",
    "        else:\n",
    "            return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cff635",
   "metadata": {},
   "source": [
    "To run this code we would need to iterate over the previous function in order to get a proper output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe82c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.update_state = UpdateState(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x):  # x shape: (batch_size, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.update_state(x[:, t, :], h_t, c_t)\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf4062",
   "metadata": {},
   "source": [
    "          ┌────────┐\n",
    "x_t ----> │Forget  │--- W_f, U_f, b_f\n",
    "h_t-1 --> │ Gate   │--- controls what to erase\n",
    "          └────────┘\n",
    "\n",
    "x_t ----> ┌────────┐\n",
    "h_t-1 --> │Input   │--- W_i, U_i, b_i\n",
    "          │ Gate   │--- controls what to write\n",
    "          └────────┘\n",
    "\n",
    "x_t ----> ┌────────────┐\n",
    "h_t-1 --> │New Value   │--- W_c, U_c, b_c\n",
    "          │(Candidate) │--- generates new memory\n",
    "          └────────────┘\n",
    "\n",
    "x_t ----> ┌────────┐\n",
    "h_t-1 --> │Output  │--- W_o, U_o, b_o\n",
    "          │ Gate   │--- controls exposure of memory\n",
    "          └────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5c15d",
   "metadata": {},
   "source": [
    "Test if my implementation of LSTM works by making a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8103c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def make_sine_dataset(n_sequences=1000, seq_len=20):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _ in range(n_sequences):\n",
    "        # random phase and frequency\n",
    "        phase = torch.rand(1).item() * 2 * math.pi\n",
    "        freq  = 0.1 + torch.rand(1).item() * 0.4  # 0.1–0.5\n",
    "        t = torch.arange(seq_len + 1).float()\n",
    "        series = torch.sin(freq * t + phase)      # shape: (seq_len+1,)\n",
    "        X.append(series[:-1].unsqueeze(-1))       # (seq_len, 1)\n",
    "        Y.append(series[-1])                      # scalar target\n",
    "    X = torch.stack(X, dim=0)   # (batch, seq_len, 1)\n",
    "    Y = torch.stack(Y, dim=0)   # (batch,)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921c2c6",
   "metadata": {},
   "source": [
    "Make the dataset and apply a test train split and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef12424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: torch.Size([700, 20, 1]) torch.Size([700])\n",
      "Validation set: torch.Size([150, 20, 1]) torch.Size([150])\n",
      "Test set: torch.Size([150, 20, 1]) torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "X, Y = make_sine_dataset(n_sequences=1000, seq_len=seq_len)\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train set:\", X_train.shape, Y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, Y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d62cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batchsize, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=batchsize)\n",
    "val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302dd36",
   "metadata": {},
   "source": [
    "Create an instance of the model and create an optimizer + loss function\n",
    "Create a training loop of 20 epochs to test the strength of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "836cb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=1, hidden_size=32)\n",
    "predictor = nn.Linear(32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9c2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d38aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.4120 - Val Loss: 0.3591\n",
      "Epoch 2/20 - Train Loss: 0.3109 - Val Loss: 0.2749\n",
      "Epoch 3/20 - Train Loss: 0.2360 - Val Loss: 0.2104\n",
      "Epoch 4/20 - Train Loss: 0.1751 - Val Loss: 0.1569\n",
      "Epoch 5/20 - Train Loss: 0.1264 - Val Loss: 0.1135\n",
      "Epoch 6/20 - Train Loss: 0.0879 - Val Loss: 0.0824\n",
      "Epoch 7/20 - Train Loss: 0.0628 - Val Loss: 0.0646\n",
      "Epoch 8/20 - Train Loss: 0.0480 - Val Loss: 0.0505\n",
      "Epoch 9/20 - Train Loss: 0.0383 - Val Loss: 0.0405\n",
      "Epoch 10/20 - Train Loss: 0.0306 - Val Loss: 0.0331\n",
      "Epoch 11/20 - Train Loss: 0.0248 - Val Loss: 0.0270\n",
      "Epoch 12/20 - Train Loss: 0.0200 - Val Loss: 0.0219\n",
      "Epoch 13/20 - Train Loss: 0.0161 - Val Loss: 0.0179\n",
      "Epoch 14/20 - Train Loss: 0.0131 - Val Loss: 0.0150\n",
      "Epoch 15/20 - Train Loss: 0.0109 - Val Loss: 0.0126\n",
      "Epoch 16/20 - Train Loss: 0.0092 - Val Loss: 0.0107\n",
      "Epoch 17/20 - Train Loss: 0.0078 - Val Loss: 0.0095\n",
      "Epoch 18/20 - Train Loss: 0.0068 - Val Loss: 0.0079\n",
      "Epoch 19/20 - Train Loss: 0.0056 - Val Loss: 0.0068\n",
      "Epoch 20/20 - Train Loss: 0.0048 - Val Loss: 0.0058\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        h_T, _ = model(X_batch)\n",
    "        preds = predictor(h_T).squeeze(-1)\n",
    "\n",
    "        loss = loss_fn(preds, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation Pass\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            h_T, _ = model(X_batch)\n",
    "            preds = predictor(h_T).squeeze(-1)\n",
    "            loss = loss_fn(preds, Y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4e23",
   "metadata": {},
   "source": [
    "Evaluate of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c0438ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 0.004802806728209059\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        h_T, _ = model(X_batch)\n",
    "        preds = predictor(h_T).squeeze(-1)\n",
    "        loss = loss_fn(preds, Y_batch)\n",
    "        test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(\"Final Test MSE:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d99a5",
   "metadata": {},
   "source": [
    "Compare my LSTM model vs the torch LSTM model using IMBD sentiment analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb80516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 717136.86 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 1125455.89 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 1202847.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "train_texts = imdb[\"train\"][\"text\"]\n",
    "train_labels = imdb[\"train\"][\"label\"]\n",
    "test_texts = imdb[\"test\"][\"text\"]\n",
    "test_labels = imdb[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb211e0",
   "metadata": {},
   "source": [
    "Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614fff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "# Build vocab from training texts\n",
    "counter = Counter()\n",
    "for txt in train_texts:\n",
    "    counter.update(basic_tokenize(txt))\n",
    "\n",
    "max_vocab_size = 20000\n",
    "most_common = counter.most_common(max_vocab_size - 2)\n",
    "itos = [\"<pad>\", \"<unk>\"] + [w for w, _ in most_common]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "pad_idx = stoi[\"<pad>\"]\n",
    "unk_idx = stoi[\"<unk>\"]\n",
    "\n",
    "def encode_text(text, max_len=200):\n",
    "    tokens = basic_tokenize(text)\n",
    "    ids = [stoi.get(tok, unk_idx) for tok in tokens][:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [pad_idx] * (max_len - len(ids))\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def encode_label(label):\n",
    "    return torch.tensor(label, dtype=torch.long)  # 0 or 1 in HF IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ef63f",
   "metadata": {},
   "source": [
    "Build datasets and dataloader for IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9609a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "\n",
    "X_train_ids = torch.stack([encode_text(t, max_len) for t in train_texts])\n",
    "y_train_ids = torch.stack([encode_label(l) for l in train_labels])\n",
    "\n",
    "X_test_ids  = torch.stack([encode_text(t, max_len) for t in test_texts])\n",
    "y_test_ids  = torch.stack([encode_label(l) for l in test_labels])\n",
    "\n",
    "# train / val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_ids, y_train_ids, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val,   y_val),   batch_size=batch_size)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_ids, y_test_ids), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4f2f1",
   "metadata": {},
   "source": [
    "Make a custom LSTM sentiment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02fad2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm      = LSTM(input_size=embed_dim, hidden_size=hidden_size)\n",
    "        self.fc        = nn.Linear(hidden_size, 1)  # binary classification\n",
    "\n",
    "    def forward(self, x_ids):  # x_ids: (batch, seq_len) of token IDs\n",
    "        x_emb = self.embedding(x_ids)              # (batch, seq_len, embed_dim)\n",
    "        h_T, c_T = self.lstm(x_emb)               # (batch, hidden_size)\n",
    "        logits = self.fc(h_T).squeeze(-1)         # (batch,)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6036ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(itos)     # from your vocab list\n",
    "embed_dim  = 100\n",
    "hidden_size = 128\n",
    "\n",
    "pad_idx = stoi[\"<pad>\"]  \n",
    "\n",
    "model = CustomLSTMSentiment(vocab_size, embed_dim, hidden_size, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d68b7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4878ac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLSTMSentiment(\n",
       "  (embedding): Embedding(20000, 100, padding_idx=0)\n",
       "  (lstm): LSTM(\n",
       "    (update_state): UpdateState(\n",
       "      (updateConveyorBelt): ConveyorBeltUpdate(\n",
       "        (forget_gate): ForgetGate()\n",
       "        (input_gate): InputGate()\n",
       "        (new_value): NewValueFunction()\n",
       "      )\n",
       "      (output_gate): OutputGate()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f6e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)             # (batch, seq_len)\n",
    "        y_batch = y_batch.to(device).float()     # (batch,)\n",
    "\n",
    "        logits = model(X_batch)                  # (batch,)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # predictions\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).long()\n",
    "        total_correct += (preds == y_batch.long()).sum().item()\n",
    "        total_count += X_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc  = total_correct / total_count\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).float()\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long()\n",
    "            total_correct += (preds == y_batch.long()).sum().item()\n",
    "            total_count += X_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc  = total_correct / total_count\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30ac1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6922, Acc: 0.515 | Val Loss: 0.6997, Acc: 0.565\n",
      "Epoch 2/15 | Train Loss: 0.6725, Acc: 0.563 | Val Loss: 0.7193, Acc: 0.607\n",
      "Epoch 3/15 | Train Loss: 0.6482, Acc: 0.623 | Val Loss: 0.6799, Acc: 0.589\n",
      "Epoch 4/15 | Train Loss: 0.5938, Acc: 0.687 | Val Loss: 0.6666, Acc: 0.642\n",
      "Epoch 5/15 | Train Loss: 0.5651, Acc: 0.716 | Val Loss: 0.6571, Acc: 0.657\n",
      "Epoch 6/15 | Train Loss: 0.5205, Acc: 0.750 | Val Loss: 0.7354, Acc: 0.619\n",
      "Epoch 7/15 | Train Loss: 0.4972, Acc: 0.761 | Val Loss: 0.7145, Acc: 0.655\n",
      "Epoch 8/15 | Train Loss: 0.4744, Acc: 0.769 | Val Loss: 0.7176, Acc: 0.661\n",
      "Epoch 9/15 | Train Loss: 0.4488, Acc: 0.787 | Val Loss: 0.7333, Acc: 0.672\n",
      "Epoch 10/15 | Train Loss: 0.4242, Acc: 0.788 | Val Loss: 0.8043, Acc: 0.554\n",
      "Epoch 11/15 | Train Loss: 0.5055, Acc: 0.727 | Val Loss: 0.7352, Acc: 0.673\n",
      "Epoch 12/15 | Train Loss: 0.4158, Acc: 0.806 | Val Loss: 0.7392, Acc: 0.690\n",
      "Epoch 13/15 | Train Loss: 0.4501, Acc: 0.782 | Val Loss: 0.7259, Acc: 0.672\n",
      "Epoch 14/15 | Train Loss: 0.4136, Acc: 0.810 | Val Loss: 0.7228, Acc: 0.666\n",
      "Epoch 15/15 | Train Loss: 0.5190, Acc: 0.724 | Val Loss: 0.6368, Acc: 0.695\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15 \n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc     = eval_one_epoch(model, val_loader,   device)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13322756",
   "metadata": {},
   "source": [
    "Compute test accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f1c67be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6423, Test Acc: 0.694\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_one_epoch(model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1065a5",
   "metadata": {},
   "source": [
    "Create instance of pyTorch LSTM and use the same dataloaders and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93ca5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TorchLSTMSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm      = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True      # so input/output shapes are (batch, seq, feat)\n",
    "        )\n",
    "        self.fc        = nn.Linear(hidden_size, 1)  # binary classification\n",
    "\n",
    "    def forward(self, x_ids):  # x_ids: (batch, seq_len)\n",
    "        # 1. Embed tokens\n",
    "        x_emb = self.embedding(x_ids)              # (batch, seq_len, embed_dim)\n",
    "\n",
    "        # 2. LSTM over the sequence\n",
    "        # output: (batch, seq_len, hidden_size)\n",
    "        # (h_T, c_T): each (num_layers * num_directions, batch, hidden_size)\n",
    "        output, (h_T, c_T) = self.lstm(x_emb)\n",
    "\n",
    "        # 3. Take last layer's hidden state at final time step\n",
    "        h_T = h_T[-1]                              # (batch, hidden_size)\n",
    "\n",
    "        # 4. Classifier head → logits\n",
    "        logits = self.fc(h_T).squeeze(-1)          # (batch,)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "770231e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchLSTMSentiment(vocab_size, embed_dim, hidden_size, pad_idx).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer_torch = torch.optim.Adam(torch_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cd1c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Torch LSTM] Epoch 1/15 | Train Loss: 0.6924, Acc: 0.511 | Val Loss: 0.6934, Acc: 0.503\n",
      "[Torch LSTM] Epoch 2/15 | Train Loss: 0.6883, Acc: 0.538 | Val Loss: 0.6929, Acc: 0.517\n",
      "[Torch LSTM] Epoch 3/15 | Train Loss: 0.6757, Acc: 0.571 | Val Loss: 0.7547, Acc: 0.521\n",
      "[Torch LSTM] Epoch 4/15 | Train Loss: 0.6502, Acc: 0.610 | Val Loss: 0.6751, Acc: 0.586\n",
      "[Torch LSTM] Epoch 5/15 | Train Loss: 0.5692, Acc: 0.705 | Val Loss: 0.7141, Acc: 0.538\n",
      "[Torch LSTM] Epoch 6/15 | Train Loss: 0.5905, Acc: 0.657 | Val Loss: 0.5923, Acc: 0.706\n",
      "[Torch LSTM] Epoch 7/15 | Train Loss: 0.4713, Acc: 0.788 | Val Loss: 0.5365, Acc: 0.754\n",
      "[Torch LSTM] Epoch 8/15 | Train Loss: 0.4341, Acc: 0.800 | Val Loss: 0.4951, Acc: 0.790\n",
      "[Torch LSTM] Epoch 9/15 | Train Loss: 0.3935, Acc: 0.834 | Val Loss: 0.4913, Acc: 0.796\n",
      "[Torch LSTM] Epoch 10/15 | Train Loss: 0.3080, Acc: 0.881 | Val Loss: 0.4459, Acc: 0.816\n",
      "[Torch LSTM] Epoch 11/15 | Train Loss: 0.2454, Acc: 0.909 | Val Loss: 0.4391, Acc: 0.825\n",
      "[Torch LSTM] Epoch 12/15 | Train Loss: 0.2041, Acc: 0.929 | Val Loss: 0.4697, Acc: 0.825\n",
      "[Torch LSTM] Epoch 13/15 | Train Loss: 0.1842, Acc: 0.938 | Val Loss: 0.4495, Acc: 0.838\n",
      "[Torch LSTM] Epoch 14/15 | Train Loss: 0.1446, Acc: 0.955 | Val Loss: 0.4679, Acc: 0.835\n",
      "[Torch LSTM] Epoch 15/15 | Train Loss: 0.1148, Acc: 0.966 | Val Loss: 0.4931, Acc: 0.843\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(torch_model, train_loader, optimizer_torch, device)\n",
    "    val_loss, val_acc     = eval_one_epoch(torch_model, val_loader, device)\n",
    "\n",
    "    print(\n",
    "        f\"[Torch LSTM] Epoch {epoch}/{num_epochs} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2799b9f",
   "metadata": {},
   "source": [
    "Compare accuracy after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d02dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Torch LSTM] Test Loss: 0.5411, Test Acc: 0.823\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_one_epoch(torch_model, test_loader, device)\n",
    "print(f\"[Torch LSTM] Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
