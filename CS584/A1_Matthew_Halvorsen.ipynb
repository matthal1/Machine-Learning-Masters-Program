{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Binary Text Classification using Logistic Regression of ham and spam text messages"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Install all dependicies and make sure the data is stored in the same directory as this notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install pandas numpy"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "The workflow that this notebook will follow is as follows:\n",
                "\n",
                "1. Data Preprocessing: \n",
                "    <br>Load the dataset into sentences and labels\n",
                "    <br>Split the dataset into training, validation and testing sets \n",
                "    <br>Report the distribution in the form of a table\n",
                "    <br>Clean the data of any noise (urls, punctuation, and numbers) & change to lower case\n",
                "    <br>Tokenize input text into tokens, including work stemming and stopwords\n",
                "    <br>Build your own TD-IDF feature extractor using the training set\n",
                "2. Build a logistic regression classifier using using L2 regularization\n",
                "    <br>Derive the gradient of the objective function of LR with respect to w and b. \n",
                "    <br>Implement logistic regression via initialization, objective function, and gradient descent\n",
                "    <br>Implement accuracy, precision, recall and F1 score as test metrics\n",
                "    <br>Write a function for SGD and Mini-batch GD\n",
                "    <br>Evaluate the model of the test set and report the metrics \n",
                "3. Cross Validation\n",
                "    <br>Implement cross validation to choose the best hyperparameter lambda for the validation set\n",
                "4. Conclusion\n",
                "    <br>Analyze the results and compare to baseline\n",
                "5. Create a multiclass classifier from various authors dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import math\n",
                "import string\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load the dataset: make sure that this dataset is stored in the same folder as the juypter notebook to ensure it will run properly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>label</th>\n",
                            "      <th>text</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>ham</td>\n",
                            "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>ham</td>\n",
                            "      <td>Ok lar... Joking wif u oni...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>spam</td>\n",
                            "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>ham</td>\n",
                            "      <td>U dun say so early hor... U c already then say...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>ham</td>\n",
                            "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "  label                                               text\n",
                            "0   ham  Go until jurong point, crazy.. Available only ...\n",
                            "1   ham                      Ok lar... Joking wif u oni...\n",
                            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
                            "3   ham  U dun say so early hor... U c already then say...\n",
                            "4   ham  Nah I don't think he goes to usf, he lives aro..."
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "spam_df = pd.read_csv('a1-data/SMSSpamCollection', sep='\\t', header=None, names=['label', 'text'])\n",
                "\n",
                "spam_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Split the dataset into a training, validation and test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_dataset(df, train_size, val_size, test_size):\n",
                "    df = df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
                "    n = (len(df))\n",
                "    train_end = int(train_size * n)\n",
                "    val_end = train_end + int(val_size * n)\n",
                "\n",
                "    train_df = df.iloc[:train_end]\n",
                "    val_df = df.iloc[train_end:val_end]\n",
                "    test_df = df.iloc[val_end:n]\n",
                "\n",
                "    X_train, y_train = train_df[['text']], train_df['label']\n",
                "    X_val, y_val = val_df[['text']], val_df['label']\n",
                "    X_test, y_test = test_df[['text']], test_df['label']\n",
                "\n",
                "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
                "\n",
                "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(spam_df, 0.6, 0.2, 0.2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Report the data distribution of each class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Train</th>\n",
                            "      <th>Val</th>\n",
                            "      <th>Test</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>label</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>ham</th>\n",
                            "      <td>2898</td>\n",
                            "      <td>966</td>\n",
                            "      <td>961</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>spam</th>\n",
                            "      <td>445</td>\n",
                            "      <td>148</td>\n",
                            "      <td>154</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       Train  Val  Test\n",
                            "label                  \n",
                            "ham     2898  966   961\n",
                            "spam     445  148   154"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def data_distribution(y_train, y_val, y_test):\n",
                "    df =pd.DataFrame({'Train': y_train.value_counts(), 'Val': y_val.value_counts(), 'Test': y_test.value_counts()}).fillna(0).astype(int)\n",
                "    return df\n",
                "\n",
                "data_distribution(y_train, y_val, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocessor_cls",
            "metadata": {},
            "source": [
                "## 1. TextPreprocessor Class\n",
                "This will be used to cleaen the text. More specially remove urls, punctuation and numbers, as well as changing text to lowercase"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "preprocessor_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextPreprocessor:\n",
                "    def __init__(self, stopwords=None):\n",
                "        self.stopwords = stopwords or {\n",
                "            \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"is\", \"are\", \n",
                "            \"was\", \"were\", \"be\", \"to\", \"of\", \"in\", \"on\", \"for\"\n",
                "        }\n",
                "\n",
                "    def clean_text(self, text_series):\n",
                "        # Converts to lowercase\n",
                "        text = text_series.str.lower()\n",
                "        # Removes URLs\n",
                "        text = text.str.replace(r'https?://\\S+|www\\.\\S+', ' ', regex=True)\n",
                "        # Removes Punctuation\n",
                "        text = text.str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
                "        # Removes Digits\n",
                "        text = text.str.replace(r'\\d+', ' ', regex=True)\n",
                "        return text\n",
                "\n",
                "    def stem_token(self, token):\n",
                "        # Reduces words to there root form (e.g. -> Running to Run)\n",
                "        suffixes = [\"ing\", \"ly\", \"ed\", \"s\", \"es\", \"est\"]\n",
                "        for suf in suffixes:\n",
                "            # This avoids over stemming by checking if the length of the word could result in a real word\n",
                "            if token.endswith(suf) and len(token) > len(suf) + 2:\n",
                "                return token[:-len(suf)]\n",
                "        return token\n",
                "\n",
                "    def preprocess(self, X):\n",
                "        # This is the function that wraps the other together to create one smooth pipeline to stem and clean text\n",
                "        # This acts as a way to create raw tokens by splitting the \"documents\" in the corpus into words\n",
                "        cleaned = self.clean_text(X)\n",
                "        \n",
                "        # Tokenize, filter, and stem in one pass per document\n",
                "        def process_doc(doc):\n",
                "            words = doc.split()\n",
                "            return [self.stem_token(w) for w in words if w not in self.stopwords]\n",
                "            \n",
                "        return cleaned.apply(process_doc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run the Text preprocessor class to clean the data and output the cleaned version of the data. This will also tokenize the word inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess all three datasets\n",
                "preprocessor = TextPreprocessor()\n",
                "train_processed = preprocessor.preprocess(X_train['text'])\n",
                "val_processed = preprocessor.preprocess(X_val['text'])\n",
                "test_processed = preprocessor.preprocess(X_test['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Show the cleaned and tokenized text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0    [squeeeeeze, thi, christma, hug, if, u, lik, m...\n",
                            "1    [also, ive, sorta, blown, him, off, couple, ti...\n",
                            "2    [mmm, that, better, now, i, got, roast, down, ...\n",
                            "3    [mm, have, some, kanji, dont, eat, anyth, heav...\n",
                            "4    [so, there, ring, that, come, with, guy, costu...\n",
                            "Name: text, dtype: object"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "train_processed.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next we need to map the y labels to 0 and 1 so later the values can be used within the logistic regression model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert labels\n",
                "label_map = {\"spam\": 1, \"ham\": 0}\n",
                "y_train_bin = np.array([label_map[v] for v in y_train], dtype=float)\n",
                "y_val_bin = np.array([label_map[v] for v in y_val], dtype=float)\n",
                "y_test_bin = np.array([label_map[v] for v in y_test], dtype=float)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vectorizer_cls",
            "metadata": {},
            "source": [
                "## 2. TfidfVectorizer Class\n",
                "Build a TD-IDF vecotrizorizer from stratch:\n",
                "    <br>Goal is to create a function that would take in the list of words we have and return a dictonary of importance of each word\n",
                "    <br>TF = Term Frequency: The more the word appears in the document, the higher the TF\n",
                "    <br>IDF = Inverse Document Frequency: The less the word appears in the corpus, the higher the IDF\n",
                "What the functions do: \n",
                "    <br>fit_tfidf:\n",
                "        <br>Treats each row as a new \"document and counts the total number of documents\n",
                "        <br>Builds the document frequency required for IDF by converting the token list to a set so each word is counted at most once per document\n",
                "        <br>Update the voacb to say that the words appears in a specific document\n",
                "        <br>Building a vocabulary mapping and sorts the indices alphabetically\n",
                "        <br>Computes IDF for each word in the documents\n",
                "    <br>transform_tfidf:\n",
                "        <br>Counts term occurrences within that document\n",
                "        <br>Computes TF per term, typically count / len(doc)\n",
                "        <br>Computes TF-IDF per term\n",
                "        <br>Stores the result as a sparse vector \n",
                "        <br>Returns a list of vectors per document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "vectorizer_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class TfidfVectorizer:\n",
                "    def __init__(self):\n",
                "        # Maps word -> column index (e.g., \"python\" -> 5)\n",
                "        self.vocab = {}\n",
                "        # Stores the importance weight for every word in the vocab\n",
                "        self.idf = []\n",
                "        # List of words in the same order as the IDF values\n",
                "        self.vocab_words = []\n",
                "\n",
                "    def fit(self, text_list):\n",
                "        # Build vocab and IDF from training data\n",
                "        df = {} # Document Frequency: How many documents contain word 'w'\n",
                "        N = 0   # Total number of documents\n",
                "\n",
                "        for doc in text_list:\n",
                "            N += 1\n",
                "            # Using a set ensures we only count a word once per document\n",
                "            # for the Document Frequency (DF) calculation\n",
                "            seen = set(doc)\n",
                "            for w in seen:\n",
                "                df[w] = df.get(w, 0) + 1\n",
                "\n",
                "        # Sort keys to ensure the feature matrix columns are always in the same order\n",
                "        self.vocab_words = sorted(df.keys())\n",
                "        # Create a lookup for fast indexing during the transform phase\n",
                "        self.vocab = {w: i for i, w in enumerate(self.vocab_words)}\n",
                "\n",
                "        self.idf = [0.0] * len(self.vocab_words)\n",
                "        for w, i in self.vocab.items():\n",
                "            # THE IDF FORMULA -> log((1 + N) / (1 + df)) + 1 provides a \"smoothed\" IDF.\n",
                "            # This prevents division by zero and ensures words that appear in every document don't get a weight of exactly zero.\n",
                "            self.idf[i] = math.log((1.0 + N) / (1.0 + df[w])) + 1.0\n",
                "            \n",
                "        return self\n",
                "\n",
                "    def transform(self, text_list):\n",
                "        #Transform documents into sparse vectors\n",
                "        vectors = []\n",
                "        for doc in text_list:\n",
                "            counts = {}\n",
                "            for w in doc:\n",
                "                # IMPORTANT: If a word wasn't in the training data (fit), it is ignored here.\n",
                "                if w in self.vocab:\n",
                "                    idx = self.vocab[w]\n",
                "                    counts[idx] = counts.get(idx, 0) + 1\n",
                "\n",
                "            # Avoid division by zero for empty documents\n",
                "            doc_len = len(doc) if len(doc) > 0 else 1\n",
                "            vec = {} # Sparse representation: {index: tf-idf_score}\n",
                "            for idx, c in counts.items():\n",
                "                # TF (Term Frequency): How often word appears in THIS doc\n",
                "                tf = c / doc_len\n",
                "                # TF-IDF: Multiply local importance (TF) by global rarity (IDF)\n",
                "                vec[idx] = tf * self.idf[idx]\n",
                "            vectors.append(vec)\n",
                "        return vectors\n",
                "    \n",
                "    def fit_transform(self, text_list):\n",
                "        # Convenience method to learn vocab and return vectors in one go\n",
                "        self.fit(text_list)\n",
                "        return self.transform(text_list)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run the tfidf class on the training data to fit and transform the training class. This will establish the dictonary for tfidf while introducing a fit for the test and validation data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vectorize\n",
                "tfidf = TfidfVectorizer()\n",
                "X_train_vec = tfidf.fit_transform(train_processed.tolist())\n",
                "X_val_vec = tfidf.transform(val_processed.tolist())\n",
                "X_test_vec = tfidf.transform(test_processed.tolist())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We need to define the functions that will do the heavy lifting for logistic regression:\n",
                "<br> This includes a sparse function for the linear function that is passed before the activation function\n",
                "<br> Activation functions for binary and multiclass cases\n",
                "<br> Gradient functions for binary and multiclass cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dot_sparse(w, x):\n",
                "    # Computes w^T * x for a single document\n",
                "    s = 0\n",
                "    # Only iterates over non-zero features in the dictionary\n",
                "    for j, v in x.items():\n",
                "        # Ignores features that weren't in the training vocabulary\n",
                "        if j < len(w):\n",
                "            s += w[j] * v\n",
                "    return s"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    # Activation function mapping any real number to the range (0, 1)\n",
                "    # z is the log-odds; sigmoid(z) is the probability of the positive class\n",
                "    return 1.0 / (1.0 + np.exp(-z))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax(z):\n",
                "    # Generalization of sigmoid for multiple classes\n",
                "    # Subtracting the max prevents np.exp() from blowing up (overflow)\n",
                "    exp_z = np.exp(z - np.max(z))\n",
                "    # Normalize so that all class probabilities sum to 1.0\n",
                "    return exp_z / exp_z.sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_gradient_binary(w, b, X_batch, y_batch, lam):\n",
                "    # Calculates partial derivatives for Binary Cross-Entropy loss\n",
                "    dw = np.zeros_like(w)\n",
                "    db = 0.0\n",
                "    m = len(X_batch)\n",
                "    \n",
                "    for x_i, y_i in zip(X_batch, y_batch):\n",
                "        # 1. Forward Pass: calculate prediction\n",
                "        z = dot_sparse(w, x_i) + b\n",
                "        error = sigmoid(z) - y_i # Difference between predicted and actual\n",
                "        \n",
                "        # 2. Backward Pass: attribute error to individual feature weights\n",
                "        for j, v in x_i.items():\n",
                "            dw[j] += error * v\n",
                "        db += error\n",
                "    \n",
                "    # Return average gradient plus L2 penalty (derivative of lam * w^2 is 2 * lam * w)\n",
                "    return (dw / m) + 2 * lam * w, db / m"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_gradient_multiclass(W, b, X_batch, y_batch, lam):\n",
                "    # Calculates gradients for Categorical Cross-Entropy (Softmax Regression)\n",
                "    dw = np.zeros_like(W)  # Shape: (features, classes)\n",
                "    db = np.zeros_like(b)  # Shape: (classes,)\n",
                "    m = len(X_batch)\n",
                "    K = W.shape[1]\n",
                "    \n",
                "    for x_i, y_i in zip(X_batch, y_batch):\n",
                "        # Linear pass for all classes at once\n",
                "        z = np.zeros(K)\n",
                "        for j, v in x_i.items():\n",
                "            z += W[j, :] * v\n",
                "        z += b\n",
                "        \n",
                "        # Get probability distribution across all K classes\n",
                "        y_hat = softmax(z)\n",
                "        \n",
                "        # Convert class index (e.g., 2) to \"one-hot\" vector (e.g., [0,0,1,0])\n",
                "        target = np.zeros(K)\n",
                "        target[int(y_i)] = 1.0\n",
                "        error = y_hat - target # Vector of differences\n",
                "        \n",
                "        # Update gradient matrix: each weight j contributes to the error of class k\n",
                "        for j, v in x_i.items():\n",
                "            dw[j, :] += error * v\n",
                "        db += error\n",
                "        \n",
                "    return (dw / m) + 2 * lam * W, db / m\n",
                "\n",
                "def dot_sparse_multiclass(W, x):\n",
                "    # Efficiently computes the score for each class for a sparse document\n",
                "    # Result z contains one logit score per class (K,)\n",
                "    K = W.shape[1]\n",
                "    z = np.zeros(K)\n",
                "    for j, v in x.items():\n",
                "        if j < W.shape[0]:\n",
                "            # Add the weighted contribution of feature j to every class's score\n",
                "            z += W[j, :] * v\n",
                "    return z"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lr_cls",
            "metadata": {},
            "source": [
                "## 3. LogisticRegression Class\n",
                "<br> The goal of this class is to act as a hub to set up all three methods of gradient descent, stochastic gradient descent and mini-batch gradient descent.\n",
                "<br> This class also includes a predict function used to make preictions on validation and testing sets\n",
                "<br> For future use, this class also includes the ability for multiclass and binary class problems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "lr_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class LogisticRegressionModel:\n",
                "    def __init__(self, lr=0.01, lam=0.01, epochs=100, batch_size=32, method='minibatch', multiclass=False):\n",
                "        # Hyperparameters: lr (step size), lam (L2 strength), epochs (passes over data)\n",
                "        self.lr, self.lam, self.epochs = lr, lam, epochs\n",
                "        # Optimization settings: batch_size for 'minibatch', method choice, and problem type\n",
                "        self.batch_size, self.method, self.multiclass = batch_size, method, multiclass\n",
                "        # Weights (w), bias (b), and a container for loss history tracking\n",
                "        self.w, self.b, self.history = None, None, []\n",
                "\n",
                "    def _get_batch_indices(self, n):\n",
                "        # Generator that yields indices for different gradient descent methods\n",
                "        indices = np.arange(n)\n",
                "        # Shuffle for SGD and Minibatch to ensure variety in gradients\n",
                "        if self.method != 'batch':\n",
                "            np.random.shuffle(indices)\n",
                "        \n",
                "        if self.method == 'batch':\n",
                "            # Batch: Use all data at once for every step\n",
                "            yield indices\n",
                "        elif self.method == 'sgd':\n",
                "            # SGD: Step after every single example\n",
                "            for i in indices:\n",
                "                yield [i]\n",
                "        else: # minibatch\n",
                "            # Minibatch: Use a \"batch\" from the data\n",
                "            for i in range(0, n, self.batch_size):\n",
                "                yield indices[i : i + self.batch_size]\n",
                "\n",
                "    def _compute_loss(self, X, y):\n",
                "        # Calculates the objective function to minimize (Cross-Entropy + L2)\n",
                "        eps = 1e-15 # Prevents log(0) which leads to NaN errors\n",
                "        y_prob = self.predict_proba(X)\n",
                "        n = len(X)\n",
                "        \n",
                "        if self.multiclass:\n",
                "             # Categorical Cross-Entropy (CCE): Measures divergence for multiple labels\n",
                "             true_class_probs = y_prob[np.arange(n), y.astype(int)]\n",
                "             true_class_probs = np.clip(true_class_probs, eps, 1 - eps)\n",
                "             loss = -np.sum(np.log(true_class_probs))\n",
                "        else:\n",
                "             # Binary Cross-Entropy (BCE): Measures divergence for 0 vs 1\n",
                "             y_prob = np.clip(y_prob, eps, 1 - eps)\n",
                "             loss = -np.sum(y * np.log(y_prob) + (1 - y) * np.log(1 - y_prob))\n",
                "             \n",
                "        # L2 Regularization: penalizes large weights to prevent overfitting\n",
                "        # Loss = (Error / N) + (lambda * sum of squared weights)\n",
                "        reg = self.lam * np.sum(self.w ** 2)\n",
                "        return (loss / n) + reg\n",
                "\n",
                "    def fit(self, X, y, X_val=None, y_val=None):\n",
                "        # Finds the optimal w and b that minimize the loss function\n",
                "        # 1. Initialize weights based on the number of unique features (d)\n",
                "        all_idx = [idx for doc in X for idx in doc.keys()]\n",
                "        d = max(all_idx) + 1 if all_idx else 0\n",
                "        n = len(X)\n",
                "        \n",
                "        # Determine strategy: Binary (vector w) or Multiclass (matrix W)\n",
                "        if self.multiclass:\n",
                "            K = len(np.unique(y))\n",
                "            self.w, self.b = np.zeros((d, K)), np.zeros(K)\n",
                "            grad_func = compute_gradient_multiclass\n",
                "        else:\n",
                "            self.w, self.b = np.zeros(d), 0.0\n",
                "            grad_func = compute_gradient_binary\n",
                "\n",
                "        # 2. Main Solver Loop (Optimization)\n",
                "        for epoch in range(self.epochs):\n",
                "            for batch_idx in self._get_batch_indices(n):\n",
                "                # Pull the sparse feature dictionaries for the current batch\n",
                "                X_batch = [X[i] for i in batch_idx]\n",
                "                y_batch = y[batch_idx]\n",
                "\n",
                "                # Call the specific gradient calculator (should be implemented elsewhere)\n",
                "                dw, db = grad_func(self.w, self.b, X_batch, y_batch, self.lam)\n",
                "                \n",
                "                # Gradient Descent Update Rule: Move w in the direction that lowers loss\n",
                "                self.w -= self.lr * dw\n",
                "                self.b -= self.lr * db\n",
                "\n",
                "            # Periodic reporting for monitoring convergence\n",
                "            if epoch % 10 == 0 or epoch == self.epochs - 1:\n",
                "                train_loss = self._compute_loss(X, y)\n",
                "                log_msg = f\"Epoch {epoch:4d} | Train Loss: {train_loss:.6f}\"\n",
                "                \n",
                "                if X_val is not None and y_val is not None:\n",
                "                    val_loss = self._compute_loss(X_val, y_val)\n",
                "                    log_msg += f\" | Val Loss: {val_loss:.6f}\"\n",
                "                \n",
                "                print(log_msg)\n",
                "                \n",
                "        return self\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        # Passes the dot product through an activation function\n",
                "        probs = []\n",
                "        for x in X:\n",
                "            if self.multiclass:\n",
                "                # Softmax: Multi-class probabilities that sum to 1.0\n",
                "                z = dot_sparse_multiclass(self.w, x) + self.b\n",
                "                probs.append(softmax(z))\n",
                "            else:\n",
                "                # Sigmoid: Maps z to a value between 0 and 1\n",
                "                z = dot_sparse(self.w, x) + self.b\n",
                "                probs.append(sigmoid(z))\n",
                "        return np.array(probs)\n",
                "\n",
                "    def predict(self, X, threshold=0.5):\n",
                "            # Converts soft probabilities into hard class predictions\n",
                "            probs = self.predict_proba(X)\n",
                "            if self.multiclass:\n",
                "                # Argmax: Pick the class with the highest probability\n",
                "                return np.argmax(probs, axis=1)\n",
                "            else:\n",
                "                # Binary: Simple threshold check (usually 0.5)\n",
                "                return (probs >= threshold).astype(int)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "workflow_section",
            "metadata": {},
            "source": [
                "## 4. Cross validation function\n",
                "<br> Used to hyperparameter tune lambda for L2 regularization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Helper for Loss Calculation\n",
                "def compute_loss(model, X, y):\n",
                "    eps = 1e-15\n",
                "    y_prob = model.predict_proba(X) # Returns (n, K) or (n,)\n",
                "    n = len(X)\n",
                "    \n",
                "    if model.multiclass:\n",
                "        # y_prob is (n, K)\n",
                "        # y is indices (n,)\n",
                "        # CCE\n",
                "        true_class_probs = y_prob[np.arange(n), y.astype(int)]\n",
                "        true_class_probs = np.clip(true_class_probs, eps, 1 - eps)\n",
                "        loss = -np.sum(np.log(true_class_probs))\n",
                "        \n",
                "        # L2\n",
                "        reg = model.lam * np.sum(model.w ** 2)\n",
                "        return (loss / n) + reg\n",
                "    else:\n",
                "        # y_prob is (n,)\n",
                "        y_prob = np.clip(y_prob, eps, 1 - eps)\n",
                "        # BCE\n",
                "        loss = -np.sum(y * np.log(y_prob) + (1 - y) * np.log(1 - y_prob))\n",
                "        \n",
                "        # L2\n",
                "        reg = model.lam * np.sum(model.w ** 2)\n",
                "        return (loss / n) + reg\n",
                "\n",
                "# 2. Cross Validation Function\n",
                "def cross_validate(X_train, y_train, X_val, y_val, lambdas, config):\n",
                "    results = {}\n",
                "    best_loss = float('inf')\n",
                "    best_lam = None\n",
                "    \n",
                "    print(f\"Starting Cross-Validation on {len(lambdas)} lambda candidates...\")\n",
                "    \n",
                "    for lam in lambdas:\n",
                "        # Update config with current lambda\n",
                "        current_config = config.copy()\n",
                "        current_config['lam'] = lam\n",
                "        \n",
                "        # Initialize and Train\n",
                "        model = LogisticRegressionModel(**current_config)\n",
                "        # Pass validation data to fit to see per-epoch progress if desired, \n",
                "        # though here we are just collecting final loss.\n",
                "        model.fit(X_train, y_train)\n",
                "        \n",
                "        # Evaluate on VAL set\n",
                "        val_loss = compute_loss(model, X_val, y_val)\n",
                "        \n",
                "        results[lam] = val_loss\n",
                "        print(f\"Lambda: {lam:<6} | Validation Loss: {val_loss:.6f}\")\n",
                "        \n",
                "        if val_loss < best_loss:\n",
                "            best_loss = val_loss\n",
                "            best_lam = lam\n",
                "            \n",
                "    print(f\"Best Lambda: {best_lam}\")\n",
                "    return best_lam, results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We need to configure 3 models to cross validate, gradient descent, mini-batch gradient descent and stochastic gradient descent. The same setup will be used with a different config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Cross-Validation on 5 lambda candidates...\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677781\n",
                        "Epoch   20 | Train Loss: 0.664555\n",
                        "Epoch   30 | Train Loss: 0.651998\n",
                        "Epoch   40 | Train Loss: 0.640074\n",
                        "Epoch   50 | Train Loss: 0.628751\n",
                        "Epoch   60 | Train Loss: 0.617996\n",
                        "Epoch   70 | Train Loss: 0.607779\n",
                        "Epoch   80 | Train Loss: 0.598070\n",
                        "Epoch   90 | Train Loss: 0.588841\n",
                        "Epoch  100 | Train Loss: 0.580068\n",
                        "Epoch  110 | Train Loss: 0.571725\n",
                        "Epoch  120 | Train Loss: 0.563788\n",
                        "Epoch  130 | Train Loss: 0.556236\n",
                        "Epoch  140 | Train Loss: 0.549048\n",
                        "Epoch  150 | Train Loss: 0.542203\n",
                        "Epoch  160 | Train Loss: 0.535683\n",
                        "Epoch  170 | Train Loss: 0.529471\n",
                        "Epoch  180 | Train Loss: 0.523549\n",
                        "Epoch  190 | Train Loss: 0.517903\n",
                        "Epoch  200 | Train Loss: 0.512517\n",
                        "Epoch  210 | Train Loss: 0.507378\n",
                        "Epoch  220 | Train Loss: 0.502472\n",
                        "Epoch  230 | Train Loss: 0.497788\n",
                        "Epoch  240 | Train Loss: 0.493312\n",
                        "Epoch  250 | Train Loss: 0.489036\n",
                        "Epoch  260 | Train Loss: 0.484947\n",
                        "Epoch  270 | Train Loss: 0.481037\n",
                        "Epoch  280 | Train Loss: 0.477296\n",
                        "Epoch  290 | Train Loss: 0.473715\n",
                        "Epoch  300 | Train Loss: 0.470287\n",
                        "Epoch  310 | Train Loss: 0.467003\n",
                        "Epoch  320 | Train Loss: 0.463857\n",
                        "Epoch  330 | Train Loss: 0.460841\n",
                        "Epoch  340 | Train Loss: 0.457950\n",
                        "Epoch  350 | Train Loss: 0.455176\n",
                        "Epoch  360 | Train Loss: 0.452514\n",
                        "Epoch  370 | Train Loss: 0.449959\n",
                        "Epoch  380 | Train Loss: 0.447505\n",
                        "Epoch  390 | Train Loss: 0.445148\n",
                        "Epoch  400 | Train Loss: 0.442884\n",
                        "Epoch  410 | Train Loss: 0.440706\n",
                        "Epoch  420 | Train Loss: 0.438613\n",
                        "Epoch  430 | Train Loss: 0.436599\n",
                        "Epoch  440 | Train Loss: 0.434660\n",
                        "Epoch  450 | Train Loss: 0.432795\n",
                        "Epoch  460 | Train Loss: 0.430998\n",
                        "Epoch  470 | Train Loss: 0.429268\n",
                        "Epoch  480 | Train Loss: 0.427600\n",
                        "Epoch  490 | Train Loss: 0.425993\n",
                        "Epoch  499 | Train Loss: 0.424595\n",
                        "Lambda: 0.001  | Validation Loss: 0.425119\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677783\n",
                        "Epoch   20 | Train Loss: 0.664562\n",
                        "Epoch   30 | Train Loss: 0.652012\n",
                        "Epoch   40 | Train Loss: 0.640098\n",
                        "Epoch   50 | Train Loss: 0.628787\n",
                        "Epoch   60 | Train Loss: 0.618046\n",
                        "Epoch   70 | Train Loss: 0.607844\n",
                        "Epoch   80 | Train Loss: 0.598153\n",
                        "Epoch   90 | Train Loss: 0.588944\n",
                        "Epoch  100 | Train Loss: 0.580192\n",
                        "Epoch  110 | Train Loss: 0.571870\n",
                        "Epoch  120 | Train Loss: 0.563957\n",
                        "Epoch  130 | Train Loss: 0.556429\n",
                        "Epoch  140 | Train Loss: 0.549265\n",
                        "Epoch  150 | Train Loss: 0.542446\n",
                        "Epoch  160 | Train Loss: 0.535953\n",
                        "Epoch  170 | Train Loss: 0.529769\n",
                        "Epoch  180 | Train Loss: 0.523875\n",
                        "Epoch  190 | Train Loss: 0.518258\n",
                        "Epoch  200 | Train Loss: 0.512902\n",
                        "Epoch  210 | Train Loss: 0.507792\n",
                        "Epoch  220 | Train Loss: 0.502917\n",
                        "Epoch  230 | Train Loss: 0.498263\n",
                        "Epoch  240 | Train Loss: 0.493819\n",
                        "Epoch  250 | Train Loss: 0.489574\n",
                        "Epoch  260 | Train Loss: 0.485517\n",
                        "Epoch  270 | Train Loss: 0.481639\n",
                        "Epoch  280 | Train Loss: 0.477931\n",
                        "Epoch  290 | Train Loss: 0.474383\n",
                        "Epoch  300 | Train Loss: 0.470988\n",
                        "Epoch  310 | Train Loss: 0.467737\n",
                        "Epoch  320 | Train Loss: 0.464625\n",
                        "Epoch  330 | Train Loss: 0.461643\n",
                        "Epoch  340 | Train Loss: 0.458785\n",
                        "Epoch  350 | Train Loss: 0.456045\n",
                        "Epoch  360 | Train Loss: 0.453417\n",
                        "Epoch  370 | Train Loss: 0.450897\n",
                        "Epoch  380 | Train Loss: 0.448478\n",
                        "Epoch  390 | Train Loss: 0.446156\n",
                        "Epoch  400 | Train Loss: 0.443926\n",
                        "Epoch  410 | Train Loss: 0.441784\n",
                        "Epoch  420 | Train Loss: 0.439725\n",
                        "Epoch  430 | Train Loss: 0.437746\n",
                        "Epoch  440 | Train Loss: 0.435843\n",
                        "Epoch  450 | Train Loss: 0.434013\n",
                        "Epoch  460 | Train Loss: 0.432252\n",
                        "Epoch  470 | Train Loss: 0.430558\n",
                        "Epoch  480 | Train Loss: 0.428926\n",
                        "Epoch  490 | Train Loss: 0.427354\n",
                        "Epoch  499 | Train Loss: 0.425990\n",
                        "Lambda: 0.01   | Validation Loss: 0.426478\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677801\n",
                        "Epoch   20 | Train Loss: 0.664625\n",
                        "Epoch   30 | Train Loss: 0.652146\n",
                        "Epoch   40 | Train Loss: 0.640324\n",
                        "Epoch   50 | Train Loss: 0.629122\n",
                        "Epoch   60 | Train Loss: 0.618507\n",
                        "Epoch   70 | Train Loss: 0.608445\n",
                        "Epoch   80 | Train Loss: 0.598904\n",
                        "Epoch   90 | Train Loss: 0.589855\n",
                        "Epoch  100 | Train Loss: 0.581269\n",
                        "Epoch  110 | Train Loss: 0.573122\n",
                        "Epoch  120 | Train Loss: 0.565386\n",
                        "Epoch  130 | Train Loss: 0.558040\n",
                        "Epoch  140 | Train Loss: 0.551061\n",
                        "Epoch  150 | Train Loss: 0.544429\n",
                        "Epoch  160 | Train Loss: 0.538123\n",
                        "Epoch  170 | Train Loss: 0.532125\n",
                        "Epoch  180 | Train Loss: 0.526419\n",
                        "Epoch  190 | Train Loss: 0.520987\n",
                        "Epoch  200 | Train Loss: 0.515814\n",
                        "Epoch  210 | Train Loss: 0.510887\n",
                        "Epoch  220 | Train Loss: 0.506192\n",
                        "Epoch  230 | Train Loss: 0.501715\n",
                        "Epoch  240 | Train Loss: 0.497446\n",
                        "Epoch  250 | Train Loss: 0.493372\n",
                        "Epoch  260 | Train Loss: 0.489484\n",
                        "Epoch  270 | Train Loss: 0.485771\n",
                        "Epoch  280 | Train Loss: 0.482225\n",
                        "Epoch  290 | Train Loss: 0.478835\n",
                        "Epoch  300 | Train Loss: 0.475595\n",
                        "Epoch  310 | Train Loss: 0.472496\n",
                        "Epoch  320 | Train Loss: 0.469531\n",
                        "Epoch  330 | Train Loss: 0.466694\n",
                        "Epoch  340 | Train Loss: 0.463977\n",
                        "Epoch  350 | Train Loss: 0.461375\n",
                        "Epoch  360 | Train Loss: 0.458881\n",
                        "Epoch  370 | Train Loss: 0.456491\n",
                        "Epoch  380 | Train Loss: 0.454200\n",
                        "Epoch  390 | Train Loss: 0.452002\n",
                        "Epoch  400 | Train Loss: 0.449892\n",
                        "Epoch  410 | Train Loss: 0.447868\n",
                        "Epoch  420 | Train Loss: 0.445924\n",
                        "Epoch  430 | Train Loss: 0.444057\n",
                        "Epoch  440 | Train Loss: 0.442263\n",
                        "Epoch  450 | Train Loss: 0.440539\n",
                        "Epoch  460 | Train Loss: 0.438882\n",
                        "Epoch  470 | Train Loss: 0.437288\n",
                        "Epoch  480 | Train Loss: 0.435754\n",
                        "Epoch  490 | Train Loss: 0.434279\n",
                        "Epoch  499 | Train Loss: 0.432998\n",
                        "Lambda: 0.1    | Validation Loss: 0.433227\n",
                        "Epoch    0 | Train Loss: 0.691715\n",
                        "Epoch   10 | Train Loss: 0.677957\n",
                        "Epoch   20 | Train Loss: 0.665119\n",
                        "Epoch   30 | Train Loss: 0.653069\n",
                        "Epoch   40 | Train Loss: 0.641714\n",
                        "Epoch   50 | Train Loss: 0.630981\n",
                        "Epoch   60 | Train Loss: 0.620813\n",
                        "Epoch   70 | Train Loss: 0.611166\n",
                        "Epoch   80 | Train Loss: 0.602002\n",
                        "Epoch   90 | Train Loss: 0.593289\n",
                        "Epoch  100 | Train Loss: 0.584999\n",
                        "Epoch  110 | Train Loss: 0.577107\n",
                        "Epoch  120 | Train Loss: 0.569590\n",
                        "Epoch  130 | Train Loss: 0.562427\n",
                        "Epoch  140 | Train Loss: 0.555600\n",
                        "Epoch  150 | Train Loss: 0.549089\n",
                        "Epoch  160 | Train Loss: 0.542879\n",
                        "Epoch  170 | Train Loss: 0.536954\n",
                        "Epoch  180 | Train Loss: 0.531299\n",
                        "Epoch  190 | Train Loss: 0.525899\n",
                        "Epoch  200 | Train Loss: 0.520743\n",
                        "Epoch  210 | Train Loss: 0.515817\n",
                        "Epoch  220 | Train Loss: 0.511110\n",
                        "Epoch  230 | Train Loss: 0.506611\n",
                        "Epoch  240 | Train Loss: 0.502309\n",
                        "Epoch  250 | Train Loss: 0.498194\n",
                        "Epoch  260 | Train Loss: 0.494258\n",
                        "Epoch  270 | Train Loss: 0.490491\n",
                        "Epoch  280 | Train Loss: 0.486884\n",
                        "Epoch  290 | Train Loss: 0.483431\n",
                        "Epoch  300 | Train Loss: 0.480124\n",
                        "Epoch  310 | Train Loss: 0.476955\n",
                        "Epoch  320 | Train Loss: 0.473918\n",
                        "Epoch  330 | Train Loss: 0.471007\n",
                        "Epoch  340 | Train Loss: 0.468215\n",
                        "Epoch  350 | Train Loss: 0.465537\n",
                        "Epoch  360 | Train Loss: 0.462968\n",
                        "Epoch  370 | Train Loss: 0.460503\n",
                        "Epoch  380 | Train Loss: 0.458136\n",
                        "Epoch  390 | Train Loss: 0.455863\n",
                        "Epoch  400 | Train Loss: 0.453681\n",
                        "Epoch  410 | Train Loss: 0.451584\n",
                        "Epoch  420 | Train Loss: 0.449569\n",
                        "Epoch  430 | Train Loss: 0.447632\n",
                        "Epoch  440 | Train Loss: 0.445770\n",
                        "Epoch  450 | Train Loss: 0.443979\n",
                        "Epoch  460 | Train Loss: 0.442256\n",
                        "Epoch  470 | Train Loss: 0.440599\n",
                        "Epoch  480 | Train Loss: 0.439004\n",
                        "Epoch  490 | Train Loss: 0.437469\n",
                        "Epoch  499 | Train Loss: 0.436136\n",
                        "Lambda: 1      | Validation Loss: 0.435935\n",
                        "Epoch    0 | Train Loss: 0.691723\n",
                        "Epoch   10 | Train Loss: 0.678516\n",
                        "Epoch   20 | Train Loss: 0.666089\n",
                        "Epoch   30 | Train Loss: 0.654268\n",
                        "Epoch   40 | Train Loss: 0.643023\n",
                        "Epoch   50 | Train Loss: 0.632325\n",
                        "Epoch   60 | Train Loss: 0.622144\n",
                        "Epoch   70 | Train Loss: 0.612455\n",
                        "Epoch   80 | Train Loss: 0.603232\n",
                        "Epoch   90 | Train Loss: 0.594451\n",
                        "Epoch  100 | Train Loss: 0.586090\n",
                        "Epoch  110 | Train Loss: 0.578126\n",
                        "Epoch  120 | Train Loss: 0.570539\n",
                        "Epoch  130 | Train Loss: 0.563310\n",
                        "Epoch  140 | Train Loss: 0.556419\n",
                        "Epoch  150 | Train Loss: 0.549849\n",
                        "Epoch  160 | Train Loss: 0.543583\n",
                        "Epoch  170 | Train Loss: 0.537607\n",
                        "Epoch  180 | Train Loss: 0.531904\n",
                        "Epoch  190 | Train Loss: 0.526461\n",
                        "Epoch  200 | Train Loss: 0.521264\n",
                        "Epoch  210 | Train Loss: 0.516300\n",
                        "Epoch  220 | Train Loss: 0.511559\n",
                        "Epoch  230 | Train Loss: 0.507028\n",
                        "Epoch  240 | Train Loss: 0.502698\n",
                        "Epoch  250 | Train Loss: 0.498557\n",
                        "Epoch  260 | Train Loss: 0.494596\n",
                        "Epoch  270 | Train Loss: 0.490807\n",
                        "Epoch  280 | Train Loss: 0.487181\n",
                        "Epoch  290 | Train Loss: 0.483709\n",
                        "Epoch  300 | Train Loss: 0.480385\n",
                        "Epoch  310 | Train Loss: 0.477201\n",
                        "Epoch  320 | Train Loss: 0.474151\n",
                        "Epoch  330 | Train Loss: 0.471227\n",
                        "Epoch  340 | Train Loss: 0.468424\n",
                        "Epoch  350 | Train Loss: 0.465737\n",
                        "Epoch  360 | Train Loss: 0.463158\n",
                        "Epoch  370 | Train Loss: 0.460685\n",
                        "Epoch  380 | Train Loss: 0.458311\n",
                        "Epoch  390 | Train Loss: 0.456032\n",
                        "Epoch  400 | Train Loss: 0.453843\n",
                        "Epoch  410 | Train Loss: 0.451741\n",
                        "Epoch  420 | Train Loss: 0.449722\n",
                        "Epoch  430 | Train Loss: 0.447781\n",
                        "Epoch  440 | Train Loss: 0.445915\n",
                        "Epoch  450 | Train Loss: 0.444122\n",
                        "Epoch  460 | Train Loss: 0.442397\n",
                        "Epoch  470 | Train Loss: 0.440738\n",
                        "Epoch  480 | Train Loss: 0.439141\n",
                        "Epoch  490 | Train Loss: 0.437605\n",
                        "Epoch  499 | Train Loss: 0.436271\n",
                        "Lambda: 10     | Validation Loss: 0.435999\n",
                        "Best Lambda: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# Batch GD cross validation\n",
                "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
                "base_config = {\n",
                "    'lr': 0.01,\n",
                "    'epochs': 500,\n",
                "    'batch_size': None,\n",
                "    'method': 'batch',\n",
                "    'multiclass': False\n",
                "}\n",
                "\n",
                "best_lambda_batch, cv_results_batch = cross_validate(X_train_vec, y_train_bin, X_val_vec, y_val_bin, lambdas_to_test, base_config)\n",
                "\n",
                "# Capture best lambda for retraining and evaluation\n",
                "final_config_batch = base_config.copy()\n",
                "final_config_batch['lam'] = best_lambda_batch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Cross-Validation on 5 lambda candidates...\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677781\n",
                        "Epoch   20 | Train Loss: 0.664555\n",
                        "Epoch   30 | Train Loss: 0.651998\n",
                        "Epoch   40 | Train Loss: 0.640074\n",
                        "Epoch   50 | Train Loss: 0.628751\n",
                        "Epoch   60 | Train Loss: 0.617996\n",
                        "Epoch   70 | Train Loss: 0.607779\n",
                        "Epoch   80 | Train Loss: 0.598070\n",
                        "Epoch   90 | Train Loss: 0.588841\n",
                        "Epoch  100 | Train Loss: 0.580068\n",
                        "Epoch  110 | Train Loss: 0.571725\n",
                        "Epoch  120 | Train Loss: 0.563788\n",
                        "Epoch  130 | Train Loss: 0.556236\n",
                        "Epoch  140 | Train Loss: 0.549048\n",
                        "Epoch  150 | Train Loss: 0.542203\n",
                        "Epoch  160 | Train Loss: 0.535683\n",
                        "Epoch  170 | Train Loss: 0.529471\n",
                        "Epoch  180 | Train Loss: 0.523549\n",
                        "Epoch  190 | Train Loss: 0.517903\n",
                        "Epoch  200 | Train Loss: 0.512517\n",
                        "Epoch  210 | Train Loss: 0.507378\n",
                        "Epoch  220 | Train Loss: 0.502472\n",
                        "Epoch  230 | Train Loss: 0.497788\n",
                        "Epoch  240 | Train Loss: 0.493312\n",
                        "Epoch  250 | Train Loss: 0.489036\n",
                        "Epoch  260 | Train Loss: 0.484947\n",
                        "Epoch  270 | Train Loss: 0.481037\n",
                        "Epoch  280 | Train Loss: 0.477296\n",
                        "Epoch  290 | Train Loss: 0.473715\n",
                        "Epoch  300 | Train Loss: 0.470287\n",
                        "Epoch  310 | Train Loss: 0.467003\n",
                        "Epoch  320 | Train Loss: 0.463857\n",
                        "Epoch  330 | Train Loss: 0.460841\n",
                        "Epoch  340 | Train Loss: 0.457950\n",
                        "Epoch  350 | Train Loss: 0.455176\n",
                        "Epoch  360 | Train Loss: 0.452514\n",
                        "Epoch  370 | Train Loss: 0.449959\n",
                        "Epoch  380 | Train Loss: 0.447505\n",
                        "Epoch  390 | Train Loss: 0.445148\n",
                        "Epoch  400 | Train Loss: 0.442884\n",
                        "Epoch  410 | Train Loss: 0.440706\n",
                        "Epoch  420 | Train Loss: 0.438613\n",
                        "Epoch  430 | Train Loss: 0.436599\n",
                        "Epoch  440 | Train Loss: 0.434660\n",
                        "Epoch  450 | Train Loss: 0.432795\n",
                        "Epoch  460 | Train Loss: 0.430998\n",
                        "Epoch  470 | Train Loss: 0.429268\n",
                        "Epoch  480 | Train Loss: 0.427600\n",
                        "Epoch  490 | Train Loss: 0.425993\n",
                        "Epoch  499 | Train Loss: 0.424595\n",
                        "Lambda: 0.001  | Validation Loss: 0.425119\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677783\n",
                        "Epoch   20 | Train Loss: 0.664562\n",
                        "Epoch   30 | Train Loss: 0.652012\n",
                        "Epoch   40 | Train Loss: 0.640098\n",
                        "Epoch   50 | Train Loss: 0.628787\n",
                        "Epoch   60 | Train Loss: 0.618046\n",
                        "Epoch   70 | Train Loss: 0.607844\n",
                        "Epoch   80 | Train Loss: 0.598153\n",
                        "Epoch   90 | Train Loss: 0.588944\n",
                        "Epoch  100 | Train Loss: 0.580192\n",
                        "Epoch  110 | Train Loss: 0.571870\n",
                        "Epoch  120 | Train Loss: 0.563957\n",
                        "Epoch  130 | Train Loss: 0.556429\n",
                        "Epoch  140 | Train Loss: 0.549265\n",
                        "Epoch  150 | Train Loss: 0.542446\n",
                        "Epoch  160 | Train Loss: 0.535953\n",
                        "Epoch  170 | Train Loss: 0.529769\n",
                        "Epoch  180 | Train Loss: 0.523875\n",
                        "Epoch  190 | Train Loss: 0.518258\n",
                        "Epoch  200 | Train Loss: 0.512902\n",
                        "Epoch  210 | Train Loss: 0.507792\n",
                        "Epoch  220 | Train Loss: 0.502917\n",
                        "Epoch  230 | Train Loss: 0.498263\n",
                        "Epoch  240 | Train Loss: 0.493819\n",
                        "Epoch  250 | Train Loss: 0.489574\n",
                        "Epoch  260 | Train Loss: 0.485517\n",
                        "Epoch  270 | Train Loss: 0.481639\n",
                        "Epoch  280 | Train Loss: 0.477931\n",
                        "Epoch  290 | Train Loss: 0.474383\n",
                        "Epoch  300 | Train Loss: 0.470988\n",
                        "Epoch  310 | Train Loss: 0.467737\n",
                        "Epoch  320 | Train Loss: 0.464625\n",
                        "Epoch  330 | Train Loss: 0.461643\n",
                        "Epoch  340 | Train Loss: 0.458785\n",
                        "Epoch  350 | Train Loss: 0.456045\n",
                        "Epoch  360 | Train Loss: 0.453417\n",
                        "Epoch  370 | Train Loss: 0.450897\n",
                        "Epoch  380 | Train Loss: 0.448478\n",
                        "Epoch  390 | Train Loss: 0.446156\n",
                        "Epoch  400 | Train Loss: 0.443926\n",
                        "Epoch  410 | Train Loss: 0.441784\n",
                        "Epoch  420 | Train Loss: 0.439725\n",
                        "Epoch  430 | Train Loss: 0.437746\n",
                        "Epoch  440 | Train Loss: 0.435843\n",
                        "Epoch  450 | Train Loss: 0.434013\n",
                        "Epoch  460 | Train Loss: 0.432252\n",
                        "Epoch  470 | Train Loss: 0.430558\n",
                        "Epoch  480 | Train Loss: 0.428926\n",
                        "Epoch  490 | Train Loss: 0.427354\n",
                        "Epoch  499 | Train Loss: 0.425990\n",
                        "Lambda: 0.01   | Validation Loss: 0.426478\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677801\n",
                        "Epoch   20 | Train Loss: 0.664625\n",
                        "Epoch   30 | Train Loss: 0.652146\n",
                        "Epoch   40 | Train Loss: 0.640324\n",
                        "Epoch   50 | Train Loss: 0.629122\n",
                        "Epoch   60 | Train Loss: 0.618507\n",
                        "Epoch   70 | Train Loss: 0.608445\n",
                        "Epoch   80 | Train Loss: 0.598904\n",
                        "Epoch   90 | Train Loss: 0.589855\n",
                        "Epoch  100 | Train Loss: 0.581269\n",
                        "Epoch  110 | Train Loss: 0.573122\n",
                        "Epoch  120 | Train Loss: 0.565386\n",
                        "Epoch  130 | Train Loss: 0.558040\n",
                        "Epoch  140 | Train Loss: 0.551061\n",
                        "Epoch  150 | Train Loss: 0.544429\n",
                        "Epoch  160 | Train Loss: 0.538123\n",
                        "Epoch  170 | Train Loss: 0.532125\n",
                        "Epoch  180 | Train Loss: 0.526419\n",
                        "Epoch  190 | Train Loss: 0.520987\n",
                        "Epoch  200 | Train Loss: 0.515814\n",
                        "Epoch  210 | Train Loss: 0.510887\n",
                        "Epoch  220 | Train Loss: 0.506192\n",
                        "Epoch  230 | Train Loss: 0.501715\n",
                        "Epoch  240 | Train Loss: 0.497446\n",
                        "Epoch  250 | Train Loss: 0.493372\n",
                        "Epoch  260 | Train Loss: 0.489484\n",
                        "Epoch  270 | Train Loss: 0.485771\n",
                        "Epoch  280 | Train Loss: 0.482225\n",
                        "Epoch  290 | Train Loss: 0.478835\n",
                        "Epoch  300 | Train Loss: 0.475595\n",
                        "Epoch  310 | Train Loss: 0.472496\n",
                        "Epoch  320 | Train Loss: 0.469531\n",
                        "Epoch  330 | Train Loss: 0.466694\n",
                        "Epoch  340 | Train Loss: 0.463977\n",
                        "Epoch  350 | Train Loss: 0.461375\n",
                        "Epoch  360 | Train Loss: 0.458881\n",
                        "Epoch  370 | Train Loss: 0.456491\n",
                        "Epoch  380 | Train Loss: 0.454200\n",
                        "Epoch  390 | Train Loss: 0.452002\n",
                        "Epoch  400 | Train Loss: 0.449892\n",
                        "Epoch  410 | Train Loss: 0.447868\n",
                        "Epoch  420 | Train Loss: 0.445924\n",
                        "Epoch  430 | Train Loss: 0.444057\n",
                        "Epoch  440 | Train Loss: 0.442263\n",
                        "Epoch  450 | Train Loss: 0.440539\n",
                        "Epoch  460 | Train Loss: 0.438882\n",
                        "Epoch  470 | Train Loss: 0.437288\n",
                        "Epoch  480 | Train Loss: 0.435754\n",
                        "Epoch  490 | Train Loss: 0.434279\n",
                        "Epoch  499 | Train Loss: 0.432998\n",
                        "Lambda: 0.1    | Validation Loss: 0.433227\n",
                        "Epoch    0 | Train Loss: 0.691715\n",
                        "Epoch   10 | Train Loss: 0.677957\n",
                        "Epoch   20 | Train Loss: 0.665119\n",
                        "Epoch   30 | Train Loss: 0.653069\n",
                        "Epoch   40 | Train Loss: 0.641714\n",
                        "Epoch   50 | Train Loss: 0.630981\n",
                        "Epoch   60 | Train Loss: 0.620813\n",
                        "Epoch   70 | Train Loss: 0.611166\n",
                        "Epoch   80 | Train Loss: 0.602002\n",
                        "Epoch   90 | Train Loss: 0.593289\n",
                        "Epoch  100 | Train Loss: 0.584999\n",
                        "Epoch  110 | Train Loss: 0.577107\n",
                        "Epoch  120 | Train Loss: 0.569590\n",
                        "Epoch  130 | Train Loss: 0.562427\n",
                        "Epoch  140 | Train Loss: 0.555600\n",
                        "Epoch  150 | Train Loss: 0.549089\n",
                        "Epoch  160 | Train Loss: 0.542879\n",
                        "Epoch  170 | Train Loss: 0.536954\n",
                        "Epoch  180 | Train Loss: 0.531299\n",
                        "Epoch  190 | Train Loss: 0.525899\n",
                        "Epoch  200 | Train Loss: 0.520743\n",
                        "Epoch  210 | Train Loss: 0.515817\n",
                        "Epoch  220 | Train Loss: 0.511110\n",
                        "Epoch  230 | Train Loss: 0.506611\n",
                        "Epoch  240 | Train Loss: 0.502309\n",
                        "Epoch  250 | Train Loss: 0.498194\n",
                        "Epoch  260 | Train Loss: 0.494258\n",
                        "Epoch  270 | Train Loss: 0.490491\n",
                        "Epoch  280 | Train Loss: 0.486884\n",
                        "Epoch  290 | Train Loss: 0.483431\n",
                        "Epoch  300 | Train Loss: 0.480124\n",
                        "Epoch  310 | Train Loss: 0.476955\n",
                        "Epoch  320 | Train Loss: 0.473918\n",
                        "Epoch  330 | Train Loss: 0.471007\n",
                        "Epoch  340 | Train Loss: 0.468215\n",
                        "Epoch  350 | Train Loss: 0.465537\n",
                        "Epoch  360 | Train Loss: 0.462968\n",
                        "Epoch  370 | Train Loss: 0.460503\n",
                        "Epoch  380 | Train Loss: 0.458136\n",
                        "Epoch  390 | Train Loss: 0.455863\n",
                        "Epoch  400 | Train Loss: 0.453681\n",
                        "Epoch  410 | Train Loss: 0.451584\n",
                        "Epoch  420 | Train Loss: 0.449569\n",
                        "Epoch  430 | Train Loss: 0.447632\n",
                        "Epoch  440 | Train Loss: 0.445770\n",
                        "Epoch  450 | Train Loss: 0.443979\n",
                        "Epoch  460 | Train Loss: 0.442256\n",
                        "Epoch  470 | Train Loss: 0.440599\n",
                        "Epoch  480 | Train Loss: 0.439004\n",
                        "Epoch  490 | Train Loss: 0.437469\n",
                        "Epoch  499 | Train Loss: 0.436136\n",
                        "Lambda: 1      | Validation Loss: 0.435935\n",
                        "Epoch    0 | Train Loss: 0.691723\n",
                        "Epoch   10 | Train Loss: 0.678516\n",
                        "Epoch   20 | Train Loss: 0.666089\n",
                        "Epoch   30 | Train Loss: 0.654268\n",
                        "Epoch   40 | Train Loss: 0.643023\n",
                        "Epoch   50 | Train Loss: 0.632325\n",
                        "Epoch   60 | Train Loss: 0.622144\n",
                        "Epoch   70 | Train Loss: 0.612455\n",
                        "Epoch   80 | Train Loss: 0.603232\n",
                        "Epoch   90 | Train Loss: 0.594451\n",
                        "Epoch  100 | Train Loss: 0.586090\n",
                        "Epoch  110 | Train Loss: 0.578126\n",
                        "Epoch  120 | Train Loss: 0.570539\n",
                        "Epoch  130 | Train Loss: 0.563310\n",
                        "Epoch  140 | Train Loss: 0.556419\n",
                        "Epoch  150 | Train Loss: 0.549849\n",
                        "Epoch  160 | Train Loss: 0.543583\n",
                        "Epoch  170 | Train Loss: 0.537607\n",
                        "Epoch  180 | Train Loss: 0.531904\n",
                        "Epoch  190 | Train Loss: 0.526461\n",
                        "Epoch  200 | Train Loss: 0.521264\n",
                        "Epoch  210 | Train Loss: 0.516300\n",
                        "Epoch  220 | Train Loss: 0.511559\n",
                        "Epoch  230 | Train Loss: 0.507028\n",
                        "Epoch  240 | Train Loss: 0.502698\n",
                        "Epoch  250 | Train Loss: 0.498557\n",
                        "Epoch  260 | Train Loss: 0.494596\n",
                        "Epoch  270 | Train Loss: 0.490807\n",
                        "Epoch  280 | Train Loss: 0.487181\n",
                        "Epoch  290 | Train Loss: 0.483709\n",
                        "Epoch  300 | Train Loss: 0.480385\n",
                        "Epoch  310 | Train Loss: 0.477201\n",
                        "Epoch  320 | Train Loss: 0.474151\n",
                        "Epoch  330 | Train Loss: 0.471227\n",
                        "Epoch  340 | Train Loss: 0.468424\n",
                        "Epoch  350 | Train Loss: 0.465737\n",
                        "Epoch  360 | Train Loss: 0.463158\n",
                        "Epoch  370 | Train Loss: 0.460685\n",
                        "Epoch  380 | Train Loss: 0.458311\n",
                        "Epoch  390 | Train Loss: 0.456032\n",
                        "Epoch  400 | Train Loss: 0.453843\n",
                        "Epoch  410 | Train Loss: 0.451741\n",
                        "Epoch  420 | Train Loss: 0.449722\n",
                        "Epoch  430 | Train Loss: 0.447781\n",
                        "Epoch  440 | Train Loss: 0.445915\n",
                        "Epoch  450 | Train Loss: 0.444122\n",
                        "Epoch  460 | Train Loss: 0.442397\n",
                        "Epoch  470 | Train Loss: 0.440738\n",
                        "Epoch  480 | Train Loss: 0.439141\n",
                        "Epoch  490 | Train Loss: 0.437605\n",
                        "Epoch  499 | Train Loss: 0.436271\n",
                        "Lambda: 10     | Validation Loss: 0.435999\n",
                        "Best Lambda: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# SGD cross validation\n",
                "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
                "base_config_sgd = {\n",
                "    'lr': 0.001,\n",
                "    'epochs': 50,\n",
                "    'batch_size': 1,\n",
                "    'method': 'sgd',\n",
                "    'multiclass': False\n",
                "}\n",
                "\n",
                "best_lambda_sgd, cv_results_sgd = cross_validate(X_train_vec, y_train_bin, X_val_vec, y_val_bin, lambdas_to_test, base_config)\n",
                "\n",
                "# Capture best lambda for retraining and evaluation\n",
                "final_config_sgd = base_config_sgd.copy()\n",
                "final_config_sgd['lam'] = best_lambda_sgd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Cross-Validation on 5 lambda candidates...\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677781\n",
                        "Epoch   20 | Train Loss: 0.664555\n",
                        "Epoch   30 | Train Loss: 0.651998\n",
                        "Epoch   40 | Train Loss: 0.640074\n",
                        "Epoch   50 | Train Loss: 0.628751\n",
                        "Epoch   60 | Train Loss: 0.617996\n",
                        "Epoch   70 | Train Loss: 0.607779\n",
                        "Epoch   80 | Train Loss: 0.598070\n",
                        "Epoch   90 | Train Loss: 0.588841\n",
                        "Epoch  100 | Train Loss: 0.580068\n",
                        "Epoch  110 | Train Loss: 0.571725\n",
                        "Epoch  120 | Train Loss: 0.563788\n",
                        "Epoch  130 | Train Loss: 0.556236\n",
                        "Epoch  140 | Train Loss: 0.549048\n",
                        "Epoch  150 | Train Loss: 0.542203\n",
                        "Epoch  160 | Train Loss: 0.535683\n",
                        "Epoch  170 | Train Loss: 0.529471\n",
                        "Epoch  180 | Train Loss: 0.523549\n",
                        "Epoch  190 | Train Loss: 0.517903\n",
                        "Epoch  200 | Train Loss: 0.512517\n",
                        "Epoch  210 | Train Loss: 0.507378\n",
                        "Epoch  220 | Train Loss: 0.502472\n",
                        "Epoch  230 | Train Loss: 0.497788\n",
                        "Epoch  240 | Train Loss: 0.493312\n",
                        "Epoch  250 | Train Loss: 0.489036\n",
                        "Epoch  260 | Train Loss: 0.484947\n",
                        "Epoch  270 | Train Loss: 0.481037\n",
                        "Epoch  280 | Train Loss: 0.477296\n",
                        "Epoch  290 | Train Loss: 0.473715\n",
                        "Epoch  300 | Train Loss: 0.470287\n",
                        "Epoch  310 | Train Loss: 0.467003\n",
                        "Epoch  320 | Train Loss: 0.463857\n",
                        "Epoch  330 | Train Loss: 0.460841\n",
                        "Epoch  340 | Train Loss: 0.457950\n",
                        "Epoch  350 | Train Loss: 0.455176\n",
                        "Epoch  360 | Train Loss: 0.452514\n",
                        "Epoch  370 | Train Loss: 0.449959\n",
                        "Epoch  380 | Train Loss: 0.447505\n",
                        "Epoch  390 | Train Loss: 0.445148\n",
                        "Epoch  400 | Train Loss: 0.442884\n",
                        "Epoch  410 | Train Loss: 0.440706\n",
                        "Epoch  420 | Train Loss: 0.438613\n",
                        "Epoch  430 | Train Loss: 0.436599\n",
                        "Epoch  440 | Train Loss: 0.434660\n",
                        "Epoch  450 | Train Loss: 0.432795\n",
                        "Epoch  460 | Train Loss: 0.430998\n",
                        "Epoch  470 | Train Loss: 0.429268\n",
                        "Epoch  480 | Train Loss: 0.427600\n",
                        "Epoch  490 | Train Loss: 0.425993\n",
                        "Epoch  499 | Train Loss: 0.424595\n",
                        "Lambda: 0.001  | Validation Loss: 0.425119\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677783\n",
                        "Epoch   20 | Train Loss: 0.664562\n",
                        "Epoch   30 | Train Loss: 0.652012\n",
                        "Epoch   40 | Train Loss: 0.640098\n",
                        "Epoch   50 | Train Loss: 0.628787\n",
                        "Epoch   60 | Train Loss: 0.618046\n",
                        "Epoch   70 | Train Loss: 0.607844\n",
                        "Epoch   80 | Train Loss: 0.598153\n",
                        "Epoch   90 | Train Loss: 0.588944\n",
                        "Epoch  100 | Train Loss: 0.580192\n",
                        "Epoch  110 | Train Loss: 0.571870\n",
                        "Epoch  120 | Train Loss: 0.563957\n",
                        "Epoch  130 | Train Loss: 0.556429\n",
                        "Epoch  140 | Train Loss: 0.549265\n",
                        "Epoch  150 | Train Loss: 0.542446\n",
                        "Epoch  160 | Train Loss: 0.535953\n",
                        "Epoch  170 | Train Loss: 0.529769\n",
                        "Epoch  180 | Train Loss: 0.523875\n",
                        "Epoch  190 | Train Loss: 0.518258\n",
                        "Epoch  200 | Train Loss: 0.512902\n",
                        "Epoch  210 | Train Loss: 0.507792\n",
                        "Epoch  220 | Train Loss: 0.502917\n",
                        "Epoch  230 | Train Loss: 0.498263\n",
                        "Epoch  240 | Train Loss: 0.493819\n",
                        "Epoch  250 | Train Loss: 0.489574\n",
                        "Epoch  260 | Train Loss: 0.485517\n",
                        "Epoch  270 | Train Loss: 0.481639\n",
                        "Epoch  280 | Train Loss: 0.477931\n",
                        "Epoch  290 | Train Loss: 0.474383\n",
                        "Epoch  300 | Train Loss: 0.470988\n",
                        "Epoch  310 | Train Loss: 0.467737\n",
                        "Epoch  320 | Train Loss: 0.464625\n",
                        "Epoch  330 | Train Loss: 0.461643\n",
                        "Epoch  340 | Train Loss: 0.458785\n",
                        "Epoch  350 | Train Loss: 0.456045\n",
                        "Epoch  360 | Train Loss: 0.453417\n",
                        "Epoch  370 | Train Loss: 0.450897\n",
                        "Epoch  380 | Train Loss: 0.448478\n",
                        "Epoch  390 | Train Loss: 0.446156\n",
                        "Epoch  400 | Train Loss: 0.443926\n",
                        "Epoch  410 | Train Loss: 0.441784\n",
                        "Epoch  420 | Train Loss: 0.439725\n",
                        "Epoch  430 | Train Loss: 0.437746\n",
                        "Epoch  440 | Train Loss: 0.435843\n",
                        "Epoch  450 | Train Loss: 0.434013\n",
                        "Epoch  460 | Train Loss: 0.432252\n",
                        "Epoch  470 | Train Loss: 0.430558\n",
                        "Epoch  480 | Train Loss: 0.428926\n",
                        "Epoch  490 | Train Loss: 0.427354\n",
                        "Epoch  499 | Train Loss: 0.425990\n",
                        "Lambda: 0.01   | Validation Loss: 0.426478\n",
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677801\n",
                        "Epoch   20 | Train Loss: 0.664625\n",
                        "Epoch   30 | Train Loss: 0.652146\n",
                        "Epoch   40 | Train Loss: 0.640324\n",
                        "Epoch   50 | Train Loss: 0.629122\n",
                        "Epoch   60 | Train Loss: 0.618507\n",
                        "Epoch   70 | Train Loss: 0.608445\n",
                        "Epoch   80 | Train Loss: 0.598904\n",
                        "Epoch   90 | Train Loss: 0.589855\n",
                        "Epoch  100 | Train Loss: 0.581269\n",
                        "Epoch  110 | Train Loss: 0.573122\n",
                        "Epoch  120 | Train Loss: 0.565386\n",
                        "Epoch  130 | Train Loss: 0.558040\n",
                        "Epoch  140 | Train Loss: 0.551061\n",
                        "Epoch  150 | Train Loss: 0.544429\n",
                        "Epoch  160 | Train Loss: 0.538123\n",
                        "Epoch  170 | Train Loss: 0.532125\n",
                        "Epoch  180 | Train Loss: 0.526419\n",
                        "Epoch  190 | Train Loss: 0.520987\n",
                        "Epoch  200 | Train Loss: 0.515814\n",
                        "Epoch  210 | Train Loss: 0.510887\n",
                        "Epoch  220 | Train Loss: 0.506192\n",
                        "Epoch  230 | Train Loss: 0.501715\n",
                        "Epoch  240 | Train Loss: 0.497446\n",
                        "Epoch  250 | Train Loss: 0.493372\n",
                        "Epoch  260 | Train Loss: 0.489484\n",
                        "Epoch  270 | Train Loss: 0.485771\n",
                        "Epoch  280 | Train Loss: 0.482225\n",
                        "Epoch  290 | Train Loss: 0.478835\n",
                        "Epoch  300 | Train Loss: 0.475595\n",
                        "Epoch  310 | Train Loss: 0.472496\n",
                        "Epoch  320 | Train Loss: 0.469531\n",
                        "Epoch  330 | Train Loss: 0.466694\n",
                        "Epoch  340 | Train Loss: 0.463977\n",
                        "Epoch  350 | Train Loss: 0.461375\n",
                        "Epoch  360 | Train Loss: 0.458881\n",
                        "Epoch  370 | Train Loss: 0.456491\n",
                        "Epoch  380 | Train Loss: 0.454200\n",
                        "Epoch  390 | Train Loss: 0.452002\n",
                        "Epoch  400 | Train Loss: 0.449892\n",
                        "Epoch  410 | Train Loss: 0.447868\n",
                        "Epoch  420 | Train Loss: 0.445924\n",
                        "Epoch  430 | Train Loss: 0.444057\n",
                        "Epoch  440 | Train Loss: 0.442263\n",
                        "Epoch  450 | Train Loss: 0.440539\n",
                        "Epoch  460 | Train Loss: 0.438882\n",
                        "Epoch  470 | Train Loss: 0.437288\n",
                        "Epoch  480 | Train Loss: 0.435754\n",
                        "Epoch  490 | Train Loss: 0.434279\n",
                        "Epoch  499 | Train Loss: 0.432998\n",
                        "Lambda: 0.1    | Validation Loss: 0.433227\n",
                        "Epoch    0 | Train Loss: 0.691715\n",
                        "Epoch   10 | Train Loss: 0.677957\n",
                        "Epoch   20 | Train Loss: 0.665119\n",
                        "Epoch   30 | Train Loss: 0.653069\n",
                        "Epoch   40 | Train Loss: 0.641714\n",
                        "Epoch   50 | Train Loss: 0.630981\n",
                        "Epoch   60 | Train Loss: 0.620813\n",
                        "Epoch   70 | Train Loss: 0.611166\n",
                        "Epoch   80 | Train Loss: 0.602002\n",
                        "Epoch   90 | Train Loss: 0.593289\n",
                        "Epoch  100 | Train Loss: 0.584999\n",
                        "Epoch  110 | Train Loss: 0.577107\n",
                        "Epoch  120 | Train Loss: 0.569590\n",
                        "Epoch  130 | Train Loss: 0.562427\n",
                        "Epoch  140 | Train Loss: 0.555600\n",
                        "Epoch  150 | Train Loss: 0.549089\n",
                        "Epoch  160 | Train Loss: 0.542879\n",
                        "Epoch  170 | Train Loss: 0.536954\n",
                        "Epoch  180 | Train Loss: 0.531299\n",
                        "Epoch  190 | Train Loss: 0.525899\n",
                        "Epoch  200 | Train Loss: 0.520743\n",
                        "Epoch  210 | Train Loss: 0.515817\n",
                        "Epoch  220 | Train Loss: 0.511110\n",
                        "Epoch  230 | Train Loss: 0.506611\n",
                        "Epoch  240 | Train Loss: 0.502309\n",
                        "Epoch  250 | Train Loss: 0.498194\n",
                        "Epoch  260 | Train Loss: 0.494258\n",
                        "Epoch  270 | Train Loss: 0.490491\n",
                        "Epoch  280 | Train Loss: 0.486884\n",
                        "Epoch  290 | Train Loss: 0.483431\n",
                        "Epoch  300 | Train Loss: 0.480124\n",
                        "Epoch  310 | Train Loss: 0.476955\n",
                        "Epoch  320 | Train Loss: 0.473918\n",
                        "Epoch  330 | Train Loss: 0.471007\n",
                        "Epoch  340 | Train Loss: 0.468215\n",
                        "Epoch  350 | Train Loss: 0.465537\n",
                        "Epoch  360 | Train Loss: 0.462968\n",
                        "Epoch  370 | Train Loss: 0.460503\n",
                        "Epoch  380 | Train Loss: 0.458136\n",
                        "Epoch  390 | Train Loss: 0.455863\n",
                        "Epoch  400 | Train Loss: 0.453681\n",
                        "Epoch  410 | Train Loss: 0.451584\n",
                        "Epoch  420 | Train Loss: 0.449569\n",
                        "Epoch  430 | Train Loss: 0.447632\n",
                        "Epoch  440 | Train Loss: 0.445770\n",
                        "Epoch  450 | Train Loss: 0.443979\n",
                        "Epoch  460 | Train Loss: 0.442256\n",
                        "Epoch  470 | Train Loss: 0.440599\n",
                        "Epoch  480 | Train Loss: 0.439004\n",
                        "Epoch  490 | Train Loss: 0.437469\n",
                        "Epoch  499 | Train Loss: 0.436136\n",
                        "Lambda: 1      | Validation Loss: 0.435935\n",
                        "Epoch    0 | Train Loss: 0.691723\n",
                        "Epoch   10 | Train Loss: 0.678516\n",
                        "Epoch   20 | Train Loss: 0.666089\n",
                        "Epoch   30 | Train Loss: 0.654268\n",
                        "Epoch   40 | Train Loss: 0.643023\n",
                        "Epoch   50 | Train Loss: 0.632325\n",
                        "Epoch   60 | Train Loss: 0.622144\n",
                        "Epoch   70 | Train Loss: 0.612455\n",
                        "Epoch   80 | Train Loss: 0.603232\n",
                        "Epoch   90 | Train Loss: 0.594451\n",
                        "Epoch  100 | Train Loss: 0.586090\n",
                        "Epoch  110 | Train Loss: 0.578126\n",
                        "Epoch  120 | Train Loss: 0.570539\n",
                        "Epoch  130 | Train Loss: 0.563310\n",
                        "Epoch  140 | Train Loss: 0.556419\n",
                        "Epoch  150 | Train Loss: 0.549849\n",
                        "Epoch  160 | Train Loss: 0.543583\n",
                        "Epoch  170 | Train Loss: 0.537607\n",
                        "Epoch  180 | Train Loss: 0.531904\n",
                        "Epoch  190 | Train Loss: 0.526461\n",
                        "Epoch  200 | Train Loss: 0.521264\n",
                        "Epoch  210 | Train Loss: 0.516300\n",
                        "Epoch  220 | Train Loss: 0.511559\n",
                        "Epoch  230 | Train Loss: 0.507028\n",
                        "Epoch  240 | Train Loss: 0.502698\n",
                        "Epoch  250 | Train Loss: 0.498557\n",
                        "Epoch  260 | Train Loss: 0.494596\n",
                        "Epoch  270 | Train Loss: 0.490807\n",
                        "Epoch  280 | Train Loss: 0.487181\n",
                        "Epoch  290 | Train Loss: 0.483709\n",
                        "Epoch  300 | Train Loss: 0.480385\n",
                        "Epoch  310 | Train Loss: 0.477201\n",
                        "Epoch  320 | Train Loss: 0.474151\n",
                        "Epoch  330 | Train Loss: 0.471227\n",
                        "Epoch  340 | Train Loss: 0.468424\n",
                        "Epoch  350 | Train Loss: 0.465737\n",
                        "Epoch  360 | Train Loss: 0.463158\n",
                        "Epoch  370 | Train Loss: 0.460685\n",
                        "Epoch  380 | Train Loss: 0.458311\n",
                        "Epoch  390 | Train Loss: 0.456032\n",
                        "Epoch  400 | Train Loss: 0.453843\n",
                        "Epoch  410 | Train Loss: 0.451741\n",
                        "Epoch  420 | Train Loss: 0.449722\n",
                        "Epoch  430 | Train Loss: 0.447781\n",
                        "Epoch  440 | Train Loss: 0.445915\n",
                        "Epoch  450 | Train Loss: 0.444122\n",
                        "Epoch  460 | Train Loss: 0.442397\n",
                        "Epoch  470 | Train Loss: 0.440738\n",
                        "Epoch  480 | Train Loss: 0.439141\n",
                        "Epoch  490 | Train Loss: 0.437605\n",
                        "Epoch  499 | Train Loss: 0.436271\n",
                        "Lambda: 10     | Validation Loss: 0.435999\n",
                        "Best Lambda: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# Mini-batch cross validation\n",
                "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
                "base_config_mini = {\n",
                "    'lr': 0.01,\n",
                "    'epochs': 500,\n",
                "    'batch_size': 32,\n",
                "    'method': 'minibatch',\n",
                "    'multiclass': False\n",
                "}\n",
                "\n",
                "best_lambda_mini, cv_results_mini = cross_validate(X_train_vec, y_train_bin, X_val_vec, y_val_bin, lambdas_to_test, base_config)\n",
                "\n",
                "# Capture best lambda for retraining and evaluation\n",
                "final_config_mini = base_config_mini.copy()\n",
                "final_config_mini['lam'] = best_lambda_mini"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we \"retrain\" each model type with the best lambda and evaluate the output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch    0 | Train Loss: 0.691714\n",
                        "Epoch   10 | Train Loss: 0.677781\n",
                        "Epoch   20 | Train Loss: 0.664555\n",
                        "Epoch   30 | Train Loss: 0.651998\n",
                        "Epoch   40 | Train Loss: 0.640074\n",
                        "Epoch   50 | Train Loss: 0.628751\n",
                        "Epoch   60 | Train Loss: 0.617996\n",
                        "Epoch   70 | Train Loss: 0.607779\n",
                        "Epoch   80 | Train Loss: 0.598070\n",
                        "Epoch   90 | Train Loss: 0.588841\n",
                        "Epoch  100 | Train Loss: 0.580068\n",
                        "Epoch  110 | Train Loss: 0.571725\n",
                        "Epoch  120 | Train Loss: 0.563788\n",
                        "Epoch  130 | Train Loss: 0.556236\n",
                        "Epoch  140 | Train Loss: 0.549048\n",
                        "Epoch  150 | Train Loss: 0.542203\n",
                        "Epoch  160 | Train Loss: 0.535683\n",
                        "Epoch  170 | Train Loss: 0.529471\n",
                        "Epoch  180 | Train Loss: 0.523549\n",
                        "Epoch  190 | Train Loss: 0.517903\n",
                        "Epoch  200 | Train Loss: 0.512517\n",
                        "Epoch  210 | Train Loss: 0.507378\n",
                        "Epoch  220 | Train Loss: 0.502472\n",
                        "Epoch  230 | Train Loss: 0.497788\n",
                        "Epoch  240 | Train Loss: 0.493312\n",
                        "Epoch  250 | Train Loss: 0.489036\n",
                        "Epoch  260 | Train Loss: 0.484947\n",
                        "Epoch  270 | Train Loss: 0.481037\n",
                        "Epoch  280 | Train Loss: 0.477296\n",
                        "Epoch  290 | Train Loss: 0.473715\n",
                        "Epoch  300 | Train Loss: 0.470287\n",
                        "Epoch  310 | Train Loss: 0.467003\n",
                        "Epoch  320 | Train Loss: 0.463857\n",
                        "Epoch  330 | Train Loss: 0.460841\n",
                        "Epoch  340 | Train Loss: 0.457950\n",
                        "Epoch  350 | Train Loss: 0.455176\n",
                        "Epoch  360 | Train Loss: 0.452514\n",
                        "Epoch  370 | Train Loss: 0.449959\n",
                        "Epoch  380 | Train Loss: 0.447505\n",
                        "Epoch  390 | Train Loss: 0.445148\n",
                        "Epoch  400 | Train Loss: 0.442884\n",
                        "Epoch  410 | Train Loss: 0.440706\n",
                        "Epoch  420 | Train Loss: 0.438613\n",
                        "Epoch  430 | Train Loss: 0.436599\n",
                        "Epoch  440 | Train Loss: 0.434660\n",
                        "Epoch  450 | Train Loss: 0.432795\n",
                        "Epoch  460 | Train Loss: 0.430998\n",
                        "Epoch  470 | Train Loss: 0.429268\n",
                        "Epoch  480 | Train Loss: 0.427600\n",
                        "Epoch  490 | Train Loss: 0.425993\n",
                        "Epoch  499 | Train Loss: 0.424595\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<__main__.LogisticRegressionModel at 0x12bb37eb0>"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "batch_model = LogisticRegressionModel(**final_config_batch)\n",
                "batch_model.fit(X_train_vec, y_train_bin)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch    0 | Train Loss: 0.460685\n",
                        "Epoch   10 | Train Loss: 0.349961\n",
                        "Epoch   20 | Train Loss: 0.323132\n",
                        "Epoch   30 | Train Loss: 0.303094\n",
                        "Epoch   40 | Train Loss: 0.288225\n",
                        "Epoch   49 | Train Loss: 0.278172\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<__main__.LogisticRegressionModel at 0x12bb34760>"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sgd_model = LogisticRegressionModel(**final_config_sgd)\n",
                "sgd_model.fit(X_train_vec, y_train_bin)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch    0 | Train Loss: 0.576598\n",
                        "Epoch   10 | Train Loss: 0.381383\n",
                        "Epoch   20 | Train Loss: 0.364771\n",
                        "Epoch   30 | Train Loss: 0.353968\n",
                        "Epoch   40 | Train Loss: 0.344349\n",
                        "Epoch   50 | Train Loss: 0.335564\n",
                        "Epoch   60 | Train Loss: 0.327538\n",
                        "Epoch   70 | Train Loss: 0.320205\n",
                        "Epoch   80 | Train Loss: 0.313532\n",
                        "Epoch   90 | Train Loss: 0.307440\n",
                        "Epoch  100 | Train Loss: 0.301890\n",
                        "Epoch  110 | Train Loss: 0.296846\n",
                        "Epoch  120 | Train Loss: 0.292254\n",
                        "Epoch  130 | Train Loss: 0.288070\n",
                        "Epoch  140 | Train Loss: 0.284264\n",
                        "Epoch  150 | Train Loss: 0.280794\n",
                        "Epoch  160 | Train Loss: 0.277645\n",
                        "Epoch  170 | Train Loss: 0.274780\n",
                        "Epoch  180 | Train Loss: 0.272167\n",
                        "Epoch  190 | Train Loss: 0.269787\n",
                        "Epoch  200 | Train Loss: 0.267623\n",
                        "Epoch  210 | Train Loss: 0.265646\n",
                        "Epoch  220 | Train Loss: 0.263841\n",
                        "Epoch  230 | Train Loss: 0.262198\n",
                        "Epoch  240 | Train Loss: 0.260698\n",
                        "Epoch  250 | Train Loss: 0.259321\n",
                        "Epoch  260 | Train Loss: 0.258070\n",
                        "Epoch  270 | Train Loss: 0.256926\n",
                        "Epoch  280 | Train Loss: 0.255882\n",
                        "Epoch  290 | Train Loss: 0.254923\n",
                        "Epoch  300 | Train Loss: 0.254046\n",
                        "Epoch  310 | Train Loss: 0.253243\n",
                        "Epoch  320 | Train Loss: 0.252509\n",
                        "Epoch  330 | Train Loss: 0.251834\n",
                        "Epoch  340 | Train Loss: 0.251215\n",
                        "Epoch  350 | Train Loss: 0.250647\n",
                        "Epoch  360 | Train Loss: 0.250126\n",
                        "Epoch  370 | Train Loss: 0.249648\n",
                        "Epoch  380 | Train Loss: 0.249208\n",
                        "Epoch  390 | Train Loss: 0.248804\n",
                        "Epoch  400 | Train Loss: 0.248432\n",
                        "Epoch  410 | Train Loss: 0.248090\n",
                        "Epoch  420 | Train Loss: 0.247774\n",
                        "Epoch  430 | Train Loss: 0.247484\n",
                        "Epoch  440 | Train Loss: 0.247217\n",
                        "Epoch  450 | Train Loss: 0.246970\n",
                        "Epoch  460 | Train Loss: 0.246743\n",
                        "Epoch  470 | Train Loss: 0.246533\n",
                        "Epoch  480 | Train Loss: 0.246340\n",
                        "Epoch  490 | Train Loss: 0.246162\n",
                        "Epoch  499 | Train Loss: 0.246013\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<__main__.LogisticRegressionModel at 0x12bb36c80>"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mini_model = LogisticRegressionModel(**final_config_mini)\n",
                "mini_model.fit(X_train_vec, y_train_bin)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Make an evaluation function for binary and multiclass cases for accuracy, precision, recall and F1 score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(y_true, y_pred, is_multiclass=False):\n",
                "    metrics = {}\n",
                "    \n",
                "    if not is_multiclass:\n",
                "        # Binary Logic\n",
                "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
                "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
                "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
                "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
                "\n",
                "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "        accuracy = (tp + tn) / len(y_true)\n",
                "\n",
                "        metrics = {\n",
                "            \"accuracy\": accuracy,\n",
                "            \"precision\": precision,\n",
                "            \"recall\": recall,\n",
                "            \"f1_score\": f1\n",
                "        }\n",
                "    else:\n",
                "        # Multiclass Logic (Macro-averaged)\n",
                "        classes = np.unique(y_true)\n",
                "        acc = np.mean(y_true == y_pred)\n",
                "        \n",
                "        precisions, recalls, f1s = [], [], []\n",
                "        \n",
                "        for c in classes:\n",
                "            tp = np.sum((y_true == c) & (y_pred == c))\n",
                "            fp = np.sum((y_true != c) & (y_pred == c))\n",
                "            fn = np.sum((y_true == c) & (y_pred != c))\n",
                "            \n",
                "            p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "            r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "            f = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
                "            \n",
                "            precisions.append(p)\n",
                "            recalls.append(r)\n",
                "            f1s.append(f)\n",
                "            \n",
                "        metrics = {\n",
                "            \"accuracy\": acc,\n",
                "            \"precision\": np.mean(precisions),\n",
                "            \"recall\": np.mean(recalls),\n",
                "            \"f1_score\": np.mean(f1s)\n",
                "        }\n",
                "\n",
                "    # Print Results\n",
                "    print(f\"{'Multiclass' if is_multiclass else 'Binary'} Results\")\n",
                "    for k, v in metrics.items():\n",
                "        print(f\"{k.capitalize():10}: {v:.4f}\")\n",
                "        \n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we predict and evaluate each models performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Binary Results\n",
                        "Accuracy  : 0.8054\n",
                        "Precision : 0.4118\n",
                        "Recall    : 0.9545\n",
                        "F1_score  : 0.5753\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation of GD\n",
                "preds_batch = batch_model.predict(X_test_vec, threshold=0.25)\n",
                "results_batch = evaluate_model(y_test_bin, preds_batch, is_multiclass=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Binary Results\n",
                        "Accuracy  : 0.9677\n",
                        "Precision : 0.9470\n",
                        "Recall    : 0.8117\n",
                        "F1_score  : 0.8741\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation of SGD\n",
                "preds_sgd = sgd_model.predict(X_test_vec, threshold=0.2)\n",
                "results_sgd = evaluate_model(y_test_bin, preds_sgd, is_multiclass=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Binary Results\n",
                        "Accuracy  : 0.9704\n",
                        "Precision : 0.8903\n",
                        "Recall    : 0.8961\n",
                        "F1_score  : 0.8932\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation of Mini-Batch GD\n",
                "preds_minibatch = mini_model.predict(X_test_vec, threshold=0.2)\n",
                "results_mini = evaluate_model(y_test_bin, preds_minibatch, is_multiclass=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Analysis of the final results of the logistic regreesion model:\n",
                "    -The first thing to note is the distribution of the data, roughly 85% of the data is ham vs 15% is spam. This class inbalance could cause the model to want to predict ham correctly as positive or negative but over perform in the correct prediction for the negative class(spam). In future versions of this model sampling from the data to create a more balanced dataset could be implemented to avoid potenital overfitting issues to the ham class. \n",
                "\n",
                "    -One common ground between the models is the lambda parameter, all 3 models used lambda = 0.001 as they yield the best loss during training. The ability for all three training methods to minimize the objective function decreased as the lambda parameters increased, this is expected as the regularization strength increased causing the regularization term to become larger and larger weights to grow in size. \n",
                "\n",
                "    -When looking at the final results we also see a trend when it comes to the threshold parameter when making predictions. Due to the large class imbalance all three models require the threshold for predictions to be lower (0.25<) to yield the best results. In a balanced class case 0.5 is the optimal threshold as the model can distinguish between the two classes without any help. In the case of a large class imbalance, the model is forced to predict the majority class more often. So by setting the threshold to 0.25 or lower we are saying that if the model makes a prediction below 0.25 we will predict the negative class. \n",
                "    \n",
                "    -The gradient descent model performs poorly relative to the other models. While it achieves high recall (0.9545) at a low threshold of 0.25, indicating strong detection of positive cases, this comes at the cost of low precision (0.4118), resulting in many false positives. Although the accuracy is 0.8054, this metric is misleading due to class imbalance. The moderate F1 score of 0.5753 highlights the tradeoff between recall and precision and suggests that the model is not well balanced. As mentioned previously removing a class imbalance and redfining with a smaller dataset could allow for the model to generalize more effectively even on a smaller sample. This behavior could also be defined by the training method as we are training on the entire dataset so the minority class does not carry as much influence on the weighs of the final model.\n",
                "    \n",
                "    -For the stochastic gradient descent model we see a huge change is the models performance. We have an accuracy of 0.9677 indicating we are prediciting the correct class 96.77% of the time. We also have a high precision of 0.947 indicating we are predicting very few false positives. Now this comes at the cost of recall being roughly 0.8117 indicating we are generating more false negatives compared to the gradient descent model. Overall this yield a much more balanced model as shown by the F1 score of 0.8741. One thing to point out is for this type of training method, the epochs were lowered from 500 down to 50, this allowed for very fast training with a reduced risk of overfitting. This occurs due to the weights be updated per sample indicating more updatets per epoch resulting in the model being able to learn patterns in the data more effectively when compared to the gradient descent model. \n",
                "    \n",
                "    -The final model uses a mini-batch gradient descent approach and is the most balanced of the three models. It achieves a high accuracy of 0.9704, indicating that approximately 97% of all samples are classified correctly. Both precision and recall are close to 0.89, showing that the model maintains a low rate of false positives while also correctly identifying most true positive cases. This balance between precision and recall is reflected in the high F1 score of 0.8932, indicating strong and consistent performance across both classes. The mini-batch training approach allows for more stable weight updates compared to stochastic gradient descent, reducing gradient noise during training. As a result, the model was able to train for a larger number of epochs without overfitting, ultimately converging to a better solution than the SGD model.\n",
                "\n",
                "    -When it comes to improvements to be made in the future, the main takeaway is the more the weights are updated the better the model performs, but there must be a proper balance. The sgd model was updated per samples resulting in over 3000 updates per epoch, while the mini-batch model only updated for every 32 samples resulting in 100 updates per epoch, meaning even with the number of epochs being 10x for the mini-batch model the total number of updates was about 1/3 of the sgd model. This allowed for the mini-batch model to train with less noise and resulting in a more balanced model. When it comes to balancing the dataset the goal should be to have the decision boundary for a binary classification problem be as close to the center of the data as possible. So if sampling from the data to create a dataset where we have roughly a 60/40 split between the positive and negative cases or even 50/50 split would most likely return a model where the decision boundary does not need to be corrected to yield good results. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "workflow_multi",
            "metadata": {},
            "source": [
                "## 5. Multiclass Classification (Books)\n",
                "<br> We must split the data, preprocess, vectorize and train the models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "load_books",
            "metadata": {},
            "outputs": [],
            "source": [
                "books_df = pd.read_csv('a1-data/books.txt', sep='\\t', header=None, names=['label', 'text'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into training, validation and testing sets\n",
                "X_train_b, X_val_b, X_test_b, y_train_b, y_val_b, y_test_b = split_dataset(books_df, 0.6, 0.2, 0.2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Train</th>\n",
                            "      <th>Val</th>\n",
                            "      <th>Test</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>label</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>Jane Austen</th>\n",
                            "      <td>6610</td>\n",
                            "      <td>2200</td>\n",
                            "      <td>2172</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Fyodor Dostoyevsky</th>\n",
                            "      <td>3555</td>\n",
                            "      <td>1189</td>\n",
                            "      <td>1200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Arthur Conan Doyle</th>\n",
                            "      <td>1382</td>\n",
                            "      <td>460</td>\n",
                            "      <td>478</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                    Train   Val  Test\n",
                            "label                                \n",
                            "Jane Austen          6610  2200  2172\n",
                            "Fyodor Dostoyevsky   3555  1189  1200\n",
                            "Arthur Conan Doyle   1382   460   478"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def data_distribution(y_train, y_val, y_test):\n",
                "    df =pd.DataFrame({'Train': y_train.value_counts(), 'Val': y_val.value_counts(), 'Test': y_test.value_counts()}).fillna(0).astype(int)\n",
                "    return df\n",
                "\n",
                "data_distribution(y_train_b, y_val_b, y_test_b)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process\n",
                "X_train_b_proc = preprocessor.preprocess(X_train_b['text'])\n",
                "X_val_b_proc = preprocessor.preprocess(X_val_b['text'])\n",
                "X_test_b_proc = preprocessor.preprocess(X_test_b['text'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vectorize\n",
                "tfidf_books = TfidfVectorizer()\n",
                "X_train_b_vec = tfidf_books.fit_transform(X_train_b_proc.tolist())\n",
                "X_val_b_vec = tfidf_books.transform(X_val_b_proc.tolist())\n",
                "X_test_b_vec = tfidf_books.transform(X_test_b_proc.tolist())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Map Labels\n",
                "book_map = {\"Arthur Conan Doyle\": 2, \"Jane Austen\": 1, \"Fyodor Dostoyevsky\": 0}\n",
                "y_train_b_idx = np.array([book_map[v] for v in y_train_b], dtype=int)\n",
                "y_val_b_idx = np.array([book_map[v] for v in y_val_b], dtype=int)\n",
                "y_test_b_idx = np.array([book_map[v] for v in y_test_b], dtype=int)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "train_books",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Cross-Validation on 5 lambda candidates...\n",
                        "Epoch    0 | Train Loss: 1.097516\n",
                        "Epoch   10 | Train Loss: 1.086950\n",
                        "Epoch   20 | Train Loss: 1.077075\n",
                        "Epoch   30 | Train Loss: 1.067848\n",
                        "Epoch   40 | Train Loss: 1.059225\n",
                        "Epoch   50 | Train Loss: 1.051167\n",
                        "Epoch   60 | Train Loss: 1.043638\n",
                        "Epoch   70 | Train Loss: 1.036601\n",
                        "Epoch   80 | Train Loss: 1.030024\n",
                        "Epoch   90 | Train Loss: 1.023876\n",
                        "Epoch  100 | Train Loss: 1.018127\n",
                        "Epoch  110 | Train Loss: 1.012749\n",
                        "Epoch  120 | Train Loss: 1.007718\n",
                        "Epoch  130 | Train Loss: 1.003009\n",
                        "Epoch  140 | Train Loss: 0.998599\n",
                        "Epoch  150 | Train Loss: 0.994469\n",
                        "Epoch  160 | Train Loss: 0.990598\n",
                        "Epoch  170 | Train Loss: 0.986968\n",
                        "Epoch  180 | Train Loss: 0.983563\n",
                        "Epoch  190 | Train Loss: 0.980367\n",
                        "Epoch  200 | Train Loss: 0.977365\n",
                        "Epoch  210 | Train Loss: 0.974543\n",
                        "Epoch  220 | Train Loss: 0.971890\n",
                        "Epoch  230 | Train Loss: 0.969393\n",
                        "Epoch  240 | Train Loss: 0.967041\n",
                        "Epoch  250 | Train Loss: 0.964825\n",
                        "Epoch  260 | Train Loss: 0.962735\n",
                        "Epoch  270 | Train Loss: 0.960762\n",
                        "Epoch  280 | Train Loss: 0.958898\n",
                        "Epoch  290 | Train Loss: 0.957137\n",
                        "Epoch  300 | Train Loss: 0.955470\n",
                        "Epoch  310 | Train Loss: 0.953892\n",
                        "Epoch  320 | Train Loss: 0.952396\n",
                        "Epoch  330 | Train Loss: 0.950977\n",
                        "Epoch  340 | Train Loss: 0.949630\n",
                        "Epoch  350 | Train Loss: 0.948351\n",
                        "Epoch  360 | Train Loss: 0.947133\n",
                        "Epoch  370 | Train Loss: 0.945975\n",
                        "Epoch  380 | Train Loss: 0.944871\n",
                        "Epoch  390 | Train Loss: 0.943819\n",
                        "Epoch  400 | Train Loss: 0.942814\n",
                        "Epoch  410 | Train Loss: 0.941854\n",
                        "Epoch  420 | Train Loss: 0.940937\n",
                        "Epoch  430 | Train Loss: 0.940058\n",
                        "Epoch  440 | Train Loss: 0.939217\n",
                        "Epoch  450 | Train Loss: 0.938410\n",
                        "Epoch  460 | Train Loss: 0.937636\n",
                        "Epoch  470 | Train Loss: 0.936892\n",
                        "Epoch  480 | Train Loss: 0.936178\n",
                        "Epoch  490 | Train Loss: 0.935490\n",
                        "Epoch  499 | Train Loss: 0.934892\n",
                        "Lambda: 0.001  | Validation Loss: 0.937131\n",
                        "Epoch    0 | Train Loss: 1.097516\n",
                        "Epoch   10 | Train Loss: 1.086951\n",
                        "Epoch   20 | Train Loss: 1.077080\n",
                        "Epoch   30 | Train Loss: 1.067857\n",
                        "Epoch   40 | Train Loss: 1.059242\n",
                        "Epoch   50 | Train Loss: 1.051193\n",
                        "Epoch   60 | Train Loss: 1.043674\n",
                        "Epoch   70 | Train Loss: 1.036649\n",
                        "Epoch   80 | Train Loss: 1.030084\n",
                        "Epoch   90 | Train Loss: 1.023950\n",
                        "Epoch  100 | Train Loss: 1.018215\n",
                        "Epoch  110 | Train Loss: 1.012853\n",
                        "Epoch  120 | Train Loss: 1.007839\n",
                        "Epoch  130 | Train Loss: 1.003147\n",
                        "Epoch  140 | Train Loss: 0.998756\n",
                        "Epoch  150 | Train Loss: 0.994645\n",
                        "Epoch  160 | Train Loss: 0.990794\n",
                        "Epoch  170 | Train Loss: 0.987185\n",
                        "Epoch  180 | Train Loss: 0.983801\n",
                        "Epoch  190 | Train Loss: 0.980626\n",
                        "Epoch  200 | Train Loss: 0.977647\n",
                        "Epoch  210 | Train Loss: 0.974848\n",
                        "Epoch  220 | Train Loss: 0.972218\n",
                        "Epoch  230 | Train Loss: 0.969745\n",
                        "Epoch  240 | Train Loss: 0.967418\n",
                        "Epoch  250 | Train Loss: 0.965226\n",
                        "Epoch  260 | Train Loss: 0.963162\n",
                        "Epoch  270 | Train Loss: 0.961215\n",
                        "Epoch  280 | Train Loss: 0.959377\n",
                        "Epoch  290 | Train Loss: 0.957643\n",
                        "Epoch  300 | Train Loss: 0.956003\n",
                        "Epoch  310 | Train Loss: 0.954452\n",
                        "Epoch  320 | Train Loss: 0.952985\n",
                        "Epoch  330 | Train Loss: 0.951595\n",
                        "Epoch  340 | Train Loss: 0.950277\n",
                        "Epoch  350 | Train Loss: 0.949026\n",
                        "Epoch  360 | Train Loss: 0.947839\n",
                        "Epoch  370 | Train Loss: 0.946711\n",
                        "Epoch  380 | Train Loss: 0.945638\n",
                        "Epoch  390 | Train Loss: 0.944616\n",
                        "Epoch  400 | Train Loss: 0.943643\n",
                        "Epoch  410 | Train Loss: 0.942715\n",
                        "Epoch  420 | Train Loss: 0.941830\n",
                        "Epoch  430 | Train Loss: 0.940984\n",
                        "Epoch  440 | Train Loss: 0.940176\n",
                        "Epoch  450 | Train Loss: 0.939403\n",
                        "Epoch  460 | Train Loss: 0.938663\n",
                        "Epoch  470 | Train Loss: 0.937954\n",
                        "Epoch  480 | Train Loss: 0.937274\n",
                        "Epoch  490 | Train Loss: 0.936621\n",
                        "Epoch  499 | Train Loss: 0.936056\n",
                        "Lambda: 0.01   | Validation Loss: 0.938225\n",
                        "Epoch    0 | Train Loss: 1.097516\n",
                        "Epoch   10 | Train Loss: 1.086964\n",
                        "Epoch   20 | Train Loss: 1.077125\n",
                        "Epoch   30 | Train Loss: 1.067953\n",
                        "Epoch   40 | Train Loss: 1.059402\n",
                        "Epoch   50 | Train Loss: 1.051432\n",
                        "Epoch   60 | Train Loss: 1.044003\n",
                        "Epoch   70 | Train Loss: 1.037077\n",
                        "Epoch   80 | Train Loss: 1.030621\n",
                        "Epoch   90 | Train Loss: 1.024602\n",
                        "Epoch  100 | Train Loss: 1.018989\n",
                        "Epoch  110 | Train Loss: 1.013753\n",
                        "Epoch  120 | Train Loss: 1.008869\n",
                        "Epoch  130 | Train Loss: 1.004312\n",
                        "Epoch  140 | Train Loss: 1.000057\n",
                        "Epoch  150 | Train Loss: 0.996085\n",
                        "Epoch  160 | Train Loss: 0.992375\n",
                        "Epoch  170 | Train Loss: 0.988908\n",
                        "Epoch  180 | Train Loss: 0.985667\n",
                        "Epoch  190 | Train Loss: 0.982636\n",
                        "Epoch  200 | Train Loss: 0.979801\n",
                        "Epoch  210 | Train Loss: 0.977146\n",
                        "Epoch  220 | Train Loss: 0.974661\n",
                        "Epoch  230 | Train Loss: 0.972332\n",
                        "Epoch  240 | Train Loss: 0.970149\n",
                        "Epoch  250 | Train Loss: 0.968101\n",
                        "Epoch  260 | Train Loss: 0.966179\n",
                        "Epoch  270 | Train Loss: 0.964375\n",
                        "Epoch  280 | Train Loss: 0.962680\n",
                        "Epoch  290 | Train Loss: 0.961087\n",
                        "Epoch  300 | Train Loss: 0.959588\n",
                        "Epoch  310 | Train Loss: 0.958178\n",
                        "Epoch  320 | Train Loss: 0.956850\n",
                        "Epoch  330 | Train Loss: 0.955598\n",
                        "Epoch  340 | Train Loss: 0.954419\n",
                        "Epoch  350 | Train Loss: 0.953306\n",
                        "Epoch  360 | Train Loss: 0.952256\n",
                        "Epoch  370 | Train Loss: 0.951264\n",
                        "Epoch  380 | Train Loss: 0.950327\n",
                        "Epoch  390 | Train Loss: 0.949440\n",
                        "Epoch  400 | Train Loss: 0.948602\n",
                        "Epoch  410 | Train Loss: 0.947808\n",
                        "Epoch  420 | Train Loss: 0.947056\n",
                        "Epoch  430 | Train Loss: 0.946343\n",
                        "Epoch  440 | Train Loss: 0.945668\n",
                        "Epoch  450 | Train Loss: 0.945026\n",
                        "Epoch  460 | Train Loss: 0.944418\n",
                        "Epoch  470 | Train Loss: 0.943839\n",
                        "Epoch  480 | Train Loss: 0.943290\n",
                        "Epoch  490 | Train Loss: 0.942767\n",
                        "Epoch  499 | Train Loss: 0.942319\n",
                        "Lambda: 0.1    | Validation Loss: 0.943955\n",
                        "Epoch    0 | Train Loss: 1.097517\n",
                        "Epoch   10 | Train Loss: 1.087075\n",
                        "Epoch   20 | Train Loss: 1.077477\n",
                        "Epoch   30 | Train Loss: 1.068611\n",
                        "Epoch   40 | Train Loss: 1.060395\n",
                        "Epoch   50 | Train Loss: 1.052761\n",
                        "Epoch   60 | Train Loss: 1.045656\n",
                        "Epoch   70 | Train Loss: 1.039034\n",
                        "Epoch   80 | Train Loss: 1.032857\n",
                        "Epoch   90 | Train Loss: 1.027090\n",
                        "Epoch  100 | Train Loss: 1.021703\n",
                        "Epoch  110 | Train Loss: 1.016670\n",
                        "Epoch  120 | Train Loss: 1.011964\n",
                        "Epoch  130 | Train Loss: 1.007563\n",
                        "Epoch  140 | Train Loss: 1.003445\n",
                        "Epoch  150 | Train Loss: 0.999593\n",
                        "Epoch  160 | Train Loss: 0.995986\n",
                        "Epoch  170 | Train Loss: 0.992609\n",
                        "Epoch  180 | Train Loss: 0.989446\n",
                        "Epoch  190 | Train Loss: 0.986482\n",
                        "Epoch  200 | Train Loss: 0.983703\n",
                        "Epoch  210 | Train Loss: 0.981098\n",
                        "Epoch  220 | Train Loss: 0.978655\n",
                        "Epoch  230 | Train Loss: 0.976362\n",
                        "Epoch  240 | Train Loss: 0.974210\n",
                        "Epoch  250 | Train Loss: 0.972189\n",
                        "Epoch  260 | Train Loss: 0.970289\n",
                        "Epoch  270 | Train Loss: 0.968505\n",
                        "Epoch  280 | Train Loss: 0.966826\n",
                        "Epoch  290 | Train Loss: 0.965247\n",
                        "Epoch  300 | Train Loss: 0.963761\n",
                        "Epoch  310 | Train Loss: 0.962362\n",
                        "Epoch  320 | Train Loss: 0.961044\n",
                        "Epoch  330 | Train Loss: 0.959802\n",
                        "Epoch  340 | Train Loss: 0.958630\n",
                        "Epoch  350 | Train Loss: 0.957525\n",
                        "Epoch  360 | Train Loss: 0.956482\n",
                        "Epoch  370 | Train Loss: 0.955498\n",
                        "Epoch  380 | Train Loss: 0.954567\n",
                        "Epoch  390 | Train Loss: 0.953688\n",
                        "Epoch  400 | Train Loss: 0.952856\n",
                        "Epoch  410 | Train Loss: 0.952069\n",
                        "Epoch  420 | Train Loss: 0.951324\n",
                        "Epoch  430 | Train Loss: 0.950619\n",
                        "Epoch  440 | Train Loss: 0.949950\n",
                        "Epoch  450 | Train Loss: 0.949316\n",
                        "Epoch  460 | Train Loss: 0.948715\n",
                        "Epoch  470 | Train Loss: 0.948145\n",
                        "Epoch  480 | Train Loss: 0.947604\n",
                        "Epoch  490 | Train Loss: 0.947089\n",
                        "Epoch  499 | Train Loss: 0.946648\n",
                        "Lambda: 1      | Validation Loss: 0.947273\n",
                        "Epoch    0 | Train Loss: 1.097522\n",
                        "Epoch   10 | Train Loss: 1.087473\n",
                        "Epoch   20 | Train Loss: 1.078169\n",
                        "Epoch   30 | Train Loss: 1.069471\n",
                        "Epoch   40 | Train Loss: 1.061339\n",
                        "Epoch   50 | Train Loss: 1.053737\n",
                        "Epoch   60 | Train Loss: 1.046632\n",
                        "Epoch   70 | Train Loss: 1.039990\n",
                        "Epoch   80 | Train Loss: 1.033782\n",
                        "Epoch   90 | Train Loss: 1.027979\n",
                        "Epoch  100 | Train Loss: 1.022554\n",
                        "Epoch  110 | Train Loss: 1.017482\n",
                        "Epoch  120 | Train Loss: 1.012739\n",
                        "Epoch  130 | Train Loss: 1.008303\n",
                        "Epoch  140 | Train Loss: 1.004154\n",
                        "Epoch  150 | Train Loss: 1.000272\n",
                        "Epoch  160 | Train Loss: 0.996639\n",
                        "Epoch  170 | Train Loss: 0.993238\n",
                        "Epoch  180 | Train Loss: 0.990054\n",
                        "Epoch  190 | Train Loss: 0.987071\n",
                        "Epoch  200 | Train Loss: 0.984276\n",
                        "Epoch  210 | Train Loss: 0.981656\n",
                        "Epoch  220 | Train Loss: 0.979200\n",
                        "Epoch  230 | Train Loss: 0.976896\n",
                        "Epoch  240 | Train Loss: 0.974734\n",
                        "Epoch  250 | Train Loss: 0.972704\n",
                        "Epoch  260 | Train Loss: 0.970798\n",
                        "Epoch  270 | Train Loss: 0.969007\n",
                        "Epoch  280 | Train Loss: 0.967323\n",
                        "Epoch  290 | Train Loss: 0.965740\n",
                        "Epoch  300 | Train Loss: 0.964251\n",
                        "Epoch  310 | Train Loss: 0.962849\n",
                        "Epoch  320 | Train Loss: 0.961528\n",
                        "Epoch  330 | Train Loss: 0.960285\n",
                        "Epoch  340 | Train Loss: 0.959112\n",
                        "Epoch  350 | Train Loss: 0.958006\n",
                        "Epoch  360 | Train Loss: 0.956963\n",
                        "Epoch  370 | Train Loss: 0.955978\n",
                        "Epoch  380 | Train Loss: 0.955047\n",
                        "Epoch  390 | Train Loss: 0.954168\n",
                        "Epoch  400 | Train Loss: 0.953337\n",
                        "Epoch  410 | Train Loss: 0.952551\n",
                        "Epoch  420 | Train Loss: 0.951807\n",
                        "Epoch  430 | Train Loss: 0.951102\n",
                        "Epoch  440 | Train Loss: 0.950435\n",
                        "Epoch  450 | Train Loss: 0.949802\n",
                        "Epoch  460 | Train Loss: 0.949203\n",
                        "Epoch  470 | Train Loss: 0.948634\n",
                        "Epoch  480 | Train Loss: 0.948094\n",
                        "Epoch  490 | Train Loss: 0.947581\n",
                        "Epoch  499 | Train Loss: 0.947141\n",
                        "Lambda: 10     | Validation Loss: 0.947572\n",
                        "Best Lambda: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# Batch GD for cross validation of the multiclass case\n",
                "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
                "base_config = {\n",
                "    'lr': 0.01,\n",
                "    'epochs': 500,\n",
                "    'batch_size': None,\n",
                "    'method': 'batch',\n",
                "    'multiclass': True\n",
                "}\n",
                "\n",
                "best_lambda_b_batch, cv_results_b_batch = cross_validate(X_train_b_vec, y_train_b_idx, X_val_b_vec, y_val_b_idx, lambdas_to_test, base_config)\n",
                "\n",
                "# Capture best lambda for retraining and evaluation\n",
                "final_config_b_batch = base_config.copy()\n",
                "final_config_b_batch['lam'] = best_lambda_b_batch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Cross-Validation on 5 lambda candidates...\n",
                        "Epoch    0 | Train Loss: 0.803837\n",
                        "Epoch   10 | Train Loss: 0.717253\n",
                        "Epoch   20 | Train Loss: 0.717200\n",
                        "Epoch   30 | Train Loss: 0.717371\n",
                        "Epoch   40 | Train Loss: 0.718919\n",
                        "Epoch   49 | Train Loss: 0.717801\n",
                        "Lambda: 0.001  | Validation Loss: 0.747007\n",
                        "Epoch    0 | Train Loss: 0.893700\n",
                        "Epoch   10 | Train Loss: 0.893571\n",
                        "Epoch   20 | Train Loss: 0.896761\n",
                        "Epoch   30 | Train Loss: 0.894995\n",
                        "Epoch   40 | Train Loss: 0.894306\n",
                        "Epoch   49 | Train Loss: 0.894833\n",
                        "Lambda: 0.01   | Validation Loss: 0.903648\n",
                        "Epoch    0 | Train Loss: 0.935174\n",
                        "Epoch   10 | Train Loss: 0.935548\n",
                        "Epoch   20 | Train Loss: 0.934038\n",
                        "Epoch   30 | Train Loss: 0.936946\n",
                        "Epoch   40 | Train Loss: 0.936615\n",
                        "Epoch   49 | Train Loss: 0.934992\n",
                        "Lambda: 0.1    | Validation Loss: 0.937013\n",
                        "Epoch    0 | Train Loss: 0.940527\n",
                        "Epoch   10 | Train Loss: 0.939395\n",
                        "Epoch   20 | Train Loss: 0.941074\n",
                        "Epoch   30 | Train Loss: 0.947528\n",
                        "Epoch   40 | Train Loss: 0.942460\n",
                        "Epoch   49 | Train Loss: 0.940255\n",
                        "Lambda: 1      | Validation Loss: 0.940872\n",
                        "Epoch    0 | Train Loss: 0.940441\n",
                        "Epoch   10 | Train Loss: 0.939885\n",
                        "Epoch   20 | Train Loss: 0.940709\n",
                        "Epoch   30 | Train Loss: 0.947632\n",
                        "Epoch   40 | Train Loss: 0.938798\n",
                        "Epoch   49 | Train Loss: 0.944694\n",
                        "Lambda: 10     | Validation Loss: 0.945194\n",
                        "Best Lambda: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# Batch sgd for cross validation of the multiclass case\n",
                "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
                "base_config = {\n",
                "    'lr': 0.01,\n",
                "    'epochs': 50,\n",
                "    'batch_size': 1,\n",
                "    'method': 'sgd',\n",
                "    'multiclass': True\n",
                "}\n",
                "\n",
                "best_lambda_b_sgd, cv_results_b_sgd = cross_validate(X_train_b_vec, y_train_b_idx, X_val_b_vec, y_val_b_idx, lambdas_to_test, base_config)\n",
                "\n",
                "# Capture best lambda for retraining and evaluation\n",
                "final_config_b_sgd = base_config.copy()\n",
                "final_config_b_sgd['lam'] = best_lambda_b_sgd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Cross-Validation on 5 lambda candidates...\n",
                        "Epoch    0 | Train Loss: 0.946886\n",
                        "Epoch   10 | Train Loss: 0.870920\n",
                        "Epoch   20 | Train Loss: 0.832263\n",
                        "Epoch   30 | Train Loss: 0.804354\n",
                        "Epoch   40 | Train Loss: 0.783849\n",
                        "Epoch   50 | Train Loss: 0.768579\n",
                        "Epoch   60 | Train Loss: 0.757063\n",
                        "Epoch   70 | Train Loss: 0.748318\n",
                        "Epoch   80 | Train Loss: 0.741598\n",
                        "Epoch   90 | Train Loss: 0.736419\n",
                        "Epoch  100 | Train Loss: 0.732403\n",
                        "Epoch  110 | Train Loss: 0.729257\n",
                        "Epoch  120 | Train Loss: 0.726794\n",
                        "Epoch  130 | Train Loss: 0.724855\n",
                        "Epoch  140 | Train Loss: 0.723317\n",
                        "Epoch  150 | Train Loss: 0.722097\n",
                        "Epoch  160 | Train Loss: 0.721133\n",
                        "Epoch  170 | Train Loss: 0.720350\n",
                        "Epoch  180 | Train Loss: 0.719723\n",
                        "Epoch  190 | Train Loss: 0.719229\n",
                        "Epoch  200 | Train Loss: 0.718819\n",
                        "Epoch  210 | Train Loss: 0.718493\n",
                        "Epoch  220 | Train Loss: 0.718225\n",
                        "Epoch  230 | Train Loss: 0.718009\n",
                        "Epoch  240 | Train Loss: 0.717834\n",
                        "Epoch  250 | Train Loss: 0.717691\n",
                        "Epoch  260 | Train Loss: 0.717575\n",
                        "Epoch  270 | Train Loss: 0.717478\n",
                        "Epoch  280 | Train Loss: 0.717404\n",
                        "Epoch  290 | Train Loss: 0.717334\n",
                        "Epoch  300 | Train Loss: 0.717282\n",
                        "Epoch  310 | Train Loss: 0.717237\n",
                        "Epoch  320 | Train Loss: 0.717204\n",
                        "Epoch  330 | Train Loss: 0.717170\n",
                        "Epoch  340 | Train Loss: 0.717147\n",
                        "Epoch  350 | Train Loss: 0.717125\n",
                        "Epoch  360 | Train Loss: 0.717108\n",
                        "Epoch  370 | Train Loss: 0.717093\n",
                        "Epoch  380 | Train Loss: 0.717081\n",
                        "Epoch  390 | Train Loss: 0.717071\n",
                        "Epoch  400 | Train Loss: 0.717062\n",
                        "Epoch  410 | Train Loss: 0.717055\n",
                        "Epoch  420 | Train Loss: 0.717050\n",
                        "Epoch  430 | Train Loss: 0.717051\n",
                        "Epoch  440 | Train Loss: 0.717041\n",
                        "Epoch  450 | Train Loss: 0.717042\n",
                        "Epoch  460 | Train Loss: 0.717034\n",
                        "Epoch  470 | Train Loss: 0.717032\n",
                        "Epoch  480 | Train Loss: 0.717031\n",
                        "Epoch  490 | Train Loss: 0.717027\n",
                        "Epoch  499 | Train Loss: 0.717032\n",
                        "Lambda: 0.001  | Validation Loss: 0.745461\n",
                        "Epoch    0 | Train Loss: 0.947794\n",
                        "Epoch   10 | Train Loss: 0.898994\n",
                        "Epoch   20 | Train Loss: 0.893562\n",
                        "Epoch   30 | Train Loss: 0.892499\n",
                        "Epoch   40 | Train Loss: 0.892284\n",
                        "Epoch   50 | Train Loss: 0.892244\n",
                        "Epoch   60 | Train Loss: 0.892243\n",
                        "Epoch   70 | Train Loss: 0.892238\n",
                        "Epoch   80 | Train Loss: 0.892231\n",
                        "Epoch   90 | Train Loss: 0.892231\n",
                        "Epoch  100 | Train Loss: 0.892237\n",
                        "Epoch  110 | Train Loss: 0.892232\n",
                        "Epoch  120 | Train Loss: 0.892238\n",
                        "Epoch  130 | Train Loss: 0.892238\n",
                        "Epoch  140 | Train Loss: 0.892230\n",
                        "Epoch  150 | Train Loss: 0.892231\n",
                        "Epoch  160 | Train Loss: 0.892233\n",
                        "Epoch  170 | Train Loss: 0.892234\n",
                        "Epoch  180 | Train Loss: 0.892238\n",
                        "Epoch  190 | Train Loss: 0.892239\n",
                        "Epoch  200 | Train Loss: 0.892231\n",
                        "Epoch  210 | Train Loss: 0.892237\n",
                        "Epoch  220 | Train Loss: 0.892233\n",
                        "Epoch  230 | Train Loss: 0.892231\n",
                        "Epoch  240 | Train Loss: 0.892230\n",
                        "Epoch  250 | Train Loss: 0.892233\n",
                        "Epoch  260 | Train Loss: 0.892236\n",
                        "Epoch  270 | Train Loss: 0.892231\n",
                        "Epoch  280 | Train Loss: 0.892236\n",
                        "Epoch  290 | Train Loss: 0.892231\n",
                        "Epoch  300 | Train Loss: 0.892231\n",
                        "Epoch  310 | Train Loss: 0.892240\n",
                        "Epoch  320 | Train Loss: 0.892231\n",
                        "Epoch  330 | Train Loss: 0.892232\n",
                        "Epoch  340 | Train Loss: 0.892234\n",
                        "Epoch  350 | Train Loss: 0.892233\n",
                        "Epoch  360 | Train Loss: 0.892231\n",
                        "Epoch  370 | Train Loss: 0.892233\n",
                        "Epoch  380 | Train Loss: 0.892231\n",
                        "Epoch  390 | Train Loss: 0.892244\n",
                        "Epoch  400 | Train Loss: 0.892232\n",
                        "Epoch  410 | Train Loss: 0.892231\n",
                        "Epoch  420 | Train Loss: 0.892232\n",
                        "Epoch  430 | Train Loss: 0.892232\n",
                        "Epoch  440 | Train Loss: 0.892230\n",
                        "Epoch  450 | Train Loss: 0.892239\n",
                        "Epoch  460 | Train Loss: 0.892233\n",
                        "Epoch  470 | Train Loss: 0.892252\n",
                        "Epoch  480 | Train Loss: 0.892232\n",
                        "Epoch  490 | Train Loss: 0.892233\n",
                        "Epoch  499 | Train Loss: 0.892239\n",
                        "Lambda: 0.01   | Validation Loss: 0.900813\n",
                        "Epoch    0 | Train Loss: 0.952645\n",
                        "Epoch   10 | Train Loss: 0.930416\n",
                        "Epoch   20 | Train Loss: 0.930430\n",
                        "Epoch   30 | Train Loss: 0.930422\n",
                        "Epoch   40 | Train Loss: 0.930417\n",
                        "Epoch   50 | Train Loss: 0.930424\n",
                        "Epoch   60 | Train Loss: 0.930417\n",
                        "Epoch   70 | Train Loss: 0.930417\n",
                        "Epoch   80 | Train Loss: 0.930421\n",
                        "Epoch   90 | Train Loss: 0.930416\n",
                        "Epoch  100 | Train Loss: 0.930415\n",
                        "Epoch  110 | Train Loss: 0.930427\n",
                        "Epoch  120 | Train Loss: 0.930416\n",
                        "Epoch  130 | Train Loss: 0.930423\n",
                        "Epoch  140 | Train Loss: 0.930416\n",
                        "Epoch  150 | Train Loss: 0.930416\n",
                        "Epoch  160 | Train Loss: 0.930419\n",
                        "Epoch  170 | Train Loss: 0.930454\n",
                        "Epoch  180 | Train Loss: 0.930415\n",
                        "Epoch  190 | Train Loss: 0.930433\n",
                        "Epoch  200 | Train Loss: 0.930416\n",
                        "Epoch  210 | Train Loss: 0.930415\n",
                        "Epoch  220 | Train Loss: 0.930439\n",
                        "Epoch  230 | Train Loss: 0.930416\n",
                        "Epoch  240 | Train Loss: 0.930414\n",
                        "Epoch  250 | Train Loss: 0.930416\n",
                        "Epoch  260 | Train Loss: 0.930416\n",
                        "Epoch  270 | Train Loss: 0.930420\n",
                        "Epoch  280 | Train Loss: 0.930421\n",
                        "Epoch  290 | Train Loss: 0.930417\n",
                        "Epoch  300 | Train Loss: 0.930426\n",
                        "Epoch  310 | Train Loss: 0.930426\n",
                        "Epoch  320 | Train Loss: 0.930425\n",
                        "Epoch  330 | Train Loss: 0.930415\n",
                        "Epoch  340 | Train Loss: 0.930427\n",
                        "Epoch  350 | Train Loss: 0.930417\n",
                        "Epoch  360 | Train Loss: 0.930415\n",
                        "Epoch  370 | Train Loss: 0.930418\n",
                        "Epoch  380 | Train Loss: 0.930418\n",
                        "Epoch  390 | Train Loss: 0.930420\n",
                        "Epoch  400 | Train Loss: 0.930429\n",
                        "Epoch  410 | Train Loss: 0.930415\n",
                        "Epoch  420 | Train Loss: 0.930419\n",
                        "Epoch  430 | Train Loss: 0.930415\n",
                        "Epoch  440 | Train Loss: 0.930421\n",
                        "Epoch  450 | Train Loss: 0.930425\n",
                        "Epoch  460 | Train Loss: 0.930414\n",
                        "Epoch  470 | Train Loss: 0.930416\n",
                        "Epoch  480 | Train Loss: 0.930415\n",
                        "Epoch  490 | Train Loss: 0.930437\n",
                        "Epoch  499 | Train Loss: 0.930418\n",
                        "Lambda: 0.1    | Validation Loss: 0.932342\n",
                        "Epoch    0 | Train Loss: 0.957105\n",
                        "Epoch   10 | Train Loss: 0.935578\n",
                        "Epoch   20 | Train Loss: 0.935634\n",
                        "Epoch   30 | Train Loss: 0.935587\n",
                        "Epoch   40 | Train Loss: 0.935589\n",
                        "Epoch   50 | Train Loss: 0.935604\n",
                        "Epoch   60 | Train Loss: 0.935599\n",
                        "Epoch   70 | Train Loss: 0.935588\n",
                        "Epoch   80 | Train Loss: 0.935622\n",
                        "Epoch   90 | Train Loss: 0.935612\n",
                        "Epoch  100 | Train Loss: 0.935593\n",
                        "Epoch  110 | Train Loss: 0.935602\n",
                        "Epoch  120 | Train Loss: 0.935595\n",
                        "Epoch  130 | Train Loss: 0.935603\n",
                        "Epoch  140 | Train Loss: 0.935634\n",
                        "Epoch  150 | Train Loss: 0.935590\n",
                        "Epoch  160 | Train Loss: 0.935593\n",
                        "Epoch  170 | Train Loss: 0.935601\n",
                        "Epoch  180 | Train Loss: 0.935618\n",
                        "Epoch  190 | Train Loss: 0.935619\n",
                        "Epoch  200 | Train Loss: 0.935606\n",
                        "Epoch  210 | Train Loss: 0.935602\n",
                        "Epoch  220 | Train Loss: 0.935605\n",
                        "Epoch  230 | Train Loss: 0.935596\n",
                        "Epoch  240 | Train Loss: 0.935608\n",
                        "Epoch  250 | Train Loss: 0.935632\n",
                        "Epoch  260 | Train Loss: 0.935593\n",
                        "Epoch  270 | Train Loss: 0.935602\n",
                        "Epoch  280 | Train Loss: 0.935601\n",
                        "Epoch  290 | Train Loss: 0.935603\n",
                        "Epoch  300 | Train Loss: 0.935586\n",
                        "Epoch  310 | Train Loss: 0.935590\n",
                        "Epoch  320 | Train Loss: 0.935587\n",
                        "Epoch  330 | Train Loss: 0.935600\n",
                        "Epoch  340 | Train Loss: 0.935582\n",
                        "Epoch  350 | Train Loss: 0.935586\n",
                        "Epoch  360 | Train Loss: 0.935621\n",
                        "Epoch  370 | Train Loss: 0.935606\n",
                        "Epoch  380 | Train Loss: 0.935602\n",
                        "Epoch  390 | Train Loss: 0.935589\n",
                        "Epoch  400 | Train Loss: 0.935590\n",
                        "Epoch  410 | Train Loss: 0.935588\n",
                        "Epoch  420 | Train Loss: 0.935604\n",
                        "Epoch  430 | Train Loss: 0.935595\n",
                        "Epoch  440 | Train Loss: 0.935590\n",
                        "Epoch  450 | Train Loss: 0.935590\n",
                        "Epoch  460 | Train Loss: 0.935603\n",
                        "Epoch  470 | Train Loss: 0.935584\n",
                        "Epoch  480 | Train Loss: 0.935623\n",
                        "Epoch  490 | Train Loss: 0.935595\n",
                        "Epoch  499 | Train Loss: 0.935642\n",
                        "Lambda: 1      | Validation Loss: 0.936211\n",
                        "Epoch    0 | Train Loss: 0.957092\n",
                        "Epoch   10 | Train Loss: 0.936182\n",
                        "Epoch   20 | Train Loss: 0.936274\n",
                        "Epoch   30 | Train Loss: 0.936254\n",
                        "Epoch   40 | Train Loss: 0.936252\n",
                        "Epoch   50 | Train Loss: 0.936187\n",
                        "Epoch   60 | Train Loss: 0.936231\n",
                        "Epoch   70 | Train Loss: 0.936156\n",
                        "Epoch   80 | Train Loss: 0.936160\n",
                        "Epoch   90 | Train Loss: 0.936236\n",
                        "Epoch  100 | Train Loss: 0.936232\n",
                        "Epoch  110 | Train Loss: 0.936156\n",
                        "Epoch  120 | Train Loss: 0.936222\n",
                        "Epoch  130 | Train Loss: 0.936220\n",
                        "Epoch  140 | Train Loss: 0.936203\n",
                        "Epoch  150 | Train Loss: 0.936178\n",
                        "Epoch  160 | Train Loss: 0.936252\n",
                        "Epoch  170 | Train Loss: 0.936206\n",
                        "Epoch  180 | Train Loss: 0.936249\n",
                        "Epoch  190 | Train Loss: 0.936246\n",
                        "Epoch  200 | Train Loss: 0.936303\n",
                        "Epoch  210 | Train Loss: 0.936187\n",
                        "Epoch  220 | Train Loss: 0.936156\n",
                        "Epoch  230 | Train Loss: 0.936386\n",
                        "Epoch  240 | Train Loss: 0.936198\n",
                        "Epoch  250 | Train Loss: 0.936248\n",
                        "Epoch  260 | Train Loss: 0.936219\n",
                        "Epoch  270 | Train Loss: 0.936138\n",
                        "Epoch  280 | Train Loss: 0.936161\n",
                        "Epoch  290 | Train Loss: 0.936205\n",
                        "Epoch  300 | Train Loss: 0.936215\n",
                        "Epoch  310 | Train Loss: 0.936177\n",
                        "Epoch  320 | Train Loss: 0.936242\n",
                        "Epoch  330 | Train Loss: 0.936238\n",
                        "Epoch  340 | Train Loss: 0.936230\n",
                        "Epoch  350 | Train Loss: 0.936150\n",
                        "Epoch  360 | Train Loss: 0.936175\n",
                        "Epoch  370 | Train Loss: 0.936247\n",
                        "Epoch  380 | Train Loss: 0.936196\n",
                        "Epoch  390 | Train Loss: 0.936213\n",
                        "Epoch  400 | Train Loss: 0.936189\n",
                        "Epoch  410 | Train Loss: 0.936168\n",
                        "Epoch  420 | Train Loss: 0.936206\n",
                        "Epoch  430 | Train Loss: 0.936165\n",
                        "Epoch  440 | Train Loss: 0.936165\n",
                        "Epoch  450 | Train Loss: 0.936231\n",
                        "Epoch  460 | Train Loss: 0.936203\n",
                        "Epoch  470 | Train Loss: 0.936172\n",
                        "Epoch  480 | Train Loss: 0.936230\n",
                        "Epoch  490 | Train Loss: 0.936200\n",
                        "Epoch  499 | Train Loss: 0.936269\n",
                        "Lambda: 10     | Validation Loss: 0.936650\n",
                        "Best Lambda: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# Mini-Batch GD for cross validation of the multiclass case\n",
                "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
                "base_config = {\n",
                "    'lr': 0.01,\n",
                "    'epochs': 500,\n",
                "    'batch_size': 32,\n",
                "    'method': 'minibatch',\n",
                "    'multiclass': True\n",
                "}\n",
                "\n",
                "best_lambda_b_mini, cv_results_b_mini = cross_validate(X_train_b_vec, y_train_b_idx, X_val_b_vec, y_val_b_idx, lambdas_to_test, base_config)\n",
                "\n",
                "# Capture best lambda for retraining and evaluation\n",
                "final_config_b_mini = base_config.copy()\n",
                "final_config_b_mini['lam'] = best_lambda_b_mini"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We need to retrain each model with the tuned lambda "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch    0 | Train Loss: 1.097516\n",
                        "Epoch   10 | Train Loss: 1.086950\n",
                        "Epoch   20 | Train Loss: 1.077075\n",
                        "Epoch   30 | Train Loss: 1.067848\n",
                        "Epoch   40 | Train Loss: 1.059225\n",
                        "Epoch   50 | Train Loss: 1.051167\n",
                        "Epoch   60 | Train Loss: 1.043638\n",
                        "Epoch   70 | Train Loss: 1.036601\n",
                        "Epoch   80 | Train Loss: 1.030024\n",
                        "Epoch   90 | Train Loss: 1.023876\n",
                        "Epoch  100 | Train Loss: 1.018127\n",
                        "Epoch  110 | Train Loss: 1.012749\n",
                        "Epoch  120 | Train Loss: 1.007718\n",
                        "Epoch  130 | Train Loss: 1.003009\n",
                        "Epoch  140 | Train Loss: 0.998599\n",
                        "Epoch  150 | Train Loss: 0.994469\n",
                        "Epoch  160 | Train Loss: 0.990598\n",
                        "Epoch  170 | Train Loss: 0.986968\n",
                        "Epoch  180 | Train Loss: 0.983563\n",
                        "Epoch  190 | Train Loss: 0.980367\n",
                        "Epoch  200 | Train Loss: 0.977365\n",
                        "Epoch  210 | Train Loss: 0.974543\n",
                        "Epoch  220 | Train Loss: 0.971890\n",
                        "Epoch  230 | Train Loss: 0.969393\n",
                        "Epoch  240 | Train Loss: 0.967041\n",
                        "Epoch  250 | Train Loss: 0.964825\n",
                        "Epoch  260 | Train Loss: 0.962735\n",
                        "Epoch  270 | Train Loss: 0.960762\n",
                        "Epoch  280 | Train Loss: 0.958898\n",
                        "Epoch  290 | Train Loss: 0.957137\n",
                        "Epoch  300 | Train Loss: 0.955470\n",
                        "Epoch  310 | Train Loss: 0.953892\n",
                        "Epoch  320 | Train Loss: 0.952396\n",
                        "Epoch  330 | Train Loss: 0.950977\n",
                        "Epoch  340 | Train Loss: 0.949630\n",
                        "Epoch  350 | Train Loss: 0.948351\n",
                        "Epoch  360 | Train Loss: 0.947133\n",
                        "Epoch  370 | Train Loss: 0.945975\n",
                        "Epoch  380 | Train Loss: 0.944871\n",
                        "Epoch  390 | Train Loss: 0.943819\n",
                        "Epoch  400 | Train Loss: 0.942814\n",
                        "Epoch  410 | Train Loss: 0.941854\n",
                        "Epoch  420 | Train Loss: 0.940937\n",
                        "Epoch  430 | Train Loss: 0.940058\n",
                        "Epoch  440 | Train Loss: 0.939217\n",
                        "Epoch  450 | Train Loss: 0.938410\n",
                        "Epoch  460 | Train Loss: 0.937636\n",
                        "Epoch  470 | Train Loss: 0.936892\n",
                        "Epoch  480 | Train Loss: 0.936178\n",
                        "Epoch  490 | Train Loss: 0.935490\n",
                        "Epoch  499 | Train Loss: 0.934892\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<__main__.LogisticRegressionModel at 0x12a824040>"
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "batch_b_model = LogisticRegressionModel(**final_config_b_batch)\n",
                "batch_b_model.fit(X_train_b_vec, y_train_b_idx)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch    0 | Train Loss: 0.802060\n",
                        "Epoch   10 | Train Loss: 0.717192\n",
                        "Epoch   20 | Train Loss: 0.719179\n",
                        "Epoch   30 | Train Loss: 0.719001\n",
                        "Epoch   40 | Train Loss: 0.717421\n",
                        "Epoch   49 | Train Loss: 0.717264\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<__main__.LogisticRegressionModel at 0x12a824400>"
                        ]
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sgd_b_model = LogisticRegressionModel(**final_config_b_sgd)\n",
                "sgd_b_model.fit(X_train_b_vec, y_train_b_idx)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch    0 | Train Loss: 0.947193\n",
                        "Epoch   10 | Train Loss: 0.870916\n",
                        "Epoch   20 | Train Loss: 0.832270\n",
                        "Epoch   30 | Train Loss: 0.804359\n",
                        "Epoch   40 | Train Loss: 0.783851\n",
                        "Epoch   50 | Train Loss: 0.768573\n",
                        "Epoch   60 | Train Loss: 0.757063\n",
                        "Epoch   70 | Train Loss: 0.748319\n",
                        "Epoch   80 | Train Loss: 0.741598\n",
                        "Epoch   90 | Train Loss: 0.736421\n",
                        "Epoch  100 | Train Loss: 0.732397\n",
                        "Epoch  110 | Train Loss: 0.729258\n",
                        "Epoch  120 | Train Loss: 0.726795\n",
                        "Epoch  130 | Train Loss: 0.724854\n",
                        "Epoch  140 | Train Loss: 0.723320\n",
                        "Epoch  150 | Train Loss: 0.722098\n",
                        "Epoch  160 | Train Loss: 0.721125\n",
                        "Epoch  170 | Train Loss: 0.720347\n",
                        "Epoch  180 | Train Loss: 0.719723\n",
                        "Epoch  190 | Train Loss: 0.719223\n",
                        "Epoch  200 | Train Loss: 0.718819\n",
                        "Epoch  210 | Train Loss: 0.718490\n",
                        "Epoch  220 | Train Loss: 0.718232\n",
                        "Epoch  230 | Train Loss: 0.718009\n",
                        "Epoch  240 | Train Loss: 0.717838\n",
                        "Epoch  250 | Train Loss: 0.717691\n",
                        "Epoch  260 | Train Loss: 0.717574\n",
                        "Epoch  270 | Train Loss: 0.717479\n",
                        "Epoch  280 | Train Loss: 0.717401\n",
                        "Epoch  290 | Train Loss: 0.717335\n",
                        "Epoch  300 | Train Loss: 0.717280\n",
                        "Epoch  310 | Train Loss: 0.717241\n",
                        "Epoch  320 | Train Loss: 0.717206\n",
                        "Epoch  330 | Train Loss: 0.717170\n",
                        "Epoch  340 | Train Loss: 0.717144\n",
                        "Epoch  350 | Train Loss: 0.717124\n",
                        "Epoch  360 | Train Loss: 0.717108\n",
                        "Epoch  370 | Train Loss: 0.717092\n",
                        "Epoch  380 | Train Loss: 0.717084\n",
                        "Epoch  390 | Train Loss: 0.717071\n",
                        "Epoch  400 | Train Loss: 0.717063\n",
                        "Epoch  410 | Train Loss: 0.717055\n",
                        "Epoch  420 | Train Loss: 0.717050\n",
                        "Epoch  430 | Train Loss: 0.717046\n",
                        "Epoch  440 | Train Loss: 0.717041\n",
                        "Epoch  450 | Train Loss: 0.717037\n",
                        "Epoch  460 | Train Loss: 0.717038\n",
                        "Epoch  470 | Train Loss: 0.717034\n",
                        "Epoch  480 | Train Loss: 0.717031\n",
                        "Epoch  490 | Train Loss: 0.717028\n",
                        "Epoch  499 | Train Loss: 0.717027\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<__main__.LogisticRegressionModel at 0x12a826530>"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mini_b_model = LogisticRegressionModel(**final_config_b_mini)\n",
                "mini_b_model.fit(X_train_b_vec, y_train_b_idx)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we create predictions for the test set for each model type and evaluate the output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Multiclass Results\n",
                        "Accuracy  : 0.5642\n",
                        "Precision : 0.1881\n",
                        "Recall    : 0.3333\n",
                        "F1_score  : 0.2405\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation of GD\n",
                "preds_b_batch = batch_b_model.predict(X_test_b_vec, threshold=0.2)\n",
                "results_b_batch = evaluate_model(y_test_b_idx, preds_b_batch, is_multiclass=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Multiclass Results\n",
                        "Accuracy  : 0.7610\n",
                        "Precision : 0.8452\n",
                        "Recall    : 0.5549\n",
                        "F1_score  : 0.5563\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation of SGD\n",
                "preds_b_sgd= sgd_b_model.predict(X_test_b_vec, threshold=0.2)\n",
                "results_b_sgd = evaluate_model(y_test_b_idx, preds_b_sgd, is_multiclass=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Multiclass Results\n",
                        "Accuracy  : 0.7675\n",
                        "Precision : 0.8468\n",
                        "Recall    : 0.5625\n",
                        "F1_score  : 0.5636\n"
                    ]
                }
            ],
            "source": [
                "# Evaluation of Mini-Batch GD\n",
                "preds_b_mini= mini_b_model.predict(X_test_b_vec, threshold=0.2)\n",
                "results_b_mini = evaluate_model(y_test_b_idx, preds_b_mini, is_multiclass=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Overall we a similar result in this multi-class case when compared to the binary class. The best performing model is the mini-batch model, then followed closely by stochastic gradient descent and then standard gradient descent. The difference with this dataset is there are 3 classes and Jane Austin dominates the class at roughly 45% of the data. So similarily to the previous class we see that a threshold is needed to improve the predictive power of the model regardless of the training method. By looking at the F1 score there is a clear indication that even at 500 epochs the gradient descent model has a poor balance at predicting because if the model was completely randomly guessing it would have a F1 score of 33%, but this is less than that indicating the model is favoring one of the three authors, most likely Jane Austen. Looking at the stochastic gradient model and mini-batch have a similar F1 score at roughly 56%. This indicates it is better than random guessing but still is favoring one class over the others. Also more advanced tokenization or cleaning methods might result in better predictive power due to more context being learned by the model. "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "chatbot_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
