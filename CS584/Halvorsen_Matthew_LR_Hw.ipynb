{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a01ebd",
   "metadata": {},
   "source": [
    "Binary Text Classification using Logistic Regression of ham and spam text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3708d",
   "metadata": {},
   "source": [
    "The workflow that this notebook will follow is as follows:\n",
    "\n",
    "1. Data Preprocessing: \n",
    "    -Load the dataset into sentences and labels\n",
    "    -Split the dataset into training, validation and testing sets \n",
    "    -Report the distribution in the form of a table\n",
    "    -Clean the data of any noise (urls, punctuation, and numbers) & change to lower case\n",
    "    -Tokenize input text into tokens, including work stemming and stopwords\n",
    "    -Build your own TD-IDF feature extractor using the training set\n",
    "2. Build a logistic regression classifier using using L2 regularization\n",
    "    -Derive the gradient of the objective function of LR with respect to w and b. \n",
    "    -Implement logistic regression via initialization, objective function, and gradient descent\n",
    "    -Implement accuracy, precision, recall and F1 score as test metrics\n",
    "    -Write a function for SGD and Mini-batch GD\n",
    "    -Evaluate the model of the test set and report the metrics \n",
    "3. Cross Validation\n",
    "    -Implement cross validation to choose the best hyperparameter lambda for the validation set\n",
    "4. Conclusion\n",
    "    -Analyze the results and compare to baseline\n",
    "5. Create a multiclass classifier from various authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4616ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21b144",
   "metadata": {},
   "source": [
    "Load the dataset, include more information about what this importing section does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751a07e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv('a1-data/SMSSpamCollection', sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbfba93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79824c",
   "metadata": {},
   "source": [
    "Objective of the split_dataset function: \n",
    "    -Split the dataset ino training, validation and test sets\n",
    "    -Return each set split into features and labels (this enables easier tokenization later)\n",
    "    -This also allows for reproducubility as the split will not be exactly the same each time meaning the if the structure of the model is effecive it should learn at the same rate regardless of the data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c3da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_size, val_size, test_size):\n",
    "    df = df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    n = (len(df))\n",
    "    train_end = int(train_size *n)\n",
    "    val_end = train_end + int(val_size *n)\n",
    "\n",
    "    train_df = df.iloc[:train_end]\n",
    "    val_df = df.iloc[train_end:val_end]\n",
    "    test_df = df.iloc[val_end:n]\n",
    "\n",
    "    X_train, y_train = train_df[['text']], train_df['label']\n",
    "    X_val, y_val = val_df[['text']], val_df['label']\n",
    "    X_test, y_test = test_df[['text']], test_df['label']\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(spam_df, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e2ab6",
   "metadata": {},
   "source": [
    "Output a table showing the number of samples in each class for the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098e7e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>2898</td>\n",
       "      <td>966</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>445</td>\n",
       "      <td>148</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Train  Validation  Test\n",
       "label                         \n",
       "ham     2898         966   961\n",
       "spam     445         148   154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_distribution(y_train, y_val, y_test):\n",
    "    df =pd.DataFrame({'Train': y_train.value_counts(), 'Validation': y_val.value_counts(), 'Test': y_test.value_counts()}).fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "data_distribution(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbe3ca",
   "metadata": {},
   "source": [
    "Objective of the clean_data function:\n",
    "    -Remove punctuation, urls and numbers\n",
    "    -Change text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364f7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(X):\n",
    "    X = X.str.lower()\n",
    "    X = X.str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    X = X.str.replace(\"http\\\\S+\", \"\", regex=True)\n",
    "    X = X.str.replace(\"https\\\\S+\", \"\", regex=True)\n",
    "    X = X.str.replace(\"\\\\d+\", \"\", regex=True)\n",
    "    return X\n",
    "\n",
    "# Convert spam and ham to 0 and 1s for classification\n",
    "map = {\"spam\": 1, \"ham\": 0}\n",
    "y_train = np.array([map[v] for v in y_train], dtype=float)\n",
    "y_val = np.array([map[v] for v in y_val], dtype=float)\n",
    "y_test = np.array([map[v] for v in y_test], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb62c8",
   "metadata": {},
   "source": [
    "Tokenize the dataset:\n",
    "    -Remove whitespace between words\n",
    "    -Including word stems\n",
    "    -Removing stop words (removing common words that do not add any semantic value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\",\n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\",\n",
    "    \"to\", \"of\", \"in\", \"on\", \"for\", \"with\",\n",
    "    \"that\", \"this\", \"it\", \"as\", \"at\"\n",
    "}\n",
    "\n",
    "def tokenize_text(X):\n",
    "    return X.apply(lambda x: x.split())\n",
    "\n",
    "def remove_stopwords(tokens, stopwords = STOPWORDS):\n",
    "    return tokens.apply(lambda x: [t for t in x if t not in stopwords])\n",
    "\n",
    "def stem_token(token):\n",
    "    suffixes = [\"ing\", \"ly\", \"ed\", \"s\", \"es\", \"est\"]\n",
    "    for suf in suffixes:\n",
    "        if token.endswith(suf) and len(token) > len(suf) + 2:\n",
    "            return token[:-len(suf)]\n",
    "    return token\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return tokens.apply(lambda x: [stem_token(t) for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7848012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[squeeeeeze, christma, hug, if, u, lik, my, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[also, ive, sorta, blown, him, off, couple, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[mmm, that, better, now, i, got, roast, down, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[mm, have, some, kanji, dont, eat, anyth, heav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[so, there, ring, come, guy, costume, its, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [squeeeeeze, christma, hug, if, u, lik, my, fr...\n",
       "1  [also, ive, sorta, blown, him, off, couple, ti...\n",
       "2  [mmm, that, better, now, i, got, roast, down, ...\n",
       "3  [mm, have, some, kanji, dont, eat, anyth, heav...\n",
       "4  [so, there, ring, come, guy, costume, its, the..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(X):\n",
    "    X  = X.copy()\n",
    "    X = clean_text(X)\n",
    "    X = tokenize_text(X)\n",
    "    X = remove_stopwords(X)\n",
    "    X = stem_tokens(X)\n",
    "    return X\n",
    "\n",
    "X_train['text'] = preprocess_text(X_train['text'])\n",
    "X_val['text'] = preprocess_text(X_val['text'])\n",
    "X_test['text'] = preprocess_text(X_test['text'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01733485",
   "metadata": {},
   "source": [
    "Build a TD-IDF vecotrizorizer from stratch:\n",
    "    -Goal is to create a function that would take in the list of words we have and return a matrix of importance of each word\n",
    "    -TF = Term Frequency: The more the word appears in the document, the higher the TF\n",
    "    -IDF = Inverse Document Frequency: The less the word appears in the corpus, the higher the IDF\n",
    "What the functions do: \n",
    "    -fit_tfidf:\n",
    "        -Treats each row as a new \"document and counts the total number of documents\n",
    "        -Builds the document frequency required for IDF by converting the token list to a set so each word is counted at most once per document\n",
    "        -Update the voacb to say that the words appears in a specific document\n",
    "        -Building a vocabulary mapping and sorts the indices alphabetically\n",
    "        -Computes IDF for each word in the documents\n",
    "    -transform_tfidf:\n",
    "        -Counts term occurrences within that document\n",
    "        -Computes TF per term, typically count / len(doc)\n",
    "        -Computes TF-IDF per term\n",
    "        -Stores the result as a sparse vector \n",
    "        -Returns a list of vectors per document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94fb8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tfidf(text):\n",
    "    # Document frequency: df[word] = number of docs containing word\n",
    "    df = {}\n",
    "    N = 0\n",
    "\n",
    "    for doc in text:\n",
    "        N += 1\n",
    "        seen = set(doc)\n",
    "        for w in seen:\n",
    "            df[w] = df.get(w, 0) + 1\n",
    "\n",
    "    # Build vocab (deterministic order: alphabetical)\n",
    "    vocab_words = sorted(df.keys())\n",
    "    vocab = {w: i for i, w in enumerate(vocab_words)}\n",
    "\n",
    "    idf = [0.0] * len(vocab_words)\n",
    "    for w, i in vocab.items():\n",
    "        idf[i] = math.log((1.0 + N) / (1.0 + df[w])) + 1.0\n",
    "\n",
    "    return vocab, idf\n",
    "\n",
    "\n",
    "def transform_tfidf(text, vocab, idf, normalize=True):\n",
    "    vectors = []\n",
    "\n",
    "    for doc in text:\n",
    "        # term counts\n",
    "        counts = {}\n",
    "        for w in doc:\n",
    "            if w in vocab:\n",
    "                idx = vocab[w]\n",
    "                counts[idx] = counts.get(idx, 0) + 1\n",
    "\n",
    "        # compute TF-IDF (using TF = count / len(doc))\n",
    "        doc_len = len(doc) if len(doc) > 0 else 1\n",
    "        vec = {}\n",
    "        for idx, c in counts.items():\n",
    "            tf = c / doc_len\n",
    "            vec[idx] = tf * idf[idx]\n",
    "\n",
    "        '''\n",
    "        # optional L2 normalization\n",
    "        if normalize and vec:\n",
    "            import math\n",
    "            norm = math.sqrt(sum(v * v for v in vec.values()))\n",
    "            if norm > 0:\n",
    "                for idx in list(vec.keys()):\n",
    "                    vec[idx] = vec[idx] / norm\n",
    "        '''\n",
    "        vectors.append(vec)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ade02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{4589: 0.29040606184981543,\n",
       "  873: 0.24260280801809503,\n",
       "  2255: 0.5330088698679104,\n",
       "  2305: 0.26567122657239356,\n",
       "  5139: 0.6067940829285435,\n",
       "  2731: 0.25880982971622385,\n",
       "  3174: 0.10983909761429993,\n",
       "  1809: 0.29040606184981543,\n",
       "  1214: 0.22092664734835799,\n",
       "  2952: 0.10849018669721469,\n",
       "  373: 0.15991676412849606,\n",
       "  1889: 0.12648915870821661,\n",
       "  3907: 0.48333845355664923,\n",
       "  1113: 0.25880982971622385,\n",
       "  2847: 0.29040606184981543,\n",
       "  4481: 0.12203358176063021,\n",
       "  2836: 0.22330571188239082,\n",
       "  3295: 0.2764245063978097,\n",
       "  3596: 0.1970250204324978,\n",
       "  2081: 0.25252287948194957},\n",
       " {159: 0.280564912646649,\n",
       "  2444: 0.2842816267105413,\n",
       "  4527: 0.4432513575602446,\n",
       "  544: 0.4432513575602446,\n",
       "  2158: 0.5233461313487431,\n",
       "  3378: 0.2896844769242299,\n",
       "  1042: 0.35854409901108136,\n",
       "  4968: 0.22436277422552497,\n",
       "  3978: 0.377316464481541,\n",
       "  4481: 0.18626178268727767,\n",
       "  2294: 0.3276079587530751,\n",
       "  3938: 0.3854296581566598,\n",
       "  3311: 0.19105926530073353,\n",
       "  4877: 0.22775953849592345,\n",
       "  3487: 0.20881202617848102,\n",
       "  546: 0.3854296581566598,\n",
       "  2782: 0.2782253461955525,\n",
       "  5399: 0.37028849644867134},\n",
       " {3059: 0.38172908026364194,\n",
       "  4893: 0.2449824314596414,\n",
       "  485: 0.5322440428179458,\n",
       "  3324: 0.1645447980211891,\n",
       "  2281: 0.21193634912858345,\n",
       "  1960: 0.19941282328797083,\n",
       "  4106: 0.38172908026364194,\n",
       "  1363: 0.5322440428179458,\n",
       "  2952: 0.2996395632589739,\n",
       "  2448: 0.38172908026364194,\n",
       "  361: 0.2487191750520936,\n",
       "  2305: 0.18343965644284319,\n",
       "  2040: 0.2360018995640438,\n",
       "  1665: 0.27208217107344934,\n",
       "  1381: 0.2964072007765918,\n",
       "  1942: 0.2008134844882705,\n",
       "  2349: 0.34872207166554936}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs = X_train['text'].tolist()\n",
    "vocab, idf = fit_tfidf(train_docs)\n",
    "X_train_tfidf = transform_tfidf(train_docs, vocab, idf)\n",
    "\n",
    "X_train_tfidf[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ddf23",
   "metadata": {},
   "source": [
    "We only transform the validation and test sets using the transform method to not introduce any data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b03f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doc = X_val['text'].tolist()\n",
    "X_val_tfidf = transform_tfidf(val_doc, vocab, idf)\n",
    "\n",
    "test_doc = X_test['text'].tolist()\n",
    "X_test_tfidf = transform_tfidf(test_doc, vocab, idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9ef19",
   "metadata": {},
   "source": [
    "Next we need to create a logistic regression model with L2 normalization:\n",
    "    -Import Equation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b4dc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sparse(w, x):\n",
    "    s = 0\n",
    "    for j, v in x.items():\n",
    "        s += w[j]*v\n",
    "    return s\n",
    "    \n",
    "def objective(w, b, X, y, lam):\n",
    "    n = len(X)\n",
    "    eps = 1e-15\n",
    "    loss=0\n",
    "    for x_i, y_i in zip(X,y):\n",
    "        # Linear Transformation of the objective function\n",
    "        z = dot_sparse(w, x_i) + b\n",
    "        # Pass the linear transformation through the sigmoid activation function\n",
    "        y_hat = 1 / (1 + np.exp(-z))\n",
    "        # Binary Cross Entropy Loss\n",
    "        # This ensures we do not have any log(0) errors\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "        loss += -(y_i * np.log(y_hat) + (1-y_i) * np.log(1-y_hat))\n",
    "        \n",
    "    loss = loss / n\n",
    "    # L2 regularization\n",
    "    reg = lam * np.sum(w ** 2)\n",
    "\n",
    "    return loss + reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22200a",
   "metadata": {},
   "source": [
    "Generate a function for the gradient and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9616370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w, b, X, y, lam):\n",
    "    n = len(X)\n",
    "    d = len(w)\n",
    "    dw = np.zeros(d)\n",
    "    db = 0.0\n",
    "\n",
    "    for x_i, y_i in zip(X, y):\n",
    "    # Get predictions\n",
    "        z = dot_sparse(w, x_i) + b\n",
    "        y_hat = 1 / (1+np.exp(-z))\n",
    "        # Error term\n",
    "        error = y_hat - y_i\n",
    "\n",
    "        for j, v, in x_i.items():\n",
    "            dw[j] += error * v\n",
    "        db += error\n",
    "    dw = (1.0 / n) * dw + 2 * lam * w\n",
    "    db = (1.0 / n) * db\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e160aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, X, y, lam, learning_rate, max_epochs=100, print_every=50):\n",
    "    objvals = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Compute gradients for both the weight and bias\n",
    "        dw, db = gradient(w, b, X, y, lam)\n",
    "\n",
    "        # Update parameters using the learning rate\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        # Update the objective value\n",
    "        obj = objective(w, b, X, y, lam)\n",
    "        objvals.append(obj)\n",
    "        # Update progress during training\n",
    "        if epoch % print_every == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss = {obj:.6f}\")\n",
    "        # If gradient starts to diverge we should stop early\n",
    "        if not np.isfinite(obj):\n",
    "            print('Stopped early at {epoch}')\n",
    "            break\n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3792e6",
   "metadata": {},
   "source": [
    "A secondary objective is to implement a stochastic gradient descent and mini-batch gradient descent function for this model\n",
    "This requires a new gradient function and training function for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_sgd(w, b, x_i, y_i, lam):\n",
    "    grad_w = np.zeros_like(w)\n",
    "\n",
    "    z = dot_sparse(w, x_i) + b\n",
    "    y_hat = 1 / (1 = np.exp(-z))\n",
    "    error = y_hat - y_i\n",
    "    \n",
    "    for j, v in x_i.items()\n",
    "        dw[j] += error * v\n",
    "    \n",
    "    db = error\n",
    "\n",
    "    dw += 2 * lam * 2\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, b, X, y, lam, learning_rate, max_epochs=10, print_every=1, shuffle=True):\n",
    "    objvals = []\n",
    "    n = len(X)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            # single-sample gradient\n",
    "            dw, db = gradient_sgd_step(w, b, X[i], y[i], lam)\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "        # track loss once per epoch\n",
    "        obj = objective_sparse(w, b, X, y, lam)\n",
    "        objvals.append(obj)\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss = {obj:.6f}\")\n",
    "\n",
    "        if not np.isfinite(obj):\n",
    "            print(f\"Stopped early at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3445d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_minibatch(w, b, X_batch, y_batch, lam):\n",
    "    batch_size = len(X_batch)\n",
    "    d = len(w)\n",
    "\n",
    "    grad_w = np.zeros(d)\n",
    "    grad_b = 0.0\n",
    "\n",
    "    for x_i, y_i in zip(X_batch, y_batch):\n",
    "        z = dot_sparse(w, x_i) + b\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-z))\n",
    "        error = y_hat - y_i\n",
    "\n",
    "        for j, v in x_i.items():\n",
    "            grad_w[j] += error * v\n",
    "\n",
    "        grad_b += error\n",
    "\n",
    "    # average over batch + L2\n",
    "    dw = (1.0 / batch_size) * grad_w + 2 * lam * w\n",
    "    db = (1.0 / batch_size) * grad_b\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_minibatch(\n",
    "    w, b, X, y, lam, learning_rate,\n",
    "    max_epochs=100, batch_size=32,\n",
    "    shuffle=True, print_every=50\n",
    "):\n",
    "    objvals = []\n",
    "    n = len(X)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # shuffle indices each epoch\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        # iterate over mini-batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "\n",
    "            X_batch = [X[i] for i in batch_idx]\n",
    "            y_batch = y[batch_idx] \n",
    "\n",
    "            dw, db = gradient_minibatch(w, b, X_batch, y_batch, lam)\n",
    "\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "        # track full training loss once per epoch\n",
    "        obj = objective_sparse(w, b, X, y, lam)\n",
    "        objvals.append(obj)\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss = {obj:.6f}\")\n",
    "\n",
    "        if not np.isfinite(obj):\n",
    "            print(f\"Stopped early at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e37f6",
   "metadata": {},
   "source": [
    "Create a cross fold using the validation set created earlier to update lambda function:\n",
    "    -Key point here is to fit and transform the TF-IDF on training data only and transform the validation data on using the same TF-IDF vectorizer\n",
    "    -We only want the validation data to be used to update the lambda function and then average metrics across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab42207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(n, k=5, shuffle=True, seed=42):\n",
    "    idx = np.arrange(n)\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        rng.shuffle(idx)\n",
    "    folds = np.array_split(idx, k)\n",
    "    for i in range(k):\n",
    "        val_idx = folds[i]\n",
    "        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])\n",
    "        yield train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc64c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_logreg(w, b, X_train_tfidf, y_train, X_val_tfidf, y_val, learning_rate=0.01, lam=0.01, max_epochs=500, print_every=None):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        dw, db = gradient(w, b, X_train_tfidf, y_train, lam)\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        train_loss = objective(w, b, X_train_tfidf, y_train, lam)\n",
    "        val_loss = objective(w, b, X_val_tfidf, y_val, lam)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        if print_every is not None and (epoch % print_every == 0 or epoch == max_epochs - 1):\n",
    "            print(f\"Epoch {epoch:4d} | Train Loss={train_loss:.6f} | Val Loss={val_loss:.6f}\")\n",
    "\n",
    "        if not np.isfinite(train_loss):\n",
    "            print(f\"Stopped at epoch {epoch}: train loss diverged.\")\n",
    "            break\n",
    "    return w, b, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a49122",
   "metadata": {},
   "source": [
    "Now we can initilization the model to create a loop for it to begin to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ba7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Train Loss=0.691713 | Val Loss=0.691734\n",
      "Epoch   25 | Train Loss=0.658175 | Val Loss=0.658712\n",
      "Epoch   50 | Train Loss=0.628715 | Val Loss=0.629736\n",
      "Epoch   75 | Train Loss=0.602813 | Val Loss=0.604288\n",
      "Epoch  100 | Train Loss=0.580008 | Val Loss=0.581909\n",
      "Epoch  125 | Train Loss=0.559895 | Val Loss=0.562199\n",
      "Epoch  150 | Train Loss=0.542124 | Val Loss=0.544806\n",
      "Epoch  175 | Train Loss=0.526388 | Val Loss=0.529427\n",
      "Epoch  200 | Train Loss=0.512424 | Val Loss=0.515801\n",
      "Epoch  225 | Train Loss=0.500004 | Val Loss=0.503700\n",
      "Epoch  250 | Train Loss=0.488930 | Val Loss=0.492930\n",
      "Epoch  275 | Train Loss=0.479035 | Val Loss=0.483321\n",
      "Epoch  300 | Train Loss=0.470171 | Val Loss=0.474730\n",
      "Epoch  325 | Train Loss=0.462213 | Val Loss=0.467031\n",
      "Epoch  350 | Train Loss=0.455050 | Val Loss=0.460116\n",
      "Epoch  375 | Train Loss=0.448590 | Val Loss=0.453892\n",
      "Epoch  400 | Train Loss=0.442749 | Val Loss=0.448276\n",
      "Epoch  425 | Train Loss=0.437457 | Val Loss=0.443199\n",
      "Epoch  450 | Train Loss=0.432652 | Val Loss=0.438600\n",
      "Epoch  475 | Train Loss=0.428278 | Val Loss=0.434424\n",
      "Epoch  500 | Train Loss=0.424291 | Val Loss=0.430625\n",
      "Epoch  525 | Train Loss=0.420646 | Val Loss=0.427163\n",
      "Epoch  550 | Train Loss=0.417309 | Val Loss=0.424000\n",
      "Epoch  575 | Train Loss=0.414248 | Val Loss=0.421107\n",
      "Epoch  600 | Train Loss=0.411433 | Val Loss=0.418454\n",
      "Epoch  625 | Train Loss=0.408840 | Val Loss=0.416017\n",
      "Epoch  650 | Train Loss=0.406448 | Val Loss=0.413774\n",
      "Epoch  675 | Train Loss=0.404235 | Val Loss=0.411707\n",
      "Epoch  700 | Train Loss=0.402186 | Val Loss=0.409798\n",
      "Epoch  725 | Train Loss=0.400284 | Val Loss=0.408031\n",
      "Epoch  750 | Train Loss=0.398516 | Val Loss=0.406394\n",
      "Epoch  775 | Train Loss=0.396869 | Val Loss=0.404874\n",
      "Epoch  800 | Train Loss=0.395332 | Val Loss=0.403460\n",
      "Epoch  825 | Train Loss=0.393895 | Val Loss=0.402142\n",
      "Epoch  850 | Train Loss=0.392550 | Val Loss=0.400912\n",
      "Epoch  875 | Train Loss=0.391288 | Val Loss=0.399762\n",
      "Epoch  900 | Train Loss=0.390102 | Val Loss=0.398685\n",
      "Epoch  925 | Train Loss=0.388985 | Val Loss=0.397674\n",
      "Epoch  950 | Train Loss=0.387932 | Val Loss=0.396724\n",
      "Epoch  975 | Train Loss=0.386938 | Val Loss=0.395830\n",
      "Epoch  999 | Train Loss=0.386033 | Val Loss=0.395019\n"
     ]
    }
   ],
   "source": [
    "d =len(vocab)\n",
    "w0 = np.zeros(d)\n",
    "b0 = 0.0\n",
    "\n",
    "w_final, b_final, train_history, val_history = cross_validate_logreg(\n",
    "    w0,\n",
    "    b0,\n",
    "    X_train_tfidf,\n",
    "    y_train,\n",
    "    X_test_tfidf,\n",
    "    y_test,\n",
    "    lam=0.001,\n",
    "    learning_rate=0.01,\n",
    "    max_epochs=1000,\n",
    "    print_every=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67359c34",
   "metadata": {},
   "source": [
    "We need to make predictions for the test set to evaluate the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f480eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, w, b, threshold=0.5):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        z = dot_sparse(w, x) + b\n",
    "        p = 1.0 / (1.0 + np.exp(-z))\n",
    "        preds.append(1 if p >= threshold else 0)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7024b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = make_predictions(X_test_tfidf, w_final, b_final, threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a7d0e",
   "metadata": {},
   "source": [
    "Evaluation metrics after training:\n",
    "    -Implement accuracy, precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd612f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, verbose=True):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    accuracy  = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"tp\": tp,\n",
    "            \"tn\": tn,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn\n",
    "        }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy : {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall   : {recall:.4f}\")\n",
    "        print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03d836",
   "metadata": {},
   "source": [
    "Evaluate the preductions vs the actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "420e4715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8942\n",
      "Precision: 1.0000\n",
      "Recall   : 0.2338\n",
      "F1-score : 0.3789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': np.float64(0.8941704035874439),\n",
       " 'precision': np.float64(1.0),\n",
       " 'recall': np.float64(0.23376623376623376),\n",
       " 'f1_score': np.float64(0.3789473684210526),\n",
       " 'tp': np.int64(36),\n",
       " 'tn': np.int64(961),\n",
       " 'fp': np.int64(0),\n",
       " 'fn': np.int64(118)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
