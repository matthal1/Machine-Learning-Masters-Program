{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a01ebd",
   "metadata": {},
   "source": [
    "Binary Text Classification using Logistic Regression of ham and spam text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3708d",
   "metadata": {},
   "source": [
    "The workflow that this notebook will follow is as follows:\n",
    "\n",
    "1. Data Preprocessing: \n",
    "    -Load the dataset into sentences and labels\n",
    "    -Split the dataset into training, validation and testing sets \n",
    "    -Report the distribution in the form of a table\n",
    "    -Clean the data of any noise (urls, punctuation, and numbers) & change to lower case\n",
    "    -Tokenize input text into tokens, including work stemming and stopwords\n",
    "    -Build your own TD-IDF feature extractor using the training set\n",
    "2. Build a logistic regression classifier using using L2 regularization\n",
    "    -Derive the gradient of the objective function of LR with respect to w and b. \n",
    "    -Implement logistic regression via initialization, objective function, and gradient descent\n",
    "    -Implement accuracy, precision, recall and F1 score as test metrics\n",
    "    -Write a function for SGD and Mini-batch GD\n",
    "    -Evaluate the model of the test set and report the metrics \n",
    "3. Cross Validation\n",
    "    -Implement cross validation to choose the best hyperparameter lambda for the validation set\n",
    "4. Conclusion\n",
    "    -Analyze the results and compare to baseline\n",
    "5. Create a multiclass classifier from various authors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4616ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21b144",
   "metadata": {},
   "source": [
    "Load the dataset, include more information about what this importing section does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751a07e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv('a1-data/SMSSpamCollection', sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbfba93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79824c",
   "metadata": {},
   "source": [
    "Objective of the split_dataset function: \n",
    "    -Split the dataset ino training, validation and test sets\n",
    "    -Return each set split into features and labels (this enables easier tokenization later)\n",
    "    -This also allows for reproducubility as the split will not be exactly the same each time meaning the if the structure of the model is effecive it should learn at the same rate regardless of the data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c3da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_size, val_size, test_size):\n",
    "    df = df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    n = (len(df))\n",
    "    train_end = int(train_size * n)\n",
    "    val_end = train_end + int(val_size * n)\n",
    "\n",
    "    train_df = df.iloc[:train_end]\n",
    "    val_df = df.iloc[train_end:val_end]\n",
    "    test_df = df.iloc[val_end:n]\n",
    "\n",
    "    X_train, y_train = train_df[['text']], train_df['label']\n",
    "    X_val, y_val = val_df[['text']], val_df['label']\n",
    "    X_test, y_test = test_df[['text']], test_df['label']\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(spam_df, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e2ab6",
   "metadata": {},
   "source": [
    "Output a table showing the number of samples in each class for the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098e7e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Val</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>2898</td>\n",
       "      <td>966</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>445</td>\n",
       "      <td>148</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Train  Val  Test\n",
       "label                  \n",
       "ham     2898  966   961\n",
       "spam     445  148   154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_distribution(y_train, y_val, y_test):\n",
    "    df =pd.DataFrame({'Train': y_train.value_counts(), 'Val': y_val.value_counts(), 'Test': y_test.value_counts()}).fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "data_distribution(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbe3ca",
   "metadata": {},
   "source": [
    "Objective of the clean_data function:\n",
    "    -Remove punctuation, urls and numbers\n",
    "    -Change text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364f7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(X):\n",
    "    X = X.str.lower()\n",
    "    X = X.str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    X = X.str.replace(\"http\\\\S+\", \"\", regex=True)\n",
    "    X = X.str.replace(\"https\\\\S+\", \"\", regex=True)\n",
    "    X = X.str.replace(\"\\\\d+\", \"\", regex=True)\n",
    "    return X\n",
    "\n",
    "# Convert spam and ham to 0 and 1s for classification\n",
    "map = {\"spam\": 1, \"ham\": 0}\n",
    "y_train = np.array([map[v] for v in y_train], dtype=float)\n",
    "y_val = np.array([map[v] for v in y_val], dtype=float)\n",
    "y_test = np.array([map[v] for v in y_test], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb62c8",
   "metadata": {},
   "source": [
    "Tokenize the dataset:\n",
    "    -Remove whitespace between words\n",
    "    -Including word stems\n",
    "    -Removing stop words (removing common words that do not add any semantic value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\",\n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\",\n",
    "    \"to\", \"of\", \"in\", \"on\", \"for\", \"with\",\n",
    "    \"that\", \"this\", \"it\", \"as\", \"at\"\n",
    "}\n",
    "\n",
    "def tokenize_text(X):\n",
    "    return X.apply(lambda x: x.split())\n",
    "\n",
    "def remove_stopwords(tokens, stopwords = STOPWORDS):\n",
    "    return tokens.apply(lambda x: [t for t in x if t not in stopwords])\n",
    "\n",
    "def stem_token(token):\n",
    "    suffixes = [\"ing\", \"ly\", \"ed\", \"s\", \"es\", \"est\"]\n",
    "    for suf in suffixes:\n",
    "        if token.endswith(suf) and len(token) > len(suf) + 2:\n",
    "            return token[:-len(suf)]\n",
    "    return token\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return tokens.apply(lambda x: [stem_token(t) for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7848012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[squeeeeeze, christma, hug, if, u, lik, my, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[also, ive, sorta, blown, him, off, couple, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[mmm, that, better, now, i, got, roast, down, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[mm, have, some, kanji, dont, eat, anyth, heav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[so, there, ring, come, guy, costume, its, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [squeeeeeze, christma, hug, if, u, lik, my, fr...\n",
       "1  [also, ive, sorta, blown, him, off, couple, ti...\n",
       "2  [mmm, that, better, now, i, got, roast, down, ...\n",
       "3  [mm, have, some, kanji, dont, eat, anyth, heav...\n",
       "4  [so, there, ring, come, guy, costume, its, the..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(X):\n",
    "    X  = X.copy()\n",
    "    X = clean_text(X)\n",
    "    X = tokenize_text(X)\n",
    "    X = remove_stopwords(X)\n",
    "    X = stem_tokens(X)\n",
    "    return X\n",
    "\n",
    "X_train['text'] = preprocess_text(X_train['text'])\n",
    "X_val['text'] = preprocess_text(X_val['text'])\n",
    "X_test['text'] = preprocess_text(X_test['text'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01733485",
   "metadata": {},
   "source": [
    "Build a TD-IDF vecotrizorizer from stratch:\n",
    "    -Goal is to create a function that would take in the list of words we have and return a matrix of importance of each word\n",
    "    -TF = Term Frequency: The more the word appears in the document, the higher the TF\n",
    "    -IDF = Inverse Document Frequency: The less the word appears in the corpus, the higher the IDF\n",
    "What the functions do: \n",
    "    -fit_tfidf:\n",
    "        -Treats each row as a new \"document and counts the total number of documents\n",
    "        -Builds the document frequency required for IDF by converting the token list to a set so each word is counted at most once per document\n",
    "        -Update the voacb to say that the words appears in a specific document\n",
    "        -Building a vocabulary mapping and sorts the indices alphabetically\n",
    "        -Computes IDF for each word in the documents\n",
    "    -transform_tfidf:\n",
    "        -Counts term occurrences within that document\n",
    "        -Computes TF per term, typically count / len(doc)\n",
    "        -Computes TF-IDF per term\n",
    "        -Stores the result as a sparse vector \n",
    "        -Returns a list of vectors per document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94fb8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tfidf(text):\n",
    "    # Document frequency: df[word] = number of docs containing word\n",
    "    df = {}\n",
    "    N = 0\n",
    "\n",
    "    for doc in text:\n",
    "        N += 1\n",
    "        seen = set(doc)\n",
    "        for w in seen:\n",
    "            df[w] = df.get(w, 0) + 1\n",
    "\n",
    "    # Build vocab (deterministic order: alphabetical)\n",
    "    vocab_words = sorted(df.keys())\n",
    "    vocab = {w: i for i, w in enumerate(vocab_words)}\n",
    "\n",
    "    idf = [0.0] * len(vocab_words)\n",
    "    for w, i in vocab.items():\n",
    "        idf[i] = math.log((1.0 + N) / (1.0 + df[w])) + 1.0\n",
    "\n",
    "    return vocab, idf\n",
    "\n",
    "\n",
    "def transform_tfidf(text, vocab, idf, normalize=True):\n",
    "    vectors = []\n",
    "\n",
    "    for doc in text:\n",
    "        # term counts\n",
    "        counts = {}\n",
    "        for w in doc:\n",
    "            if w in vocab:\n",
    "                idx = vocab[w]\n",
    "                counts[idx] = counts.get(idx, 0) + 1\n",
    "\n",
    "        # compute TF-IDF (using TF = count / len(doc))\n",
    "        doc_len = len(doc) if len(doc) > 0 else 1\n",
    "        vec = {}\n",
    "        for idx, c in counts.items():\n",
    "            tf = c / doc_len\n",
    "            vec[idx] = tf * idf[idx]\n",
    "\n",
    "        '''\n",
    "        # optional L2 normalization\n",
    "        if normalize and vec:\n",
    "            import math\n",
    "            norm = math.sqrt(sum(v * v for v in vec.values()))\n",
    "            if norm > 0:\n",
    "                for idx in list(vec.keys()):\n",
    "                    vec[idx] = vec[idx] / norm\n",
    "        '''\n",
    "        vectors.append(vec)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ade02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{4589: 0.29040606184981543,\n",
       "  873: 0.24260280801809503,\n",
       "  2255: 0.5330088698679104,\n",
       "  2305: 0.26567122657239356,\n",
       "  5139: 0.6067940829285435,\n",
       "  2731: 0.25880982971622385,\n",
       "  3174: 0.10983909761429993,\n",
       "  1809: 0.29040606184981543,\n",
       "  1214: 0.22092664734835799,\n",
       "  2952: 0.10849018669721469,\n",
       "  373: 0.15991676412849606,\n",
       "  1889: 0.12648915870821661,\n",
       "  3907: 0.48333845355664923,\n",
       "  1113: 0.25880982971622385,\n",
       "  2847: 0.29040606184981543,\n",
       "  4481: 0.12203358176063021,\n",
       "  2836: 0.22330571188239082,\n",
       "  3295: 0.2764245063978097,\n",
       "  3596: 0.1970250204324978,\n",
       "  2081: 0.25252287948194957},\n",
       " {159: 0.280564912646649,\n",
       "  2444: 0.2842816267105413,\n",
       "  4527: 0.4432513575602446,\n",
       "  544: 0.4432513575602446,\n",
       "  2158: 0.5233461313487431,\n",
       "  3378: 0.2896844769242299,\n",
       "  1042: 0.35854409901108136,\n",
       "  4968: 0.22436277422552497,\n",
       "  3978: 0.377316464481541,\n",
       "  4481: 0.18626178268727767,\n",
       "  2294: 0.3276079587530751,\n",
       "  3938: 0.3854296581566598,\n",
       "  3311: 0.19105926530073353,\n",
       "  4877: 0.22775953849592345,\n",
       "  3487: 0.20881202617848102,\n",
       "  546: 0.3854296581566598,\n",
       "  2782: 0.2782253461955525,\n",
       "  5399: 0.37028849644867134},\n",
       " {3059: 0.38172908026364194,\n",
       "  4893: 0.2449824314596414,\n",
       "  485: 0.5322440428179458,\n",
       "  3324: 0.1645447980211891,\n",
       "  2281: 0.21193634912858345,\n",
       "  1960: 0.19941282328797083,\n",
       "  4106: 0.38172908026364194,\n",
       "  1363: 0.5322440428179458,\n",
       "  2952: 0.2996395632589739,\n",
       "  2448: 0.38172908026364194,\n",
       "  361: 0.2487191750520936,\n",
       "  2305: 0.18343965644284319,\n",
       "  2040: 0.2360018995640438,\n",
       "  1665: 0.27208217107344934,\n",
       "  1381: 0.2964072007765918,\n",
       "  1942: 0.2008134844882705,\n",
       "  2349: 0.34872207166554936}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs = X_train['text'].tolist()\n",
    "vocab, idf = fit_tfidf(train_docs)\n",
    "X_train_tfidf = transform_tfidf(train_docs, vocab, idf)\n",
    "\n",
    "X_train_tfidf[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ddf23",
   "metadata": {},
   "source": [
    "We only transform the validation and test sets using the transform method to not introduce any data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b03f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doc = X_val['text'].tolist()\n",
    "X_val_tfidf = transform_tfidf(val_doc, vocab, idf)\n",
    "\n",
    "test_doc = X_test['text'].tolist()\n",
    "X_test_tfidf = transform_tfidf(test_doc, vocab, idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9ef19",
   "metadata": {},
   "source": [
    "Next we need to create a logistic regression model with L2 normalization:\n",
    "    -Import Equation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b4dc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sparse(w, x):\n",
    "    s = 0\n",
    "    for j, v in x.items():\n",
    "        s += w[j]*v\n",
    "    return s\n",
    "    \n",
    "def objective(w, b, X, y, lam):\n",
    "    n = len(X)\n",
    "    eps = 1e-15\n",
    "    loss=0\n",
    "    for x_i, y_i in zip(X,y):\n",
    "        # Linear Transformation of the objective function\n",
    "        z = dot_sparse(w, x_i) + b\n",
    "        # Pass the linear transformation through the sigmoid activation function\n",
    "        y_hat = 1 / (1 + np.exp(-z))\n",
    "        # Binary Cross Entropy Loss\n",
    "        # This ensures we do not have any log(0) errors\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "        loss += -(y_i * np.log(y_hat) + (1-y_i) * np.log(1-y_hat))\n",
    "        \n",
    "    loss = loss / n\n",
    "    # L2 regularization\n",
    "    reg = lam * np.sum(w ** 2)\n",
    "\n",
    "    return loss + reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22200a",
   "metadata": {},
   "source": [
    "Generate a function for the gradient and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9616370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w, b, X, y, lam):\n",
    "    n = len(X)\n",
    "    d = len(w)\n",
    "    dw = np.zeros(d)\n",
    "    db = 0.0\n",
    "\n",
    "    for x_i, y_i in zip(X, y):\n",
    "    # Get predictions\n",
    "        z = dot_sparse(w, x_i) + b\n",
    "        y_hat = 1 / (1 + np.exp(-z))\n",
    "        # Error term\n",
    "        error = y_hat - y_i\n",
    "\n",
    "        for j, v, in x_i.items():\n",
    "            dw[j] += error * v\n",
    "        db += error\n",
    "    dw = (1.0 / n) * dw + 2 * lam * w\n",
    "    db = (1.0 / n) * db\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e160aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, X, y, lam, learning_rate, max_epochs=100, print_every=50):\n",
    "    objvals = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Compute gradients for both the weight and bias\n",
    "        dw, db = gradient(w, b, X, y, lam)\n",
    "\n",
    "        # Update parameters using the learning rate\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        # Update the objective value\n",
    "        obj = objective(w, b, X, y, lam)\n",
    "        objvals.append(obj)\n",
    "        # Update progress during training\n",
    "        if epoch % print_every == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss = {obj:.6f}\")\n",
    "        # If gradient starts to diverge we should stop early\n",
    "        if not np.isfinite(obj):\n",
    "            print('Stopped early at {epoch}')\n",
    "            break\n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3792e6",
   "metadata": {},
   "source": [
    "A secondary objective is to implement a stochastic gradient descent and mini-batch gradient descent function for this model\n",
    "This requires a new gradient function and training function for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c67f5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_sgd(w, b, x_i, y_i, lam):\n",
    "    d = len(w)\n",
    "    dw = np.zeros(d)\n",
    "\n",
    "    z = dot_sparse(w, x_i) + b\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    error = y_hat - y_i\n",
    "    \n",
    "    for j, v in x_i.items():\n",
    "        dw[j] += error * v\n",
    "    \n",
    "    db = error\n",
    "\n",
    "    dw += 2 * lam * w\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bdd8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, b, X, y, lam, learning_rate, max_epochs=10, print_every=100, shuffle=True):\n",
    "    objvals = []\n",
    "    n = len(X)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            # single-sample gradient\n",
    "            dw, db = gradient_sgd(w, b, X[i], y[i], lam)\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "        # track loss once per epoch\n",
    "        obj = objective(w, b, X, y, lam)\n",
    "        objvals.append(obj)\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss = {obj:.6f}\")\n",
    "\n",
    "        if not np.isfinite(obj):\n",
    "            print(f\"Stopped early at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3445d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_minibatch(w, b, X_batch, y_batch, lam):\n",
    "    batch_size = len(X_batch)\n",
    "    d = len(w)\n",
    "\n",
    "    grad_w = np.zeros(d)\n",
    "    grad_b = 0.0\n",
    "\n",
    "    for x_i, y_i in zip(X_batch, y_batch):\n",
    "        z = dot_sparse(w, x_i) + b\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-z))\n",
    "        error = y_hat - y_i\n",
    "\n",
    "        for j, v in x_i.items():\n",
    "            grad_w[j] += error * v\n",
    "\n",
    "        grad_b += error\n",
    "\n",
    "    # average over batch + L2\n",
    "    dw = (1.0 / batch_size) * grad_w + 2 * lam * w\n",
    "    db = (1.0 / batch_size) * grad_b\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d455dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_minibatch(\n",
    "    w, b, X, y, lam, learning_rate,\n",
    "    max_epochs=100, batch_size=32,\n",
    "    shuffle=True, print_every=50\n",
    "):\n",
    "    objvals = []\n",
    "    n = len(X)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # shuffle indices each epoch\n",
    "        indices = np.arange(n)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        # iterate over mini-batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "\n",
    "            X_batch = [X[i] for i in batch_idx]\n",
    "            y_batch = y[batch_idx] \n",
    "\n",
    "            dw, db = gradient_minibatch(w, b, X_batch, y_batch, lam)\n",
    "\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "        # track full training loss once per epoch\n",
    "        obj = objective(w, b, X, y, lam)\n",
    "        objvals.append(obj)\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss = {obj:.6f}\")\n",
    "\n",
    "        if not np.isfinite(obj):\n",
    "            print(f\"Stopped early at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return w, b, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67359c34",
   "metadata": {},
   "source": [
    "We need to make predictions for the test set to evaluate the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f480eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, w, b, threshold=0.5):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        z = dot_sparse(w, x) + b\n",
    "        p = 1.0 / (1.0 + np.exp(-z))\n",
    "        preds.append(1 if p >= threshold else 0)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a7d0e",
   "metadata": {},
   "source": [
    "Evaluation metrics after training:\n",
    "    -Implement accuracy, precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd612f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, verbose=True):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    accuracy  = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"tp\": tp,\n",
    "            \"tn\": tn,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn\n",
    "        }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy : {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall   : {recall:.4f}\")\n",
    "        print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd4bbd",
   "metadata": {},
   "source": [
    "Create a cross fold using the validation set created earlier to update lambda function:\n",
    "    -Key point here is to fit and transform the TF-IDF on training data only and transform the validation data on using the same TF-IDF vectorizer\n",
    "    -We only want the validation data to be used to update the lambda function and then average metrics across folds\n",
    "    -This functiion will have 3 variations in the final code; one for each gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1b59118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_gd(X_train, y_train, X_val, y_val, lambdas, lr, epochs):\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Determine weight vector size (d) from training data\n",
    "    # (Assuming X_train is a list of TF-IDF dicts)\n",
    "    all_indices = [idx for doc in X_train for idx in doc.keys()]\n",
    "    d = max(all_indices) + 1 if all_indices else 0\n",
    "    \n",
    "    print(f\"Starting Validation on {len(lambdas)} candidates...\")\n",
    "\n",
    "    for lam in lambdas:\n",
    "        # 2. ALWAYS reset weights for each new lambda\n",
    "        w = np.zeros(d)\n",
    "        b = 0.0\n",
    "        \n",
    "        # 3. Train on the fixed Training Set\n",
    "        # Note: 'history' contains the loss per epoch if you want to plot it\n",
    "        w_final, b_final, history = gradient_descent(\n",
    "            w, b, X_train, y_train, lam, lr, epochs, print_every=100\n",
    "        )\n",
    "        \n",
    "        # 4. Evaluate on the 'Held-out' Validation Set\n",
    "        # Use the objective function (Loss) to see how well it generalizes\n",
    "        val_loss = objective(w_final, b_final, X_val, y_val, lam)\n",
    "        \n",
    "        results[lam] = val_loss\n",
    "        print(f\"Lambda: {lam:6.3f} | Validation Loss: {val_loss:.6f}\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddc70abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Validation on 5 candidates...\n",
      "Epoch    0 | Loss = 0.691713\n",
      "Epoch  100 | Loss = 0.580008\n",
      "Epoch  200 | Loss = 0.512424\n",
      "Epoch  300 | Loss = 0.470171\n",
      "Epoch  400 | Loss = 0.442749\n",
      "Epoch  499 | Loss = 0.424443\n",
      "Lambda:  0.001 | Validation Loss: 0.430770\n",
      "Epoch    0 | Loss = 0.691713\n",
      "Epoch  100 | Loss = 0.580132\n",
      "Epoch  200 | Loss = 0.512812\n",
      "Epoch  300 | Loss = 0.470879\n",
      "Epoch  400 | Loss = 0.443802\n",
      "Epoch  499 | Loss = 0.425853\n",
      "Lambda:  0.010 | Validation Loss: 0.432135\n",
      "Epoch    0 | Loss = 0.691713\n",
      "Epoch  100 | Loss = 0.581220\n",
      "Epoch  200 | Loss = 0.515754\n",
      "Epoch  300 | Loss = 0.475534\n",
      "Epoch  400 | Loss = 0.449834\n",
      "Epoch  499 | Loss = 0.432942\n",
      "Lambda:  0.100 | Validation Loss: 0.438900\n",
      "Epoch    0 | Loss = 0.691714\n",
      "Epoch  100 | Loss = 0.584986\n",
      "Epoch  200 | Loss = 0.520735\n",
      "Epoch  300 | Loss = 0.480118\n",
      "Epoch  400 | Loss = 0.453675\n",
      "Epoch  499 | Loss = 0.436131\n",
      "Lambda:  1.000 | Validation Loss: 0.441601\n",
      "Epoch    0 | Loss = 0.691722\n",
      "Epoch  100 | Loss = 0.586089\n",
      "Epoch  200 | Loss = 0.521263\n",
      "Epoch  300 | Loss = 0.480385\n",
      "Epoch  400 | Loss = 0.453843\n",
      "Epoch  499 | Loss = 0.436271\n",
      "Lambda: 10.000 | Validation Loss: 0.441674\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "lambdas_to_test = [0.001, 0.01, 0.1, 1, 10]\n",
    "cv_results = validation_gd(X_train_tfidf, y_train, X_test_tfidf, y_test, lambdas=lambdas_to_test, lr=0.01, epochs = 500)\n",
    "best_lambda =  min(cv_results, key=cv_results.get)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d175bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_sgd(X_train, y_train, X_val, y_val, lambdas, lr, epochs):\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Determine weight vector size (d) from training data\n",
    "    # (Assuming X_train is a list of TF-IDF dicts)\n",
    "    all_indices = [idx for doc in X_train for idx in doc.keys()]\n",
    "    d = max(all_indices) + 1 if all_indices else 0\n",
    "    \n",
    "    print(f\"Starting Validation on {len(lambdas)} candidates...\")\n",
    "\n",
    "    for lam in lambdas:\n",
    "        # 2. ALWAYS reset weights for each new lambda\n",
    "        w = np.zeros(d)\n",
    "        b = 0.0\n",
    "        \n",
    "        # 3. Train on the fixed Training Set\n",
    "        # Note: 'history' contains the loss per epoch if you want to plot it\n",
    "        w_final, b_final, history = sgd(\n",
    "            w, b, X_train, y_train, lam, lr, epochs, print_every=100\n",
    "        )\n",
    "        \n",
    "        # 4. Evaluate on the Validation Set\n",
    "        # Use the objective function (Loss) to see how well it generalizes\n",
    "        val_loss = objective(w_final, b_final, X_val, y_val, lam)\n",
    "        \n",
    "        results[lam] = val_loss\n",
    "        print(f\"Lambda: {lam:6.3f} | Validation Loss: {val_loss:.6f}\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fd5630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Validation on 5 candidates...\n",
      "Epoch    0 | Loss = 0.352875\n",
      "Epoch  100 | Loss = 0.243251\n",
      "Epoch  200 | Loss = 0.243203\n",
      "Epoch  300 | Loss = 0.243213\n",
      "Epoch  400 | Loss = 0.243354\n",
      "Epoch  499 | Loss = 0.243199\n",
      "Lambda:  0.001 | Validation Loss: 0.271996\n",
      "Epoch    0 | Loss = 0.369421\n",
      "Epoch  100 | Loss = 0.363053\n",
      "Epoch  200 | Loss = 0.364274\n",
      "Epoch  300 | Loss = 0.363041\n",
      "Epoch  400 | Loss = 0.363039\n",
      "Epoch  499 | Loss = 0.363048\n",
      "Lambda:  0.010 | Validation Loss: 0.369985\n",
      "Epoch    0 | Loss = 0.389631\n",
      "Epoch  100 | Loss = 0.389577\n",
      "Epoch  200 | Loss = 0.389782\n",
      "Epoch  300 | Loss = 0.390614\n",
      "Epoch  400 | Loss = 0.389608\n",
      "Epoch  499 | Loss = 0.389569\n",
      "Lambda:  0.100 | Validation Loss: 0.389956\n",
      "Epoch    0 | Loss = 0.392638\n",
      "Epoch  100 | Loss = 0.392572\n",
      "Epoch  200 | Loss = 0.392647\n",
      "Epoch  300 | Loss = 0.392637\n",
      "Epoch  400 | Loss = 0.394034\n",
      "Epoch  499 | Loss = 0.392828\n",
      "Lambda:  1.000 | Validation Loss: 0.392381\n",
      "Epoch    0 | Loss = 0.392724\n",
      "Epoch  100 | Loss = 0.392512\n",
      "Epoch  200 | Loss = 0.392884\n",
      "Epoch  300 | Loss = 0.392619\n",
      "Epoch  400 | Loss = 0.392976\n",
      "Epoch  499 | Loss = 0.392537\n",
      "Lambda: 10.000 | Validation Loss: 0.392052\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "cv_results = validation_sgd(X_train_tfidf, y_train, X_val_tfidf, y_val, lambdas=lambdas_to_test, lr=0.01, epochs = 500)\n",
    "best_lambda =  min(cv_results, key=cv_results.get)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1669a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_minibatch(X_train, y_train, X_val, y_val, lambdas, lr, epochs):\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Determine weight vector size (d) from training data\n",
    "    # (Assuming X_train is a list of TF-IDF dicts)\n",
    "    all_indices = [idx for doc in X_train for idx in doc.keys()]\n",
    "    d = max(all_indices) + 1 if all_indices else 0\n",
    "    \n",
    "    print(f\"Starting Validation on {len(lambdas)} candidates...\")\n",
    "\n",
    "    for lam in lambdas:\n",
    "        # 2. ALWAYS reset weights for each new lambda\n",
    "        w = np.zeros(d)\n",
    "        b = 0.0\n",
    "        \n",
    "        # 3. Train on the fixed Training Set\n",
    "        # Note: 'history' contains the loss per epoch if you want to plot it\n",
    "        w_final, b_final, history = gradient_descent_minibatch(\n",
    "            w, b, X_train, y_train, lam, lr, epochs, print_every=100\n",
    "        )\n",
    "        \n",
    "        # 4. Evaluate on the 'Validation Set\n",
    "        # Use the objective function (Loss) to see how well it generalizes\n",
    "        val_loss = objective(w_final, b_final, X_val, y_val, lam)\n",
    "        \n",
    "        results[lam] = val_loss\n",
    "        print(f\"Lambda: {lam:6.3f} | Validation Loss: {val_loss:.6f}\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce23bee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Validation on 5 candidates...\n",
      "Epoch    0 | Loss = 0.576502\n",
      "Epoch  100 | Loss = 0.300758\n",
      "Epoch  200 | Loss = 0.266621\n",
      "Epoch  300 | Loss = 0.253238\n",
      "Epoch  400 | Loss = 0.247724\n",
      "Epoch  499 | Loss = 0.245343\n",
      "Lambda:  0.001 | Validation Loss: 0.270833\n",
      "Epoch    0 | Loss = 0.576231\n",
      "Epoch  100 | Loss = 0.363267\n",
      "Epoch  200 | Loss = 0.363008\n",
      "Epoch  300 | Loss = 0.363005\n",
      "Epoch  400 | Loss = 0.363005\n",
      "Epoch  499 | Loss = 0.363005\n",
      "Lambda:  0.010 | Validation Loss: 0.369845\n",
      "Epoch    0 | Loss = 0.577953\n",
      "Epoch  100 | Loss = 0.389058\n",
      "Epoch  200 | Loss = 0.389058\n",
      "Epoch  300 | Loss = 0.389057\n",
      "Epoch  400 | Loss = 0.389058\n",
      "Epoch  499 | Loss = 0.389058\n",
      "Lambda:  0.100 | Validation Loss: 0.389405\n",
      "Epoch    0 | Loss = 0.581901\n",
      "Epoch  100 | Loss = 0.391947\n",
      "Epoch  200 | Loss = 0.391947\n",
      "Epoch  300 | Loss = 0.391947\n",
      "Epoch  400 | Loss = 0.391946\n",
      "Epoch  499 | Loss = 0.391949\n",
      "Lambda:  1.000 | Validation Loss: 0.391550\n",
      "Epoch    0 | Loss = 0.582723\n",
      "Epoch  100 | Loss = 0.392276\n",
      "Epoch  200 | Loss = 0.392279\n",
      "Epoch  300 | Loss = 0.392266\n",
      "Epoch  400 | Loss = 0.392273\n",
      "Epoch  499 | Loss = 0.392288\n",
      "Lambda: 10.000 | Validation Loss: 0.391813\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "cv_results = validation_minibatch(X_train_tfidf, y_train, X_val_tfidf, y_val, lambdas=lambdas_to_test, lr=0.01, epochs =500)\n",
    "best_lambda =  min(cv_results, key=cv_results.get)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5b3b8",
   "metadata": {},
   "source": [
    "Retrain each model only on the training set with the fine tuned lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cd64693",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = [idx for doc in X_train_tfidf for idx in doc.keys()]\n",
    "d = max(all_indices) + 1 if all_indices else 0\n",
    "\n",
    "w = np.zeros(d)\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb12eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss = 0.691713\n",
      "Epoch   50 | Loss = 0.628715\n",
      "Epoch  100 | Loss = 0.580008\n",
      "Epoch  150 | Loss = 0.542124\n",
      "Epoch  200 | Loss = 0.512424\n",
      "Epoch  250 | Loss = 0.488930\n",
      "Epoch  300 | Loss = 0.470171\n",
      "Epoch  350 | Loss = 0.455050\n",
      "Epoch  400 | Loss = 0.442749\n",
      "Epoch  450 | Loss = 0.432652\n",
      "Epoch  500 | Loss = 0.424291\n",
      "Epoch  550 | Loss = 0.417309\n",
      "Epoch  600 | Loss = 0.411433\n",
      "Epoch  650 | Loss = 0.406448\n",
      "Epoch  700 | Loss = 0.402186\n",
      "Epoch  750 | Loss = 0.398516\n",
      "Epoch  800 | Loss = 0.395332\n",
      "Epoch  850 | Loss = 0.392550\n",
      "Epoch  900 | Loss = 0.390102\n",
      "Epoch  950 | Loss = 0.387932\n",
      "Epoch  999 | Loss = 0.386033\n"
     ]
    }
   ],
   "source": [
    "w_gd, b_gd, obj_vals_gd = gradient_descent(w, b, X_train_tfidf, y_train, lam=0.001, learning_rate=0.01, max_epochs=1000, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bcbbaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss = 0.352426\n",
      "Epoch   50 | Loss = 0.243205\n",
      "Epoch  100 | Loss = 0.243200\n",
      "Epoch  150 | Loss = 0.243324\n",
      "Epoch  200 | Loss = 0.243207\n",
      "Epoch  250 | Loss = 0.243195\n",
      "Epoch  300 | Loss = 0.243330\n",
      "Epoch  350 | Loss = 0.243203\n",
      "Epoch  400 | Loss = 0.243204\n",
      "Epoch  450 | Loss = 0.243216\n",
      "Epoch  500 | Loss = 0.243219\n",
      "Epoch  550 | Loss = 0.243218\n",
      "Epoch  600 | Loss = 0.243245\n",
      "Epoch  650 | Loss = 0.243277\n",
      "Epoch  700 | Loss = 0.243259\n",
      "Epoch  750 | Loss = 0.243229\n",
      "Epoch  800 | Loss = 0.243203\n",
      "Epoch  850 | Loss = 0.243196\n",
      "Epoch  900 | Loss = 0.243210\n",
      "Epoch  950 | Loss = 0.243287\n",
      "Epoch  999 | Loss = 0.243198\n"
     ]
    }
   ],
   "source": [
    "w_sgd, b_sgd, obj_vals_sgd = sgd(w, b, X_train_tfidf, y_train, lam=0.001, learning_rate=0.01, max_epochs=1000, print_every=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb673936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss = 0.576424\n",
      "Epoch   50 | Loss = 0.334676\n",
      "Epoch  100 | Loss = 0.300768\n",
      "Epoch  150 | Loss = 0.279705\n",
      "Epoch  200 | Loss = 0.266631\n",
      "Epoch  250 | Loss = 0.258441\n",
      "Epoch  300 | Loss = 0.253240\n",
      "Epoch  350 | Loss = 0.249904\n",
      "Epoch  400 | Loss = 0.247727\n",
      "Epoch  450 | Loss = 0.246290\n",
      "Epoch  500 | Loss = 0.245334\n",
      "Epoch  550 | Loss = 0.244686\n",
      "Epoch  600 | Loss = 0.244246\n",
      "Epoch  650 | Loss = 0.243942\n",
      "Epoch  700 | Loss = 0.243731\n",
      "Epoch  750 | Loss = 0.243583\n",
      "Epoch  800 | Loss = 0.243478\n",
      "Epoch  850 | Loss = 0.243403\n",
      "Epoch  900 | Loss = 0.243349\n",
      "Epoch  950 | Loss = 0.243309\n",
      "Epoch  999 | Loss = 0.243281\n"
     ]
    }
   ],
   "source": [
    "w_mgd, b_mgd, obj_vals_mgd = gradient_descent_minibatch(w, b, X_train_tfidf, y_train, lam=0.001, learning_rate=0.01, max_epochs=1000, batch_size=32, shuffle=True, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be447b5",
   "metadata": {},
   "source": [
    "Make prediction for each fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f57902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from gradient descent training\n",
    "preds_gd = make_predictions(X_test_tfidf, w_gd, b_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5d5a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from stochastic gradient descent training\n",
    "preds_sgd = make_predictions(X_test_tfidf, w_sgd, b_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0051b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from mini-batch gradient descent training\n",
    "preds_mgd = make_predictions(X_test_tfidf, w_mgd, b_mgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03d836",
   "metadata": {},
   "source": [
    "Evaluate the predictions vs the actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "420e4715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8619\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': np.float64(0.8618834080717489),\n",
       " 'precision': 0.0,\n",
       " 'recall': np.float64(0.0),\n",
       " 'f1_score': 0.0,\n",
       " 'tp': np.int64(0),\n",
       " 'tn': np.int64(961),\n",
       " 'fp': np.int64(0),\n",
       " 'fn': np.int64(154)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_test, preds_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4243b7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9067\n",
      "Precision: 0.9808\n",
      "Recall   : 0.3312\n",
      "F1-score : 0.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': np.float64(0.9067264573991032),\n",
       " 'precision': np.float64(0.9807692307692307),\n",
       " 'recall': np.float64(0.33116883116883117),\n",
       " 'f1_score': np.float64(0.4951456310679612),\n",
       " 'tp': np.int64(51),\n",
       " 'tn': np.int64(960),\n",
       " 'fp': np.int64(1),\n",
       " 'fn': np.int64(103)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_test, preds_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "105871ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9067\n",
      "Precision: 0.9808\n",
      "Recall   : 0.3312\n",
      "F1-score : 0.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': np.float64(0.9067264573991032),\n",
       " 'precision': np.float64(0.9807692307692307),\n",
       " 'recall': np.float64(0.33116883116883117),\n",
       " 'f1_score': np.float64(0.4951456310679612),\n",
       " 'tp': np.int64(51),\n",
       " 'tn': np.int64(960),\n",
       " 'fp': np.int64(1),\n",
       " 'fn': np.int64(103)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_test, preds_mgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89194f2a",
   "metadata": {},
   "source": [
    "Multiclass Classification Logistic Regreesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7cbaaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>PERSUASION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>by Jane Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>(1818)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Chapter 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Sir Walter Elliot, of Kellynch Hall, in Somers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "0  Jane Austen                                        PERSUASION\n",
       "1  Jane Austen                                     by Jane Austen\n",
       "2  Jane Austen                                             (1818)\n",
       "3  Jane Austen                                          Chapter 1\n",
       "4  Jane Austen  Sir Walter Elliot, of Kellynch Hall, in Somers..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df = pd.read_csv('a1-data/books.txt', sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad594e",
   "metadata": {},
   "source": [
    "Steps:\n",
    "    -Split data into train, validation and test \n",
    "    -Use the preprocess function to clean all text\n",
    "    -Map the labels for multiclass classification\n",
    "    -Use tf-idf function to train on the new information and create new vectors\n",
    "    -Use transform_idf on all three dataset\n",
    "    -Rebuild objective function for categorical cross entropy loss (new objective function)\n",
    "    -Update gradient function to include softmax in place of sigmoid as the activation function\n",
    "    -Make predictions \n",
    "    -Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67db092",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_train = split_dataset(books_df, 0.6, 0.2, 0.2)\n",
    "\n",
    "\n",
    "X_train['text'] = preprocess_text(X_train['text'])\n",
    "X_val['text'] = preprocess_text(X_val['text'])\n",
    "X_test['text'] = preprocess_text(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "606bd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {\"Arthur Conan Doyle\": 2, \"Jane Austen\": 1, \"Fyodor Dostoyevsky\": 0}\n",
    "y_train = np.array([map[v] for v in y_train], dtype=float)\n",
    "y_val = np.array([map[v] for v in y_val], dtype=float)\n",
    "y_test = np.array([map[v] for v in y_test], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "821ef8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = X_train['text'].tolist()\n",
    "vocab, idf = fit_tfidf(train_docs)\n",
    "X_train_tfidf = transform_tfidf(train_docs, vocab, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d41e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doc = X_val['text'].tolist()\n",
    "X_val_tfidf = transform_tfidf(val_doc, vocab, idf)\n",
    "\n",
    "test_doc = X_test['text'].tolist()\n",
    "X_test_tfidf = transform_tfidf(test_doc, vocab, idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc17d09",
   "metadata": {},
   "source": [
    "We need to create a softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c3ac57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Subtracting max(z) is a common trick to prevent numerical overflow (Exp getting too big)\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / exp_z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdd4b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cce(w, b, X, y, lam):\n",
    "    \"\"\"\n",
    "    y: One-hot encoded labels (n_samples, n_classes)\n",
    "    W: Weight matrix (n_features, n_classes)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    n_classes = w.shape[1]\n",
    "    eps = 1e-15\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # 1. Linear pass for ALL classes\n",
    "        # z will be a vector of length n_classes\n",
    "        z = np.zeros(n_classes)\n",
    "        for j, v in X[i].items():\n",
    "            z += w[j, :] * v\n",
    "        z += b\n",
    "        \n",
    "        # 2. Get probability distribution\n",
    "        y_hat = softmax(z)\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "        \n",
    "        # 3. Add the loss for the true class\n",
    "        # In CCE, we only sum the log of the predicted prob for the actual class\n",
    "        total_loss += -np.sum(y[i] * np.log(y_hat))\n",
    "        \n",
    "    avg_loss = total_loss / n\n",
    "    \n",
    "    # 4. Frobenius Norm for L2 Regularization (sum of squares of all weights)\n",
    "    reg = lam * np.sum(w**2)\n",
    "    \n",
    "    return avg_loss + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a704005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_minibatch_cce(w, b, X_batch, y_batch, lam):\n",
    "    \"\"\"\n",
    "    W: Weight matrix (features x classes)\n",
    "    b: Bias vector (classes)\n",
    "    X_batch: List of sparse dictionaries\n",
    "    y_batch: One-hot encoded labels for the batch\n",
    "    \"\"\"\n",
    "    batch_size = len(X_batch)\n",
    "    d, K = w.shape\n",
    "    grad_W = np.zeros_like(w)\n",
    "    grad_b = np.zeros_like(b)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # 1. Linear Pass (Scores for all classes)\n",
    "        z = np.zeros(K)\n",
    "        for j, v in X_batch[i].items():\n",
    "            z += w[j, :] * v\n",
    "        z += b\n",
    "        \n",
    "        # 2. Softmax for probability distribution\n",
    "        y_hat = softmax(z) \n",
    "        \n",
    "        # 3. Error (Prediction - Actual)\n",
    "        error = y_hat - y_batch[i]\n",
    "        \n",
    "        # 4. Aggregate gradients\n",
    "        for j, v in X_batch[i].items():\n",
    "            grad_w[j, :] += error * v\n",
    "        grad_b += error\n",
    "\n",
    "    # 5. Average and Regularize\n",
    "    # Note: L2 is applied to the whole matrix\n",
    "    dw = (1.0 / batch_size) * grad_w + 2 * lam * w\n",
    "    db = (1.0 / batch_size) * grad_b\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5708587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_cce(X, y, n_classes, lam, lr, epochs, batch_size=32):\n",
    "    # 1. Initialize Matrix and Bias\n",
    "    all_indices = [idx for doc in X for idx in doc.keys()]\n",
    "    d = max(all_indices) + 1\n",
    "    w = np.zeros((d, n_classes))\n",
    "    b = np.zeros(n_classes)\n",
    "    \n",
    "    n = len(X)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle indices at the start of each epoch\n",
    "        indices = np.random.permutation(n)\n",
    "        \n",
    "        for start in range(0, n, batch_size):\n",
    "            # Slice the batch\n",
    "            batch_idx = indices[start : start + batch_size]\n",
    "            X_batch = [X[i] for i in batch_idx]\n",
    "            y_batch = y[batch_idx] # y must be one-hot encoded\n",
    "            \n",
    "            # Compute Gradient\n",
    "            dw, db = gradient_minibatch_cce(w, b, X_batch, y_batch, lam)\n",
    "            \n",
    "            # Update Parameters\n",
    "            w -= lr * dW\n",
    "            b -= lr * db\n",
    "            \n",
    "        # Optional: Track loss (Warning: objective_cce is slow for large data)\n",
    "        # loss = objective_cce(W, b, X, y, lam)\n",
    "        # history.append(loss)\n",
    "        \n",
    "    return w, b, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11a3fa8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4015 is out of bounds for axis 0 with size 3850",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv_results_cce \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_minibatch_cce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m best_lambda \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mmin\u001b[39m(cv_results, key\u001b[38;5;241m=\u001b[39mcv_results\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_lambda)\n",
      "Cell \u001b[0;32mIn[73], line 19\u001b[0m, in \u001b[0;36mtrain_minibatch_cce\u001b[0;34m(X, y, n_classes, lam, lr, epochs, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m indices[start : start \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     18\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m [X[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_idx]\n\u001b[0;32m---> 19\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# y must be one-hot encoded\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute Gradient\u001b[39;00m\n\u001b[1;32m     22\u001b[0m dw, db \u001b[38;5;241m=\u001b[39m gradient_minibatch_cce(w, b, X_batch, y_batch, lam)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4015 is out of bounds for axis 0 with size 3850"
     ]
    }
   ],
   "source": [
    "cv_results_cce = train_minibatch_cce(X_train_tfidf, y_train, n_classes = 3, lam=0.001, lr=0.01, epochs = 500)\n",
    "best_lambda =  min(cv_results, key=cv_results.get)\n",
    "print(best_lambda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
