{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3daad581",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: (Matthew Halvorsen)\n",
    "#### Stevens ID: (10444976)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650e6b5",
   "metadata": {},
   "source": [
    "## Part A: Multi-Layer Perceptron (MLP) (total 30 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "2. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
    "3. Implement the MLP with Tensorflow and compare to your implementation.\n",
    "4. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may not run this cell after the first installation\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e21f785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea8c53",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515d659",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d846e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a2-data/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a2-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5c110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8015001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b79b9e",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0678ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "                # expected format: label \\t sentence\n",
    "            parts = line.split(\"\\t\", 1)\n",
    "            if len(parts) != 2:\n",
    "                # skip malformed lines (or raise an error if you prefer)\n",
    "                continue\n",
    "\n",
    "            label, sentence = parts[0].strip(), parts[1].strip()\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "    # End\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2133f2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
      "Number of sentences and labels: 19536 19536\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a2-data', 'books.txt')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec877161",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6c6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    \n",
    "    n = len(sentences)\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    test_size = int(n * test_ratio)\n",
    "\n",
    "    test_index = indices[:test_size]\n",
    "    train_index = indices[test_size:]\n",
    "\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca49b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 14067\n",
      "Validation data length: 1562\n",
      "Test data length: 3907\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ecd26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    id_to_label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "    for lab_id in labels:\n",
    "        label = id_to_label[int(lab_id)]\n",
    "        label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80269569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'Arthur Conan Doyle': 1870, 'Fyodor Dostoyevsky': 4234, 'Jane Austen': 7963}\n",
      "Validation: {'Arthur Conan Doyle': 193, 'Fyodor Dostoyevsky': 464, 'Jane Austen': 905}\n",
      "Test: {'Arthur Conan Doyle': 475, 'Fyodor Dostoyevsky': 1246, 'Jane Austen': 2186}\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f8b58",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
    "|:--------------:|--------------------|--------------------|-------------|-------|\n",
    "|  **Training**  |        1870        |         4234       |    7963     | 14067 |\n",
    "| **Validation** |         193        |         464        |     905     |  1562 |\n",
    "|    **Test**    |         475        |         1246       |     2186    |  3907 |\n",
    "|    **Total**   |          2538      |         5944       |     11054   | 19536 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efbc90",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3f16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        sentence = re.sub(r\"http\\S+|www\\S+\", \"\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c47ddd3",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5403253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f999dcf",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45e6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hadn't\", \"it'll\", 'nor', \"she'd\", 'between', 'are', 'be', 'each', 'haven', 'he']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e82e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    #     Step 1. Split sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "    for word in tokens:\n",
    "        if word.isalpha():\n",
    "            stemmed = stemmer.stem(word)\n",
    "            if stemmed not in stopwords_set:\n",
    "                words.append(stemmed)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af10efe",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b53e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d0552",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bc19d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            \n",
    "            clean = self.processor.apply(sentence)\n",
    "            toks = tokenize(clean)\n",
    "\n",
    "            # Step 1: update corpus token counts\n",
    "            for t in toks:\n",
    "                self.vocab[t] += 1\n",
    "\n",
    "            # Step 2: update document frequency (count each token once per doc)\n",
    "            for t in set(toks):\n",
    "                self.df[t] += 1\n",
    "            \n",
    "        print_line('\\n')\n",
    "        \n",
    "        sorted_tokens = sorted(self.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.token2index = {tok: idx for idx, (tok, _) in enumerate(sorted_tokens)}\n",
    "        \n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        N = self.num_doc if self.num_doc > 0 else 1\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "\n",
    "            clean = self.processor.apply(sentence)\n",
    "            toks = tokenize(clean)\n",
    "\n",
    "            if not toks:\n",
    "                continue\n",
    "\n",
    "            # term frequency in THIS document\n",
    "            tf_counts = defaultdict(int)\n",
    "            for t in toks:\n",
    "                if t in self.token2index:   # ignore unseen tokens\n",
    "                    tf_counts[t] += 1\n",
    "\n",
    "            doc_len = sum(tf_counts.values())\n",
    "            if doc_len == 0:\n",
    "                continue\n",
    "\n",
    "            for t, c in tf_counts.items():\n",
    "                j = self.token2index[t]\n",
    "\n",
    "                # TF (normalized)\n",
    "                tf = c / doc_len\n",
    "\n",
    "                # IDF (smoothed)\n",
    "                df_t = self.df.get(t, 0)\n",
    "                idf = math.log((1 + N) / (1 + df_t)) + 1.0\n",
    "\n",
    "                features[i, j] = tf * idf\n",
    "\n",
    "        print_line('\\n')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b97fe0",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d193da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 1459\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.23192837 ... 0.         0.         0.        ]\n",
      " [0.         0.13428918 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03174377 0.09809405 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8fd24",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22e969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 14067 / 14067\n",
      "The number of distinct tokens: 16212\n",
      "Encoding with TF-IDF encoder: 14067 / 14067\n",
      "Encoding with TF-IDF encoder: 1562 / 1562\n",
      "Encoding with TF-IDF encoder: 3907 / 3907\n",
      "The size of training set: (14067, 16212) (14067, 3)\n",
      "The size of validation set: (1562, 16212) (1562, 3)\n",
      "The size of test set: (3907, 16212) (3907, 3)\n"
     ]
    }
   ],
   "source": [
    "num_class = 3\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = np.zeros((len(train_labels), num_class))\n",
    "y_valid = np.zeros((len(valid_labels), num_class))\n",
    "y_test = np.zeros((len(test_labels), num_class))\n",
    "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
    "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
    "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a7b26",
   "metadata": {},
   "source": [
    "## 2. MLP\n",
    "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for multi-class classification:\n",
    "\n",
    "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
    "\n",
    "- $z_1 = w_1x$\n",
    "- $h_1 = activation(z_1)$\n",
    "- $z_2 = w_2 h_1$\n",
    "- $\\hat{y} = softmax(z_2)$\n",
    "\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "- Here, you can use `sigmoid` as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c3521",
   "metadata": {},
   "source": [
    "### 2.1 MLP Model (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4abe5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\" The softmax activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
    "    Returns:\n",
    "        output: Softmax value of the specified dimension in x\n",
    "    \"\"\"\n",
    "    \n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    x = exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7a9bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        self.w1 = np.random.randn(feature_dim, hidden_dim) * np.sqrt(2.0 / feature_dim)\n",
    "        self.b1 = np.zeros((hidden_dim,), dtype=float)\n",
    "        self.w2 = np.random.randn(hidden_dim, num_class) * np.sqrt(1.0 / hidden_dim)\n",
    "        self.b2 = np.zeros((num_class,), dtype=float)\n",
    "\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray, return_hiddens: bool = False) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_hiddens: If true the function will return h1 for gradient calculation\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        h1 = 0, 0\n",
    "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
    "        \n",
    "        z1 = x @ w1 + b1\n",
    "        a1 = np.maximum(0.0, z1)\n",
    "        z2 = a1 @ w2 + b2\n",
    "        y_hat = softmax(z2, axis=-1)\n",
    "        h1 = (z1, a1)\n",
    "        \n",
    "        if return_hiddens:\n",
    "            return y_hat, h1\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "            h1: Hidden output of the hidden layer\n",
    "        Returns:\n",
    "            dw1: Gradient of w1\n",
    "            db1: Gradient of b1\n",
    "            dw2: Gradient of w2\n",
    "            db2: Gradient of b2\n",
    "        \"\"\"\n",
    "        w1, w2 = self.w1, self.w2\n",
    "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
    "        n = len(x)\n",
    "        \n",
    "        z1, a1 = h1\n",
    "        \n",
    "        dz2 = (y_hat - y) / n               # (n, num_class)\n",
    "\n",
    "        dw2 = a1.T @ dz2                    # (hidden, num_class)\n",
    "        db2 = np.sum(dz2, axis=0)           # (num_class,)\n",
    "\n",
    "        da1 = dz2 @ w2.T                    # (n, hidden)\n",
    "        dz1 = da1 * (z1 > 0).astype(float)  # ReLU grad\n",
    "\n",
    "        dw1 = x.T @ dz1                     # (feature_dim, hidden)\n",
    "        db1 = np.sum(dz1, axis=0)           # (hidden,)\n",
    "\n",
    "        # L2 regularization (biases typically not regularized)\n",
    "        dw2 += self.lambda_ * w2\n",
    "        dw1 += self.lambda_ * w1\n",
    "        \n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def categorical_cross_entropy_loss(self,\n",
    "                                       y_hat: np.ndarray,\n",
    "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        loss = 0\n",
    "        \n",
    "        data_loss = -np.sum(y * np.log(y_hat)) / len(y)\n",
    "\n",
    "        reg_loss = 0.5 * self.lambda_ * (np.sum(self.w1 ** 2) + np.sum(self.w2 ** 2))\n",
    "        loss = data_loss + reg_loss\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.zeros_like(y_hat)\n",
    "        index = np.argmax(y_hat, axis=-1)\n",
    "        pred[np.arange(len(y_hat)), index] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c5e80",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "878adc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy: float number. The accuracy for the whole dataset\n",
    "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    pred_labels = np.argmax(y_pred, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "    accuracy = np.mean(pred_labels == true_labels)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        average=\"micro\"\n",
    "    )\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4a6ef",
   "metadata": {},
   "source": [
    "### 2.3 AdaGrad (5 points)\n",
    "\n",
    "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
    "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fcf8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, init_lr, model):\n",
    "        self.init_lr = init_lr\n",
    "        self.model = model\n",
    "        \n",
    "        self.accumulative_dw1 = 0\n",
    "        self.accumulative_db1 = 0\n",
    "        self.accumulative_dw2 = 0\n",
    "        self.accumulative_db2 = 0\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
    "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
    "            2. Calculate the new gradient with the accumulative gradient\n",
    "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
    "        \n",
    "        Do not return anything\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1\n",
    "        self.accumulative_dw1 = self.accumulative_dw1 + dw1 * dw1\n",
    "        self.accumulative_db1 = self.accumulative_db1 + db1 * db1\n",
    "        self.accumulative_dw2 = self.accumulative_dw2 + dw2 * dw2\n",
    "        self.accumulative_db2 = self.accumulative_db2 + db2 * db2\n",
    "        # Step 2\n",
    "        adj_dw1 = dw1 / (np.sqrt(self.accumulative_dw1) + self.eps)\n",
    "        adj_db1 = db1 / (np.sqrt(self.accumulative_db1) + self.eps)\n",
    "        adj_dw2 = dw2 / (np.sqrt(self.accumulative_dw2) + self.eps)\n",
    "        adj_db2 = db2 / (np.sqrt(self.accumulative_db2) + self.eps)\n",
    "        # Step 3\n",
    "        self.model.gradient_descent(adj_dw1, adj_db1, adj_dw2, adj_db2, self.init_lr)\n",
    "        # End\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2d24e",
   "metadata": {},
   "source": [
    "### 2.4 Mini-batch Gradient Descent (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fca95ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_mbgd(model: 'MLP',\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
    "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        perm = np.random.permutation(train_num_samples)\n",
    "        x_shuf = x_train[perm]\n",
    "        y_shuf = y_train[perm]\n",
    "\n",
    "        for b in range(n_batch):\n",
    "            start = b * batch_size\n",
    "            end = start + batch_size\n",
    "            xb = x_shuf[start:end]\n",
    "            yb = y_shuf[start:end]\n",
    "\n",
    "            # Step 1. Model forward\n",
    "            y_hat, h1 = model.forward(xb, return_hiddens=True)\n",
    "\n",
    "            # Step 2. Calculate loss\n",
    "            batch_loss = model.categorical_cross_entropy_loss(y_hat, yb)\n",
    "            epoch_loss += batch_loss * len(xb)  # sum over samples\n",
    "\n",
    "            # Step 3. Model backward\n",
    "            dw1, db1, dw2, db2 = model.backward(xb, y_hat, yb, h1)\n",
    "\n",
    "            # Step 4. Optimization with Adagrad\n",
    "            optimizer.update(dw1, db1, dw2, db2)\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        \n",
    "        y_hat_valid = model.forward(x_valid, return_hiddens=False)\n",
    "        y_pred_valid = model.predict(y_hat_valid)\n",
    "\n",
    "        # Step 2. Calculate loss\n",
    "        valid_loss = model.categorical_cross_entropy_loss(y_hat_valid, y_valid)\n",
    "\n",
    "        # Step 3. Calculate metrics\n",
    "        accuracy, precision, recall, f1 = get_metrics(y_pred_valid, y_valid)\n",
    "\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c966c62",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "958d77db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100: train_loss: 0.4442 - valid_loss: 0.2846 - valid_accuracy: 0.9161 - valid_precision: 0.9161 - valid_recall: 0.9161 - valid_f1: 0.9161\n",
      "Epoch 11 / 100: train_loss: 0.0592 - valid_loss: 0.2544 - valid_accuracy: 0.9270 - valid_precision: 0.9270 - valid_recall: 0.9270 - valid_f1: 0.9270\n",
      "Epoch 21 / 100: train_loss: 0.0417 - valid_loss: 0.2863 - valid_accuracy: 0.9251 - valid_precision: 0.9251 - valid_recall: 0.9251 - valid_f1: 0.9251\n",
      "Epoch 31 / 100: train_loss: 0.0346 - valid_loss: 0.3077 - valid_accuracy: 0.9257 - valid_precision: 0.9257 - valid_recall: 0.9257 - valid_f1: 0.9257\n",
      "Epoch 41 / 100: train_loss: 0.0310 - valid_loss: 0.3243 - valid_accuracy: 0.9238 - valid_precision: 0.9238 - valid_recall: 0.9238 - valid_f1: 0.9238\n",
      "Epoch 51 / 100: train_loss: 0.0287 - valid_loss: 0.3401 - valid_accuracy: 0.9238 - valid_precision: 0.9238 - valid_recall: 0.9238 - valid_f1: 0.9238\n",
      "Epoch 61 / 100: train_loss: 0.0272 - valid_loss: 0.3524 - valid_accuracy: 0.9238 - valid_precision: 0.9238 - valid_recall: 0.9238 - valid_f1: 0.9238\n",
      "Epoch 71 / 100: train_loss: 0.0260 - valid_loss: 0.3630 - valid_accuracy: 0.9251 - valid_precision: 0.9251 - valid_recall: 0.9251 - valid_f1: 0.9251\n",
      "Epoch 81 / 100: train_loss: 0.0253 - valid_loss: 0.3726 - valid_accuracy: 0.9251 - valid_precision: 0.9251 - valid_recall: 0.9251 - valid_f1: 0.9251\n",
      "Epoch 91 / 100: train_loss: 0.0243 - valid_loss: 0.3813 - valid_accuracy: 0.9251 - valid_precision: 0.9251 - valid_recall: 0.9251 - valid_f1: 0.9251\n",
      "Epoch 100 / 100: train_loss: 0.0240 - valid_loss: 0.3881 - valid_accuracy: 0.9264 - valid_precision: 0.9264 - valid_recall: 0.9264 - valid_f1: 0.9264\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ab731",
   "metadata": {},
   "source": [
    "### 2.5 MLP using Tensorflow (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88ac7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "\n",
    "class MLPTF(Model):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model using tensorflow.keras\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(\n",
    "            hidden_dim,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.l2(lambda_),\n",
    "            input_shape=(feature_dim,)\n",
    "        )\n",
    "        self.dense2 = Dense(\n",
    "            num_class,\n",
    "            activation=None,  # logits\n",
    "            kernel_regularizer=regularizers.l2(lambda_)\n",
    "        )\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Forward function of tf. It should be named 'call'\n",
    "        \n",
    "        Args:\n",
    "            x: (n x feature_dim) tensor\n",
    "        Returns:\n",
    "            y_hat: (n x num_class) tensor\n",
    "        \"\"\"\n",
    "        h1 = self.dense1(x)\n",
    "        logits = self.dense2(h1)\n",
    "        y_hat = self.softmax(logits)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dea7f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlptf_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             multiple                  2075264   \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  387       \n",
      "                                                                 \n",
      " softmax_1 (Softmax)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2075651 (7.92 MB)\n",
      "Trainable params: 2075651 (7.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "np.random.seed(6666)\n",
    "tf.random.set_seed(6666)\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "\n",
    "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "model_tf.build(input_shape=(None, vocab_size))\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b419e104",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8742 - categorical_accuracy: 0.6008 - f1_score: 0.6009 - val_loss: 0.7450 - val_categorical_accuracy: 0.6940 - val_f1_score: 0.6940\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.6266 - categorical_accuracy: 0.7742 - f1_score: 0.7742 - val_loss: 0.5339 - val_categorical_accuracy: 0.7990 - val_f1_score: 0.7990\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.4546 - categorical_accuracy: 0.8428 - f1_score: 0.8428 - val_loss: 0.4310 - val_categorical_accuracy: 0.8547 - val_f1_score: 0.8547\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.3545 - categorical_accuracy: 0.8817 - f1_score: 0.8817 - val_loss: 0.3665 - val_categorical_accuracy: 0.8745 - val_f1_score: 0.8745\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2898 - categorical_accuracy: 0.9107 - f1_score: 0.9107 - val_loss: 0.3287 - val_categorical_accuracy: 0.8950 - val_f1_score: 0.8950\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2436 - categorical_accuracy: 0.9279 - f1_score: 0.9279 - val_loss: 0.3073 - val_categorical_accuracy: 0.9072 - val_f1_score: 0.9072\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2109 - categorical_accuracy: 0.9396 - f1_score: 0.9396 - val_loss: 0.2898 - val_categorical_accuracy: 0.9046 - val_f1_score: 0.9046\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1860 - categorical_accuracy: 0.9473 - f1_score: 0.9473 - val_loss: 0.2771 - val_categorical_accuracy: 0.9136 - val_f1_score: 0.9136\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1665 - categorical_accuracy: 0.9546 - f1_score: 0.9546 - val_loss: 0.2666 - val_categorical_accuracy: 0.9104 - val_f1_score: 0.9104\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1498 - categorical_accuracy: 0.9588 - f1_score: 0.9588 - val_loss: 0.2651 - val_categorical_accuracy: 0.9155 - val_f1_score: 0.9155\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1372 - categorical_accuracy: 0.9630 - f1_score: 0.9630 - val_loss: 0.2597 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1264 - categorical_accuracy: 0.9666 - f1_score: 0.9666 - val_loss: 0.2596 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1173 - categorical_accuracy: 0.9689 - f1_score: 0.9689 - val_loss: 0.2611 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1094 - categorical_accuracy: 0.9702 - f1_score: 0.9702 - val_loss: 0.2556 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1020 - categorical_accuracy: 0.9731 - f1_score: 0.9731 - val_loss: 0.2584 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0960 - categorical_accuracy: 0.9741 - f1_score: 0.9741 - val_loss: 0.2602 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0920 - categorical_accuracy: 0.9747 - f1_score: 0.9747 - val_loss: 0.2561 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0859 - categorical_accuracy: 0.9770 - f1_score: 0.9770 - val_loss: 0.2575 - val_categorical_accuracy: 0.9213 - val_f1_score: 0.9213\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0816 - categorical_accuracy: 0.9773 - f1_score: 0.9773 - val_loss: 0.2635 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0790 - categorical_accuracy: 0.9779 - f1_score: 0.9779 - val_loss: 0.2608 - val_categorical_accuracy: 0.9213 - val_f1_score: 0.9213\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0752 - categorical_accuracy: 0.9801 - f1_score: 0.9801 - val_loss: 0.2610 - val_categorical_accuracy: 0.9245 - val_f1_score: 0.9245\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0720 - categorical_accuracy: 0.9797 - f1_score: 0.9797 - val_loss: 0.2643 - val_categorical_accuracy: 0.9225 - val_f1_score: 0.9225\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0690 - categorical_accuracy: 0.9821 - f1_score: 0.9821 - val_loss: 0.2637 - val_categorical_accuracy: 0.9232 - val_f1_score: 0.9232\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0665 - categorical_accuracy: 0.9811 - f1_score: 0.9811 - val_loss: 0.2681 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0638 - categorical_accuracy: 0.9824 - f1_score: 0.9824 - val_loss: 0.2711 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0622 - categorical_accuracy: 0.9827 - f1_score: 0.9827 - val_loss: 0.2687 - val_categorical_accuracy: 0.9213 - val_f1_score: 0.9213\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0596 - categorical_accuracy: 0.9840 - f1_score: 0.9840 - val_loss: 0.2682 - val_categorical_accuracy: 0.9232 - val_f1_score: 0.9232\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0579 - categorical_accuracy: 0.9842 - f1_score: 0.9842 - val_loss: 0.2751 - val_categorical_accuracy: 0.9225 - val_f1_score: 0.9225\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0562 - categorical_accuracy: 0.9847 - f1_score: 0.9847 - val_loss: 0.2750 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0542 - categorical_accuracy: 0.9851 - f1_score: 0.9851 - val_loss: 0.2782 - val_categorical_accuracy: 0.9213 - val_f1_score: 0.9213\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0534 - categorical_accuracy: 0.9848 - f1_score: 0.9848 - val_loss: 0.2836 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0517 - categorical_accuracy: 0.9854 - f1_score: 0.9854 - val_loss: 0.2811 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0502 - categorical_accuracy: 0.9856 - f1_score: 0.9856 - val_loss: 0.2871 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0500 - categorical_accuracy: 0.9854 - f1_score: 0.9854 - val_loss: 0.2861 - val_categorical_accuracy: 0.9225 - val_f1_score: 0.9225\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0487 - categorical_accuracy: 0.9854 - f1_score: 0.9854 - val_loss: 0.2860 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0474 - categorical_accuracy: 0.9851 - f1_score: 0.9851 - val_loss: 0.2882 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0459 - categorical_accuracy: 0.9862 - f1_score: 0.9862 - val_loss: 0.2976 - val_categorical_accuracy: 0.9225 - val_f1_score: 0.9225\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0456 - categorical_accuracy: 0.9867 - f1_score: 0.9867 - val_loss: 0.2947 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0439 - categorical_accuracy: 0.9873 - f1_score: 0.9873 - val_loss: 0.2935 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0434 - categorical_accuracy: 0.9866 - f1_score: 0.9866 - val_loss: 0.2949 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0427 - categorical_accuracy: 0.9868 - f1_score: 0.9868 - val_loss: 0.2987 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0417 - categorical_accuracy: 0.9871 - f1_score: 0.9871 - val_loss: 0.3013 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0420 - categorical_accuracy: 0.9868 - f1_score: 0.9868 - val_loss: 0.3000 - val_categorical_accuracy: 0.9232 - val_f1_score: 0.9232\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0405 - categorical_accuracy: 0.9871 - f1_score: 0.9871 - val_loss: 0.3080 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0398 - categorical_accuracy: 0.9883 - f1_score: 0.9883 - val_loss: 0.3080 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0398 - categorical_accuracy: 0.9873 - f1_score: 0.9873 - val_loss: 0.3051 - val_categorical_accuracy: 0.9232 - val_f1_score: 0.9232\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0396 - categorical_accuracy: 0.9871 - f1_score: 0.9871 - val_loss: 0.3097 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0379 - categorical_accuracy: 0.9877 - f1_score: 0.9877 - val_loss: 0.3098 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0384 - categorical_accuracy: 0.9878 - f1_score: 0.9878 - val_loss: 0.3121 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0369 - categorical_accuracy: 0.9887 - f1_score: 0.9887 - val_loss: 0.3118 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0374 - categorical_accuracy: 0.9877 - f1_score: 0.9877 - val_loss: 0.3167 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0364 - categorical_accuracy: 0.9886 - f1_score: 0.9886 - val_loss: 0.3195 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0357 - categorical_accuracy: 0.9887 - f1_score: 0.9887 - val_loss: 0.3184 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0359 - categorical_accuracy: 0.9879 - f1_score: 0.9879 - val_loss: 0.3206 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0355 - categorical_accuracy: 0.9886 - f1_score: 0.9886 - val_loss: 0.3229 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0349 - categorical_accuracy: 0.9881 - f1_score: 0.9881 - val_loss: 0.3277 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0345 - categorical_accuracy: 0.9885 - f1_score: 0.9885 - val_loss: 0.3264 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0340 - categorical_accuracy: 0.9884 - f1_score: 0.9884 - val_loss: 0.3271 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0335 - categorical_accuracy: 0.9883 - f1_score: 0.9883 - val_loss: 0.3267 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0331 - categorical_accuracy: 0.9892 - f1_score: 0.9892 - val_loss: 0.3302 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0332 - categorical_accuracy: 0.9887 - f1_score: 0.9887 - val_loss: 0.3304 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0332 - categorical_accuracy: 0.9883 - f1_score: 0.9883 - val_loss: 0.3308 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0331 - categorical_accuracy: 0.9884 - f1_score: 0.9884 - val_loss: 0.3355 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0324 - categorical_accuracy: 0.9892 - f1_score: 0.9892 - val_loss: 0.3369 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0317 - categorical_accuracy: 0.9891 - f1_score: 0.9891 - val_loss: 0.3351 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0320 - categorical_accuracy: 0.9888 - f1_score: 0.9888 - val_loss: 0.3383 - val_categorical_accuracy: 0.9161 - val_f1_score: 0.9161\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0314 - categorical_accuracy: 0.9888 - f1_score: 0.9888 - val_loss: 0.3401 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0321 - categorical_accuracy: 0.9886 - f1_score: 0.9886 - val_loss: 0.3411 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0313 - categorical_accuracy: 0.9887 - f1_score: 0.9887 - val_loss: 0.3418 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0310 - categorical_accuracy: 0.9888 - f1_score: 0.9888 - val_loss: 0.3461 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0311 - categorical_accuracy: 0.9887 - f1_score: 0.9887 - val_loss: 0.3424 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0309 - categorical_accuracy: 0.9889 - f1_score: 0.9889 - val_loss: 0.3453 - val_categorical_accuracy: 0.9155 - val_f1_score: 0.9155\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0302 - categorical_accuracy: 0.9885 - f1_score: 0.9885 - val_loss: 0.3468 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0302 - categorical_accuracy: 0.9889 - f1_score: 0.9889 - val_loss: 0.3500 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0303 - categorical_accuracy: 0.9888 - f1_score: 0.9888 - val_loss: 0.3469 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0302 - categorical_accuracy: 0.9882 - f1_score: 0.9882 - val_loss: 0.3465 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0291 - categorical_accuracy: 0.9895 - f1_score: 0.9895 - val_loss: 0.3533 - val_categorical_accuracy: 0.9149 - val_f1_score: 0.9149\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0294 - categorical_accuracy: 0.9890 - f1_score: 0.9890 - val_loss: 0.3502 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0293 - categorical_accuracy: 0.9887 - f1_score: 0.9887 - val_loss: 0.3562 - val_categorical_accuracy: 0.9161 - val_f1_score: 0.9161\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0289 - categorical_accuracy: 0.9889 - f1_score: 0.9889 - val_loss: 0.3523 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0284 - categorical_accuracy: 0.9895 - f1_score: 0.9895 - val_loss: 0.3526 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0285 - categorical_accuracy: 0.9888 - f1_score: 0.9888 - val_loss: 0.3592 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0286 - categorical_accuracy: 0.9883 - f1_score: 0.9883 - val_loss: 0.3564 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0287 - categorical_accuracy: 0.9891 - f1_score: 0.9891 - val_loss: 0.3577 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0289 - categorical_accuracy: 0.9880 - f1_score: 0.9880 - val_loss: 0.3597 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0272 - categorical_accuracy: 0.9899 - f1_score: 0.9899 - val_loss: 0.3596 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0276 - categorical_accuracy: 0.9892 - f1_score: 0.9892 - val_loss: 0.3609 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0275 - categorical_accuracy: 0.9895 - f1_score: 0.9895 - val_loss: 0.3614 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0276 - categorical_accuracy: 0.9892 - f1_score: 0.9892 - val_loss: 0.3614 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0276 - categorical_accuracy: 0.9886 - f1_score: 0.9886 - val_loss: 0.3594 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0274 - categorical_accuracy: 0.9894 - f1_score: 0.9894 - val_loss: 0.3625 - val_categorical_accuracy: 0.9193 - val_f1_score: 0.9193\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0272 - categorical_accuracy: 0.9898 - f1_score: 0.9898 - val_loss: 0.3629 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0269 - categorical_accuracy: 0.9900 - f1_score: 0.9900 - val_loss: 0.3651 - val_categorical_accuracy: 0.9206 - val_f1_score: 0.9206\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0264 - categorical_accuracy: 0.9901 - f1_score: 0.9901 - val_loss: 0.3673 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0274 - categorical_accuracy: 0.9891 - f1_score: 0.9891 - val_loss: 0.3681 - val_categorical_accuracy: 0.9161 - val_f1_score: 0.9161\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.0271 - categorical_accuracy: 0.9881 - f1_score: 0.9881 - val_loss: 0.3680 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0272 - categorical_accuracy: 0.9883 - f1_score: 0.9883 - val_loss: 0.3688 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0265 - categorical_accuracy: 0.9895 - f1_score: 0.9895 - val_loss: 0.3704 - val_categorical_accuracy: 0.9181 - val_f1_score: 0.9181\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0263 - categorical_accuracy: 0.9898 - f1_score: 0.9898 - val_loss: 0.3685 - val_categorical_accuracy: 0.9187 - val_f1_score: 0.9187\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0264 - categorical_accuracy: 0.9893 - f1_score: 0.9893 - val_loss: 0.3753 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n"
     ]
    }
   ],
   "source": [
    "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5979a",
   "metadata": {},
   "source": [
    "#### Evaluation with Tensorflow\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "715fbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD: (0.9111850524699258, 0.9111850524699258, 0.9111850524699258, 0.9111850524699258)\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.4718 - categorical_accuracy: 0.9122 - f1_score: 0.9122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4718446433544159, 0.9122088551521301, 0.9122088551521301]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7bbf3d",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "|     Optimizer                     | Accuracy    | F1 Score    |\n",
    "|:---------------------------------:|-------------|-------------|\n",
    "|      **Your Implementation**      |    0.911    |    0.911    |\n",
    "| **Tensorflow**                    |    0.912    |    0.912    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1b399",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7454c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
    "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942ee9c",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1deae578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax = axes[0]\n",
    "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation Accuracy')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d4e8a",
   "metadata": {},
   "source": [
    "# 3. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a3989",
   "metadata": {},
   "source": [
    "Answer: \n",
    "The results from both models were very similar. The first thing to point out is how the implementation of the tensorflow module is very compact and does not require a lot of additional code to run. My implementation requires a lot more code in order to run effectively which leads to more potential errors such as an exploding gradient if not handled properly. For the actual metrics, there were very minor differences between the two models. The accuracy for the model built from scratch was .911 while the tensorflow implementation was .912, this trend followed for f1 as my model gave a value of 0.911 and tensorflow returned a value of 0.912. This is indicative of the model being essentially identical when it comes to performance. One major difference between the two models was the epoch it took to converge. This can be seen in the graph for training loss vs epoch. My model converged ti a lower training loss from the beginning at roughly 0.44 while the tensorflow model loss started at 0.87. This could be caused for a variety of reasons like how the models inital weights were initialized or regularization method. Roughly, somewhere between 40 and 60 epochs my model and tensorflow model converged to the same point. This same trend is shown in the validation accuracy vs epoch graph and the validation vs epoch graph, my model reached a high validation accuracy and f1 score very quickly while the tensorflow model had a more gradual convergence. One clear difference between the two models was the learning rate which was set to .01 for my model while the tensorflow model used a learning rate of 0.1. This could have contributed to the tensorflow model having more instability and a higher loss during the intial epochs as it could cause noisier updates to the weights causing for more updates to be needed to reach convergence. Overall these results are promising considering the dataset is very unbalanced and small. It would be interesting to see how many of the false positive or negatives are coming from the smaller classes in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS584",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
