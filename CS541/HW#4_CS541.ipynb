{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e80d90",
   "metadata": {},
   "source": [
    "CS541 Homework 4: Compute the stochastic gradient of the objective function of 1-bit recommendation system\n",
    "\n",
    "Work Completed by Matthew Halvorsen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389a594",
   "metadata": {},
   "source": [
    "This acts as a one bit recommendation system, the data used in not real so the output is a hypothetical log loss for 5 epochs on a binary system. If a real data set was used the test log-loss and accuracy would be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20741e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test log-loss: 0.6927   |   Test acc: 0.512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "class OneBitMF:\n",
    "    def __init__(\n",
    "        self, n_users, n_items, rank=32, labels=\"01\",\n",
    "        lambda_u=1e-3, lambda_v=1e-3, lambda_b=1e-4, lambda_c=1e-4,\n",
    "        lr=0.05, batch_size=4096, n_epochs=10, seed=42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Optimizes the objective:  (average logistic loss) + L2 regularization.\n",
    "        labels: \"01\" for y∈{0,1}; \"pm1\" for y∈{-1,+1}\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.U = 0.01 * rng.standard_normal((n_users, rank))\n",
    "        self.V = 0.01 * rng.standard_normal((n_items, rank))\n",
    "        self.b = np.zeros(n_users)   # user bias\n",
    "        self.c = np.zeros(n_items)   # item bias\n",
    "        self.mu = 0.0                # global bias\n",
    "\n",
    "        self.rank = rank\n",
    "        self.labels = labels\n",
    "        self.lambda_u = lambda_u\n",
    "        self.lambda_v = lambda_v\n",
    "        self.lambda_b = lambda_b\n",
    "        self.lambda_c = lambda_c\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def _batch_iter(self, I, J, Y, batch_size):\n",
    "        n = len(Y)\n",
    "        idx = np.arange(n)\n",
    "        np.random.shuffle(idx)\n",
    "        for s in range(0, n, batch_size):\n",
    "            b = idx[s:s+batch_size]\n",
    "            yield I[b], J[b], Y[b]\n",
    "\n",
    "    def fit(self, I, J, Y):\n",
    "        \"\"\"\n",
    "        I, J, Y are 1D arrays of the same length with observed (user i, item j, label y).\n",
    "        I: user indices in [0, n_users)\n",
    "        J: item indices in [0, n_items)\n",
    "        Y: labels (either 0/1 or -1/+1 depending on `labels`)\n",
    "        \"\"\"\n",
    "        n = len(Y)\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for Ib, Jb, Yb in self._batch_iter(I, J, Y, self.batch_size):\n",
    "                # Forward pass (vectorized over the mini-batch)\n",
    "                z = (self.U[Ib] * self.V[Jb]).sum(axis=1) + self.b[Ib] + self.c[Jb] + self.mu\n",
    "\n",
    "                if self.labels == \"01\":\n",
    "                    p = sigmoid(z)\n",
    "                    e = p - Yb.astype(np.float64)        # gradient wrt z\n",
    "                elif self.labels == \"pm1\":\n",
    "                    # loss = log(1 + exp(-y*z)); d/dz = - y * sigmoid(-y*z)\n",
    "                    y = Yb.astype(np.float64)\n",
    "                    e = - y * sigmoid(-y * z)\n",
    "                else:\n",
    "                    raise ValueError(\"labels must be '01' or 'pm1'\")\n",
    "\n",
    "                m = len(Yb)               # batch size (may be last partial batch)\n",
    "                scale = 1.0 / m           # we optimize average loss + regularization\n",
    "\n",
    "                # Accumulate data gradients into full-sized buffers\n",
    "                gU = np.zeros_like(self.U)\n",
    "                gV = np.zeros_like(self.V)\n",
    "                gb = np.zeros_like(self.b)\n",
    "                gc = np.zeros_like(self.c)\n",
    "                gmu = e.sum() * scale\n",
    "\n",
    "                # gU[i] += (e * V[j]); gV[j] += (e * U[i]); gb[i] += e; gc[j] += e\n",
    "                np.add.at(gU, Ib, (e[:, None] * self.V[Jb]) * scale)\n",
    "                np.add.at(gV, Jb, (e[:, None] * self.U[Ib]) * scale)\n",
    "                np.add.at(gb, Ib, e * scale)\n",
    "                np.add.at(gc, Jb, e * scale)\n",
    "\n",
    "                # Add L2 regularization gradients (on the rows/entries touched in this batch)\n",
    "                Ui = np.unique(Ib)\n",
    "                Vj = np.unique(Jb)\n",
    "                gU[Ui] += self.lambda_u * self.U[Ui]\n",
    "                gV[Vj] += self.lambda_v * self.V[Vj]\n",
    "                gb[Ui] += self.lambda_b * self.b[Ui]\n",
    "                gc[Vj] += self.lambda_c * self.c[Vj]\n",
    "\n",
    "                # SGD updates (only on touched indices for efficiency)\n",
    "                self.U[Ui] -= self.lr * gU[Ui]\n",
    "                self.V[Vj] -= self.lr * gV[Vj]\n",
    "                self.b[Ui] -= self.lr * gb[Ui]\n",
    "                self.c[Vj] -= self.lr * gc[Vj]\n",
    "                self.mu    -= self.lr * gmu\n",
    "\n",
    "    # ---- Inference helpers ----\n",
    "    def score(self, i, j):\n",
    "        \"\"\"Raw score z_ij = u_i^T v_j + b_i + c_j + mu.\"\"\"\n",
    "        return float(self.U[i] @ self.V[j] + self.b[i] + self.c[j] + self.mu)\n",
    "\n",
    "    def predict_proba(self, i, j):\n",
    "        \"\"\"P(y=1 | i,j) for logistic model.\"\"\"\n",
    "        return sigmoid(self.score(i, j))\n",
    "\n",
    "    def predict(self, i, j, thresh=0.5):\n",
    "        \"\"\"Binary prediction in {0,1} using a threshold.\"\"\"\n",
    "        return int(self.predict_proba(i, j) >= thresh)\n",
    "\n",
    "\n",
    "# Minimal working example\n",
    "if __name__ == \"__main__\":\n",
    "    rng = np.random.default_rng(0)\n",
    "    n_users, n_items, rank = 500, 800, 16\n",
    "\n",
    "    # Create a small synthetic dataset\n",
    "    true_U = rng.standard_normal((n_users, rank))\n",
    "    true_V = rng.standard_normal((n_items, rank))\n",
    "    true_b = rng.normal(0, 0.2, size=n_users)\n",
    "    true_c = rng.normal(0, 0.2, size=n_items)\n",
    "    true_mu = -0.1\n",
    "\n",
    "    # Sample observations\n",
    "    N_obs = 100_000\n",
    "    I = rng.integers(0, n_users, size=N_obs)\n",
    "    J = rng.integers(0, n_items, size=N_obs)\n",
    "    z = (true_U[I] * true_V[J]).sum(axis=1) + true_b[I] + true_c[J] + true_mu\n",
    "    P = sigmoid(z)\n",
    "    Y = rng.binomial(1, P)  # labels in {0,1}\n",
    "\n",
    "    # Train\n",
    "    model = OneBitMF(\n",
    "        n_users, n_items, rank=rank, labels=\"01\",\n",
    "        lambda_u=5e-4, lambda_v=5e-4, lambda_b=1e-4, lambda_c=1e-4,\n",
    "        lr=0.1, batch_size=4096, n_epochs=5, seed=0\n",
    "    )\n",
    "    model.fit(I, J, Y)\n",
    "\n",
    "    # Evaluate on a held-out set (simple AUC-like proxy using probabilities)\n",
    "    I_te = rng.integers(0, n_users, size=5000)\n",
    "    J_te = rng.integers(0, n_items, size=5000)\n",
    "    z_te = (true_U[I_te] * true_V[J_te]).sum(axis=1) + true_b[I_te] + true_c[J_te] + true_mu\n",
    "    p_te = sigmoid(z_te)\n",
    "    y_te = rng.binomial(1, p_te)\n",
    "\n",
    "    pred = np.array([model.predict_proba(i, j) for i, j in zip(I_te, J_te)])\n",
    "    # Log-loss\n",
    "    eps = 1e-12\n",
    "    logloss = -np.mean(y_te * np.log(pred + eps) + (1 - y_te) * np.log(1 - pred + eps))\n",
    "    # Accuracy at 0.5\n",
    "    acc = np.mean((pred >= 0.5) == y_te)\n",
    "\n",
    "    print(f\"Test log-loss: {logloss:.4f}   |   Test acc: {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
